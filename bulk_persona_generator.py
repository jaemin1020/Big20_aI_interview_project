import os
import sys
import json
import random
import logging
from datetime import datetime
import numpy as np

# ==========================================================
# [Windows GPU FIX] CUDA DLL ê²½ë¡œ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë¡œì§
# ==========================================================
if os.name == 'nt':
    possible_paths = []
    cuda_path = os.environ.get('CUDA_PATH')
    if cuda_path:
        possible_paths.append(os.path.join(cuda_path, 'bin'))
    
    base_cuda = r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA"
    if os.path.exists(base_cuda):
        try:
            versions = os.listdir(base_cuda)
            for v in versions:
                if v.startswith('v'):
                    possible_paths.append(os.path.join(base_cuda, v, 'bin'))
        except:
            pass

    for path in possible_paths:
        if os.path.exists(path):
            try:
                os.add_dll_directory(path)
            except:
                pass
# ==========================================================

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["USE_GPU"] = "true"
os.environ["N_GPU_LAYERS"] = "35"

import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
root_dir = os.path.abspath(os.path.dirname(__file__))
sys.path.append(root_dir)
# í•˜ì´í”ˆì´ í¬í•¨ëœ í´ë”ë¥¼ ì§ì ‘ pathì— ì¶”ê°€í•˜ì—¬ ë‚´ë¶€ ëª¨ë“ˆì„ ë°”ë¡œ ìž„í¬íŠ¸ ê°€ëŠ¥í•˜ê²Œ í•¨
sys.path.append(os.path.join(root_dir, "ai-worker"))
sys.path.append(os.path.join(root_dir, "backend-core"))

# [ì¤‘ìš”] Docker ë°–(ë¡œì»¬ CMD)ì—ì„œ ì‹¤í–‰í•  ë•Œë¥¼ ìœ„í•œ DB ê²½ë¡œ ì„¤ì •
# db:5432ëŠ” ë„ì»¤ ì „ìš©ì´ë¯€ë¡œ, ë¡œì»¬ ì ‘ì†ìš© í¬íŠ¸ì¸ 15432ì™€ localhostë¡œ ë³€ê²½
if not os.environ.get("DATABASE_URL") or "db:5432" in os.environ.get("DATABASE_URL", ""):
    os.environ["DATABASE_URL"] = "postgresql+psycopg://postgres:1234@localhost:15432/interview_db"
    # redis_url ë“± ë‹¤ë¥¸ í™˜ê²½ë³€ìˆ˜ë„ ë¡œì»¬ ë²„ì „ì´ í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ì„¤ì • ê°€ëŠ¥

# ëª¨ë“ˆ ìž„í¬íŠ¸ (ìˆœì„œ ì£¼ì˜: path ì„¤ì • í›„ ìž„í¬íŠ¸)
from utils.exaone_llm import get_exaone_llm
from utils.vector_utils import get_embedding_generator
from utils.question_retriever import get_question_retriever
from tasks.chunking import chunk_resume
from db import engine, save_generated_question
from db_models import User, Resume, Interview, InterviewStatus, Question, QuestionCategory
from sqlmodel import Session, select
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import OllamaLLM

# ì‹œë‚˜ë¦¬ì˜¤ ìž„í¬íŠ¸
import config.interview_scenario as std_scenario
import config.interview_scenario_transition as trans_scenario

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("bulk_gen_final.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì§€ì›ìž ì „ê³µ ì •ë³´ ì¶”ê°€)
PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ì „ë¬¸ ë©´ì ‘ê´€ìž…ë‹ˆë‹¤. 
ê°€ìƒ ì§€ì›ìž {name}ë‹˜ì˜ {position} ì§ë¬´ ë©´ì ‘ì„ ì§„í–‰í•˜ê³  ìžˆìŠµë‹ˆë‹¤.

[ì§€ì›ìž ì´ë ¥ì„œ ìš”ì•½]
- ì´ë¦„: {name}
- ì „ê³µ: {major}
- í˜„ìž¬ ì§€ì› ë¶€ì„œ: {position}
- í•µì‹¬ í”„ë¡œì íŠ¸: {project_title}
- ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸: {context}

[í˜„ìž¬ ë©´ì ‘ ë‹¨ê³„]
ë‹¨ê³„ëª…: {stage_name}
í‰ê°€ ê°€ì´ë“œ: {guide}

[ì°¸ê³  ì§ˆë¬¸ ì„¸íŠ¸]
{db_questions}

[ìž„ë¬´]
1. ìœ„ ì»¨í…ìŠ¤íŠ¸(íŠ¹ížˆ ì „ê³µê³¼ ì§€ì›ì§ë¬´ì˜ ì—°ê´€ì„±)ì™€ ê°€ì´ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì§€ì›ìžì˜ ê²½í—˜ì„ ê¹Šì´ ìžˆê²Œ ê²€ì¦í•  ìˆ˜ ìžˆëŠ” ë©´ì ‘ ì§ˆë¬¸ì„ 1ê°œë§Œ ìƒì„±í•˜ì„¸ìš”.
2. ë§Œì•½ ì „ê³µ({major})ê³¼ ì§€ì›ì§ë¬´({position})ê°€ ìƒì´í•˜ë‹¤ë©´, ì™œ ì „ê³µì„ ì‚´ë¦¬ì§€ ì•Šê³  ì´ ê¸¸ì„ íƒí–ˆëŠ”ì§€, í˜¹ì€ ì „ê³µ ì§€ì‹ì´ ì´ ì§ë¬´ì— ì–´ë–»ê²Œ ê¸°ì—¬í•  ìˆ˜ ìžˆëŠ”ì§€ì— ëŒ€í•œ ë¬¸ë§¥ì„ ì§ˆë¬¸ì— ìžì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚´ì„¸ìš”.
3. AIê°€ ìƒì„±í•œ ëŠë‚Œì´ ë“¤ì§€ ì•Šë„ë¡ ìžì—°ìŠ¤ëŸ½ê³  ì •ì¤‘í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
4. ì§ˆë¬¸ì˜ ì„œë‘ì— ë°˜ë“œì‹œ ìƒí™©ì— ë§žëŠ” ê³µê°ì´ë‚˜ ë¦¬ì•¡ì…˜(ì˜ˆ: "ì•„, ê·¸ëŸ¬ì‹œêµ°ìš”", "í¥ë¯¸ì§„ì§„í•œ í”„ë¡œì íŠ¸ë„¤ìš”")ì„ í¬í•¨í•˜ì„¸ìš”.
5. ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì„œìˆ ì€ ì œì™¸í•˜ì„¸ìš”.
"""

def get_exaone_llm_ollama():
    try:
        return OllamaLLM(model="exaone3.5:latest", temperature=0.7)
    except Exception:
        return get_exaone_llm()

# ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ê¸€ë¡œë²Œ ë ˆì§€ìŠ¤íŠ¸ë¦¬
used_names = set()
used_projects = set()
used_companies = set()
used_majors = set()

def load_existing_data():
    """DBì—ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ ì¤‘ë³µ ë°©ì§€ ì„¸íŠ¸ ì´ˆê¸°í™”"""
    try:
        with Session(engine) as session:
            # ê¸°ì¡´ ìœ ì € ì´ë¦„ ë° ì „ê³µ ë¡œë“œ
            users = session.exec(select(User.full_name)).all()
            for name in users:
                if name: used_names.add(name)
            
            # Resumeì—ì„œ ì „ê³µ ì •ë³´ ìˆ˜ì§‘
            resumes = session.exec(select(Resume.structured_data)).all()
            for data in resumes:
                if data and 'header' in data:
                    major = data['header'].get('major')
                    if major: used_majors.add(major)
            
            logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ë¦„ {len(used_names)}ê°œ, ì „ê³µ {len(used_majors)}ê°œ")
    except Exception as e:
        logger.warning(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

def generate_persona(llm, position):
    """ì ˆëŒ€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” íŽ˜ë¥´ì†Œë‚˜ ìƒì„± (ì „ê³µ ë‹¤ì–‘ì„± ê°•í™”)"""
    global used_names, used_projects, used_companies, used_majors
    
    avoid_names = list(used_names)[-20:]
    avoid_projects = list(used_projects)[-20:]
    avoid_majors = list(used_majors)[-10:]

    # AIì—ê²Œ ë˜ì ¸ì¤„ ë‹¤ì–‘í•œ ë¹„ì „ê³µ ë¦¬ìŠ¤íŠ¸ (ì˜ˆì‹œ)
    major_samples = ["êµ­ì–´êµ­ë¬¸í•™", "ì² í•™", "ì‹¬ë¦¬í•™", "ì‚¬íšŒí•™", "ê²½ì œí•™", "ê²½ì˜í•™", "ì‹ ë¬¸ë°©ì†¡í•™", "ì •ì¹˜ì™¸êµí•™", "ë²•í•™", "í–‰ì •í•™", 
                     "ì˜ì–´ì˜ë¬¸í•™", "ì¤‘ì–´ì¤‘ë¬¸í•™", "ì¼ì–´ì¼ë¬¸í•™", "ì‚¬í•™", "ì§€ë¦¬í•™", "ì˜ìƒë””ìžì¸", "ì¡°ë¦¬ì™¸ì‹ê²½ì˜", "ì‹í’ˆì˜ì–‘í•™", 
                     "ê°„í˜¸í•™", "ë¬¼ë¦¬ì¹˜ë£Œí•™", "ì²´ìœ¡í•™", "ì‚¬íšŒë³µì§€í•™", "ë¬¸í—Œì •ë³´í•™", "ìœ ì•„êµìœ¡í•™", "ì‹¤ìš©ìŒì•…", "íšŒí™”", "ì¡°ì†Œ"]
    
    prompt = f"""
    ëŒ€í•œë¯¼êµ­ IT ê¸°ì—…ì— ì§€ì›í•˜ëŠ” {position} ì‹ ìž…/ê²½í—˜ìž 1ëª…ì˜ ìƒì„¸ ê°€ìƒ íŽ˜ë¥´ì†Œë‚˜ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.
    [í•„ìˆ˜ ì„œì‚¬] ë°˜ë“œì‹œ í•´ë‹¹ ì§ë¬´ì™€ ê´€ë ¨ ì—†ëŠ” ë¹„ì „ê³µìžë‚˜ ì§ë¬´ ì „í™˜ìž ì„œì‚¬ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”.
    
    [ì „ê³µ ë‹¤ì–‘ì„± ê°€ì´ë“œ] 
    - ì•„ëž˜ ì˜ˆì‹œ ë¦¬ìŠ¤íŠ¸ ì¤‘ í•˜ë‚˜ë¥¼ ì°¸ê³ í•˜ê±°ë‚˜, ì´ì™€ ë¹„ìŠ·í•œ ë¹„IT ê³„ì—´ ì „ê³µì„ ìžìœ ë¡­ê²Œ ì„ íƒí•˜ì„¸ìš”.
    - ì˜ˆì‹œ: {major_samples}
    - ì ˆëŒ€ í”¼í•´ì•¼ í•  ìµœê·¼ ì „ê³µ: {avoid_majors} (íŠ¹ížˆ 'ë¯¸ìˆ ì‚¬'ê°€ ë„ˆë¬´ ë§Žìœ¼ë‹ˆ ì ˆëŒ€ í”¼í•  ê²ƒ)

    [ì¤‘ìš”: ì ˆëŒ€ ì¤‘ë³µ ê¸ˆì§€ ê·œì¹™]
    1. ì´ë¦„: ì•„ëž˜ ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ì•„ì£¼ í¬ê·€í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
       (í”¼í•´ì•¼ í•  ì´ë¦„: {avoid_names})
    2. í”„ë¡œì íŠ¸: ì•„ëž˜ì™€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ë§¤ìš° êµ¬ì²´ì ì´ê³  ë…íŠ¹í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë§Œë“œì„¸ìš”.
       (í”¼í•´ì•¼ í•  í”„ë¡œì íŠ¸ í‚¤ì›Œë“œ: {avoid_projects})
    3. íšŒì‚¬: ëŒ€ê¸°ì—… ì™¸ì— ìœ ë§í•œ ìŠ¤íƒ€íŠ¸ì—…ì´ë‚˜ ê°€ìƒì˜ ê¸°ì—… ì´ë¦„ì„ ì°½ì¡°í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
    
    [ì‘ë‹µ í˜•ì‹]
    {{
        "header": {{
            "name": "ì´ë¦„",
            "target_role": "{position}",
            "target_company": "ì§€ì›ì´ìœ ê°€ ëª…í™•í•œ íšŒì‚¬ëª…",
            "major": "ì‹¤ì œ ì „ê³µ",
            "is_career_changer": true/false (ë¶ˆë¦¬ì–¸ê°’)
        }},
        "education": [{{ "school_name": "ëŒ€í•™êµëª…", "major": "ì „ê³µëª…", "status": "ì¡¸ì—…" }}],
        "projects": [
            {{
                "title": "ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê³ ìœ í•œ í”„ë¡œì íŠ¸ëª…",
                "description": "ìƒì„¸í•œ ê¸°ìˆ ì  ì„¤ëª…"
            }}
        ],
        "self_intro": [
            {{ "question": "ì§€ì›ë™ê¸°", "answer": "ìƒì„¸í•œ ë‹µë³€ ë‚´ìš©..." }}
        ]
    }}
    """
    
    for attempt in range(3):
        try:
            response = llm.invoke(prompt)
            
            # JSON ì¶”ì¶œ ë¡œì§ ê°•í™” (raw_decode ì‚¬ìš©)
            content = response.strip()
            start_idx = content.find('{')
            if start_idx == -1:
                logger.error("âŒ ì‘ë‹µì—ì„œ '{'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            try:
                decoder = json.JSONDecoder()
                persona, end_idx = decoder.raw_decode(content[start_idx:])
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSON ë””ì½”ë”© ì‹¤íŒ¨ (ì‹œë„ {attempt+1}): {e}")
                continue
            
            name = persona['header']['name']
            proj = persona['projects'][0]['title']
            comp = persona['header']['target_company']
            
            if name in used_names or proj in used_projects:
                logger.warning(f"ðŸ”„ ì¤‘ë³µ ë°œê²¬ ({name} ë˜ëŠ” {proj}), ìž¬ìƒì„± ì‹œë„ì¤‘... ({attempt+1}/3)")
                continue
                
            used_names.add(name)
            used_projects.add(proj)
            used_companies.add(comp)
            return persona
            
        except Exception as e:
            logger.error(f"âŒ íŽ˜ë¥´ì†Œë‚˜ íŒŒì‹± ì—ëŸ¬: {e}")
            continue
    return None

def main():
    logger.info("ðŸš€ ë²Œí¬ ìƒì„±ê¸° ì‹œìž‘ (Ollama + ë°±ì—… + ì¤‘ë³µë°©ì§€ ëª¨ë“œ)...")
    
    load_existing_data()
    
    try:
        llm = get_exaone_llm_ollama()
        retriever = get_question_retriever()
        embedder = get_embedding_generator()
        logger.info("âœ… ì—”ì§„ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    target_positions = ["ë°±ì—”ë“œê°œë°œìž", "í”„ë¡ íŠ¸ì—”ë“œê°œë°œìž", "ë°ì´í„°ë¶„ì„ê°€", "AIê°œë°œìž", "PLÂ·PMÂ·PO"]
    
    target_count = 50
    if len(sys.argv) > 1:
        try: target_count = int(sys.argv[1])
        except: pass
            
    BATCH_SIZE = 50
    total_generated = 0
    all_results = []
    
    backup_dir = os.path.join(root_dir, "generated_data")
    os.makedirs(backup_dir, exist_ok=True)

    while total_generated < target_count:
        current_batch = min(BATCH_SIZE, target_count - total_generated)
        logger.info(f"\nðŸ“¢ [BATCH START] ëª©í‘œ: {current_batch}ëª… (ëˆ„ì : {total_generated}/{target_count})")
        
        for i in range(current_batch):
            pos = random.choice(target_positions)
            logger.info(f"\n{'#'*60}")
            logger.info(f"[{total_generated + i + 1}/{target_count}] ðŸ‘· {pos} ìƒì„± ì¤‘...")
            
            persona = generate_persona(llm, pos)
            if not persona: continue
            
            name = persona['header']['name']
            role = persona['header']['target_role']
            is_transition = persona['header'].get('is_career_changer', False)
            
            # 1. DB ì €ìž¥ (ìœ ì €, ì´ë ¥ì„œ, ì¸í„°ë·°)
            interview_id = None
            try:
                with Session(engine) as session:
                    user = User(
                        username=f"fake_{name}_{random.randint(1000,9999)}", 
                        email=f"{name}{random.randint(1000,9999)}@test.com", 
                        full_name=name, 
                        password_hash="dummy"
                    )
                    session.add(user); session.commit(); session.refresh(user)
                    
                    resume = Resume(
                        candidate_id=user.id, file_name=f"virtual_{name}.pdf", 
                        file_path="VIRTUAL", file_size=0, 
                        target_position=role, structured_data=persona, processing_status="completed"
                    )
                    session.add(resume); session.commit(); session.refresh(resume)
                    
                    interview = Interview(
                        candidate_id=user.id, resume_id=resume.id, 
                        position=role, status=InterviewStatus.COMPLETED
                    )
                    session.add(interview); session.commit(); session.refresh(interview)
                    interview_id = interview.id
                logger.info(f"âœ… ê°€ìƒ ì§€ì›ìž [{name}] DB ë“±ë¡ ì™„ë£Œ (ì§ë¬´ì „í™˜: {is_transition})")
            except Exception as e:
                logger.error(f"âŒ DB ë“±ë¡ ì‹¤íŒ¨: {e}")
                continue

            # 2. ì§ˆë¬¸ ìƒì„± íŒŒì´í”„ë¼ì¸
            backup_candidate = { "persona": persona, "generated_questions": [] }
            try:
                # ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ
                stages = trans_scenario.INTERVIEW_STAGES if is_transition else std_scenario.INTERVIEW_STAGES
                
                chunks = chunk_resume(persona)
                chunk_texts = [c['text'] for c in chunks]
                chunk_embeddings = embedder.encode_batch(chunk_texts, is_query=False)
                
                last_primary_question = ""
                prompt_tpl = PromptTemplate.from_template(PROMPT_TEMPLATE)
                chain = prompt_tpl | llm | StrOutputParser()
    
                for stage in stages:
                    stage_name = stage['stage']
                    
                    # ìžê¸°ì†Œê°œ, ì§€ì›ë™ê¸°, ìµœì¢…ë°œì–¸ ë‹¨ê³„ëŠ” ì œì™¸ (ì‚¬ìš©ìž ìš”ì²­)
                    if stage_name in ['intro', 'motivation', 'final_statement']:
                        continue
                        
                    order = stage['order']
                    persona_major = persona['header'].get('major', 'ì´ì „ ì „ê³µ')
                    
                    if stage['type'] == 'template':
                        # í…œí”Œë¦¿ ì§ˆë¬¸ì€ ì¦‰ì‹œ ì €ìž¥
                        content = stage['template'].format(
                            candidate_name=name, 
                            target_role=role, 
                            major=persona_major
                        )
                        save_generated_question(interview_id, content, "behavioral", stage['stage'], "", position=role)
                        backup_candidate["generated_questions"].append({"order": order, "stage": stage['stage'], "content": content})
                        continue
                    
                    stage_name = stage['stage']
                    stage_type = stage['type']
                    # ê°€ì´ë“œì— {major}ê°€ í¬í•¨ë˜ì–´ ìžˆì„ ê²½ìš° ì‹¤ì œ ì „ê³µìœ¼ë¡œ ì¹˜í™˜
                    guide = stage.get('guide', '').format(major=persona_major, target_role=role)
                    
                    final_content = ""
                    try:
                        if stage_type == "ai":
                            query_vec = embedder.encode_query(f"{stage_name} {guide}")
                            scores = [np.dot(query_vec, emb) / (np.linalg.norm(query_vec) * np.linalg.norm(emb)) for emb in chunk_embeddings]
                            top_idx = np.argsort(scores)[::-1][:3]
                            ctx = "\n".join([chunk_texts[idx] for idx in top_idx])
                            
                            db_qs = retriever.find_relevant_questions(text_context=ctx, question_type=stage_name, top_k=5)
                            db_qs_str = "\n".join([f"{idx+1}. {q.content}" for idx, q in enumerate(db_qs)])
                            
                            final_content = chain.invoke({
                                "stage_name": stage_name, "guide": guide, "name": name, 
                                "major": persona_major,
                                "context": ctx, "db_questions": db_qs_str,
                                "position": pos, "project_title": persona['projects'][0]['title']
                            })
                            last_primary_question = final_content
                            
                        elif stage_type == "followup":
                            final_content = chain.invoke({
                                "stage_name": stage_name, "guide": guide, "name": name,
                                "major": persona_major,
                                "context": f"ì´ì „ ì§ˆë¬¸: {last_primary_question}\n(ë©´ì ‘ê´€ìœ¼ë¡œì„œ ê¼¬ë¦¬ì§ˆë¬¸ì„ ë˜ì§€ëŠ” ìƒí™©)", 
                                "db_questions": f"{persona_major} ì „ê³µìžê°€ ê¸°ìˆ ì  ê¹Šì´ë¥¼ ì¦ëª…í•´ì•¼ í•˜ëŠ” ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„±",
                                "position": pos, "project_title": persona['projects'][0]['title']
                            })
    
                        if final_content:
                            save_generated_question(interview_id, final_content, "technical", stage_name, guide, position=role, company=persona['header'].get('target_company'))
                            backup_candidate["generated_questions"].append({"order": order, "stage": stage_name, "content": final_content, "guide": guide})
                            logger.info(f"  [{order:02d}] {stage_name} ìƒì„± ì™„ë£Œ")
                    except Exception as e:
                        logger.error(f"âŒ ì§ˆë¬¸ ìƒì„± ì—ëŸ¬ (Stage: {stage_name}): {e}")
                
                # ê²°ê³¼ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ì— ì €ìž¥ í›„ ë§ˆì§€ë§‰ì— í•œêº¼ë²ˆì— ì—‘ìŠ¤í¬íŠ¸)
                all_results.append(backup_candidate)
            except Exception as e:
                logger.error(f"âŒ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì—ëŸ¬: {e}")
                
        total_generated += current_batch
        if total_generated < target_count:
            logger.info("ðŸ’¤ ë°°ì¹˜ ì™„ë£Œ. 10ì´ˆ ëŒ€ê¸° í›„ ìž¬ê°œ...")
            time.sleep(10)

    # ìµœì¢… ì—‘ìŠ¤í¬íŠ¸
    if all_results:
        final_file = f"bulk_final_{int(time.time())}.json"
        with open(os.path.join(backup_dir, final_file), 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ¨ [CSV/JSON Export] {len(all_results)}ëª…ì˜ ë°ì´í„° ìµœì¢… ë°±ì—… ì™„ë£Œ: {final_file}")

if __name__ == "__main__":
    main()
