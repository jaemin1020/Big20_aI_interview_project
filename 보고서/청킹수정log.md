# [보고서] 소형 LLM 기반 이력서 무손실 데이터 추출 및 정제 전략

## 1. 문제 정의 (Problem Statement)

* **요약 편향(Summarization Bias)** : 3B 규모의 소형 모델은 긴 문장을 처리할 때 효율성을 위해 임의로 내용을 요약하거나 문장을 생략하는 경향이 있음.
* **개체명 오인식(Entity Misidentification)** : "기관명, 지역" 형태의 텍스트에서 지역(주소) 정보를 기관명의 일부로 오인하여 추출함.
* **멀티태스크 한계** : JSON 구조화, 데이터 정제, 장문 복사 등 여러 명령을 한 번에 수행할 때 정확도가 급격히 저하됨.

## 2. 단계별 해결 전략 (Step-by-Step Solutions)

### 1단계: Pydantic Schema를 통한 제약 조건 명시

단순히 필드명만 정의하는 것이 아니라, `Field`의 `description` 속성을 활용해 모델이 데이터를 추출할 때 참조해야 할 '가이드라인'을 데이터 구조 자체에 심는 전략입니다.

* **적용** : `organization` 필드에 "지역명 절대 제외" 명시, `self_introduction` 필드를 세부 항목별로 분리하여 모델의 인지 부하 분산.

### 2단계: 프롬프트 엔지니어링 고도화 (페르소나 및 부정 지시어)

모델의 동작 모드를 '분석가'에서 '전사(Transcribe) 로봇'으로 전환하여 창의적인 요약을 억제했습니다.

* **Persona 부여** : "당신은 무손실 데이터 전사 장치입니다"라는 역할 부여.
* **Negative Prompt** : "잘못된 예(안랩, 판교)"와 "옳은 예(안랩)"를 명시하여 반복되는 실수를 방지.
* **검증 명령** : "마지막 문장의 마침표가 누락되면 분석 실패로 간주한다"는 강력한 제약 조건 추가.

### 3단계: 기술 파라미터 최적화

LLM의 출력 일관성과 충분한 출력 길이를 보장하기 위한 설정 값을 조정했습니다.

* **Temperature (0.0)** : 결과의 무작위성을 배제하고 가장 확률이 높은(원문과 일치하는) 토큰을 선택하도록 고정.
* **Max Tokens (4096)** : 장문의 자기소개서가 물리적인 길이 제한으로 인해 잘리는 현상을 방지.

### 4단계: 하이브리드 포스트 프로세싱 (Hybrid Post-processing)

LLM이 완벽하게 처리하지 못하는 정형 패턴(주소 제거 등)을 파이썬의 정규표현식(Regex)으로 보완하는 하이브리드 방식을 채택했습니다.

* **결과** : LLM이 "안랩, 판교"로 추출하더라도 코드가 `,`, `(`, `서울/판교` 등의 키워드를 기준으로 뒷부분을 강제 삭제하여 데이터 무결성 확보.

### 5단계: 태스크 분리(Task Splitting) 파이프라인 구축 (최종 솔루션)

하나의 프롬프트에서 모든 데이터를 추출하던 방식을 버리고, 성격이 다른 데이터를 두 단계로 나누어 처리하는 파이프라인을 구축했습니다.

* **Step 1 (Metadata Extraction)** : 이름, 직무, 경력 등 짧고 정형화된 정보 추출 및 주소 정제.
* **Step 2 (Verbatim Transcription)** : 자기소개서 등 긴 텍스트만 '복사'하는 작업에 집중.
* **효과** : 모델의 집중력을 분산시키지 않아 3B 모델에서도 대형 모델 수준의 무손실 추출 성공.

## 3. 결론

소형 모델을 활용한 RAG(Retrieval-Augmented Generation) 시스템이나 데이터 파이프라인 구축 시, 모델의 크기에 따른 성능 한계를 **프롬프트 분할**과 **코드 기반 후처리**로 보완하는 것이 비용 효율성 및 정확도 측면에서 가장 효과적인 전략임을 확인하였습니다.


**[기술적 디테일: 태스크 독립화]**

* **문제** : 단일 체인(Single Chain) 구성 시 LLM이 특정 태스크(필사)에 과몰입하여 다른 태스크(정보 추출)를 방기함.
* **해결** : 추출(Extraction)과 전사(Transcription)를 별도의 LLM 호출로 분리.
* **병합(Merging)** : 모델의 불안정한 병합 능력을 믿지 않고, Python의 Dictionary 자료구조를 이용해 물리적으로 데이터를 결합하여 데이터 손실률 0% 달성.


**[6단계: 텍스트 정규화 (Text Normalization)]**

* **문제** : PDF 추출 과정에서 발생하는 물리적 줄바꿈 기호(`\n`)가 최종 데이터의 가독성을 해치고, 향후 RAG 시스템의 임베딩 성능을 저하시킬 우려가 있음.
* **해결** : 추출된 JSON 데이터를 파이썬의 `re`(정규표현식) 모듈을 사용하여 후처리. 모든 `\n`을 공백으로 치환하고 연속된 공백을 단일화하는 정규화 로직을 파이프라인 마지막 단계에 배치하여 데이터 품질을 극대화함.
