"""
BGE-M3 ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
- ë‹¤êµ­ì–´ ì„ë² ë”© ìƒì„±
- ë²¡í„° ê²€ìƒ‰
- ì„±ëŠ¥ ë¹„êµ
"""

from FlagEmbedding import BGEM3FlagModel
import numpy as np
from typing import List, Dict, Any
import time

class BGEM3Embedder:
    """BGE-M3 ì„ë² ë”© ìƒì„±ê¸°"""

    def __init__(self, use_fp16: bool = True):
        """
        Args:
            use_fp16: FP16 ì‚¬ìš© ì—¬ë¶€ (ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ)
        """
        print("ğŸ”„ BGE-M3 ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_time = time.time()

        self.model = BGEM3FlagModel(
            'BAAI/bge-m3',
            use_fp16=use_fp16  # GPU ì‚¬ìš© ì‹œ True ê¶Œì¥
        )

        load_time = time.time() - start_time
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ({load_time:.2f}ì´ˆ)")
        print(f"ğŸ“¦ ëª¨ë¸: BAAI/bge-m3")
        print(f"ğŸ“Š ì„ë² ë”© ì°¨ì›: 1024")
        print(f"ğŸŒ ì§€ì› ì–¸ì–´: 100+ (í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´ ë“±)")

    def encode(
        self,
        texts: List[str],
        batch_size: int = 12,
        max_length: int = 512
    ) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´

        Returns:
            ì„ë² ë”© ë²¡í„° ë°°ì—´ (shape: [len(texts), 1024])
        """
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            max_length=max_length
        )['dense_vecs']

        return embeddings

    def encode_queries(
        self,
        queries: List[str],
        batch_size: int = 12
    ) -> np.ndarray:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© (ê²€ìƒ‰ ìµœì í™”)

        Args:
            queries: ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
        """
        return self.encode(queries, batch_size=batch_size)

    def encode_corpus(
        self,
        corpus: List[str],
        batch_size: int = 12
    ) -> np.ndarray:
        """
        ë¬¸ì„œ ì½”í¼ìŠ¤ ì„ë² ë”©

        Args:
            corpus: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            ë¬¸ì„œ ì„ë² ë”© ë²¡í„°
        """
        return self.encode(corpus, batch_size=batch_size)

    @staticmethod
    def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

    def search(
        self,
        query: str,
        corpus: List[str],
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ë²¡í„° ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            corpus: ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            top_k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ (ë¬¸ì„œ + ìœ ì‚¬ë„)
        """
        # ì„ë² ë”© ìƒì„±
        query_emb = self.encode([query])[0]
        corpus_embs = self.encode(corpus)

        # ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_emb in enumerate(corpus_embs):
            sim = self.cosine_similarity(query_emb, doc_emb)
            similarities.append({
                'index': i,
                'text': corpus[i],
                'similarity': float(sim)
            })

        # ì •ë ¬ ë° ìƒìœ„ kê°œ ë°˜í™˜
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:top_k]


# ==================== ì‚¬ìš© ì˜ˆì‹œ ====================

def example_basic_usage():
    """ê¸°ë³¸ ì‚¬ìš©ë²•"""
    print("\n" + "="*60)
    print("ğŸ“ ê¸°ë³¸ ì‚¬ìš©ë²•")
    print("="*60)

    embedder = BGEM3Embedder(use_fp16=True)

    # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
    texts = [
        "Pythonì—ì„œ GILì´ ë¬´ì—‡ì¸ê°€ìš”?",
        "FastAPIì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "Dockerì™€ Kubernetesì˜ ì°¨ì´ì ì€?"
    ]

    embeddings = embedder.encode(texts)
    print(f"\nâœ… {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì™„ë£Œ")
    print(f"   ì„ë² ë”© shape: {embeddings.shape}")
    print(f"   ì²« ë²ˆì§¸ ë²¡í„° ìƒ˜í”Œ: {embeddings[0][:5]}...")


def example_search():
    """ë²¡í„° ê²€ìƒ‰ ì˜ˆì‹œ"""
    print("\n" + "="*60)
    print("ğŸ” ë²¡í„° ê²€ìƒ‰ ì˜ˆì‹œ")
    print("="*60)

    embedder = BGEM3Embedder(use_fp16=True)

    # ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ
    corpus = [
        "Pythonì˜ GIL(Global Interpreter Lock)ì€ í•œ ë²ˆì— í•˜ë‚˜ì˜ ìŠ¤ë ˆë“œë§Œ Python ë°”ì´íŠ¸ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ì œí•œí•˜ëŠ” ë®¤í…ìŠ¤ì…ë‹ˆë‹¤.",
        "FastAPIëŠ” Python 3.6+ ê¸°ë°˜ì˜ í˜„ëŒ€ì ì´ê³  ë¹ ë¥¸ ì›¹ í”„ë ˆì„ì›Œí¬ë¡œ, ìë™ ë¬¸ì„œí™”ì™€ íƒ€ì… íŒíŒ…ì„ ì§€ì›í•©ë‹ˆë‹¤.",
        "DockerëŠ” ì»¨í…Œì´ë„ˆ í”Œë«í¼ì´ê³ , KubernetesëŠ” ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ì…ë‹ˆë‹¤.",
        "ReactëŠ” ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ê¸° ìœ„í•œ JavaScript ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.",
        "PostgreSQLì€ ê°•ë ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œì…ë‹ˆë‹¤."
    ]

    # ê²€ìƒ‰ ì¿¼ë¦¬
    query = "íŒŒì´ì¬ ë©€í‹°ìŠ¤ë ˆë”© ì œì•½ì‚¬í•­"

    print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
    print("\nê²€ìƒ‰ ê²°ê³¼:")

    results = embedder.search(query, corpus, top_k=3)

    for i, result in enumerate(results, 1):
        print(f"\n{i}. ìœ ì‚¬ë„: {result['similarity']:.4f}")
        print(f"   ë‚´ìš©: {result['text'][:80]}...")


def example_multilingual():
    """ë‹¤êµ­ì–´ ì§€ì› ì˜ˆì‹œ"""
    print("\n" + "="*60)
    print("ğŸŒ ë‹¤êµ­ì–´ ì§€ì› ì˜ˆì‹œ")
    print("="*60)

    embedder = BGEM3Embedder(use_fp16=True)

    # ë‹¤êµ­ì–´ ë¬¸ì„œ
    corpus = [
        "Python is a high-level programming language.",  # ì˜ì–´
        "Pythonì€ ê³ ìˆ˜ì¤€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",  # í•œêµ­ì–´
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚",  # ì¤‘êµ­ì–´
        "Pythonã¯é«˜æ°´æº–ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚"  # ì¼ë³¸ì–´
    ]

    query = "íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°"

    print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
    print("\nê²€ìƒ‰ ê²°ê³¼ (ë‹¤êµ­ì–´ ë¬¸ì„œ):")

    results = embedder.search(query, corpus, top_k=4)

    for i, result in enumerate(results, 1):
        print(f"\n{i}. ìœ ì‚¬ë„: {result['similarity']:.4f}")
        print(f"   ë‚´ìš©: {result['text']}")


def example_performance_comparison():
    """ì„±ëŠ¥ ë¹„êµ (BGE-M3 vs ê¸°ì¡´ ëª¨ë¸)"""
    print("\n" + "="*60)
    print("âš¡ ì„±ëŠ¥ ë¹„êµ")
    print("="*60)

    from sentence_transformers import SentenceTransformer

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_texts = [
        "Pythonì—ì„œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ì–´ë–»ê²Œ êµ¬í˜„í•˜ë‚˜ìš”?",
        "REST APIì™€ GraphQLì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "Docker ì»¨í…Œì´ë„ˆì˜ ë„¤íŠ¸ì›Œí‚¹ì€ ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?"
    ] * 10  # 30ê°œ í…ìŠ¤íŠ¸

    # BGE-M3
    print("\n1ï¸âƒ£ BGE-M3 ëª¨ë¸")
    start = time.time()
    bge_model = BGEM3Embedder(use_fp16=True)
    bge_embs = bge_model.encode(test_texts)
    bge_time = time.time() - start
    print(f"   ì²˜ë¦¬ ì‹œê°„: {bge_time:.2f}ì´ˆ")
    print(f"   ì„ë² ë”© ì°¨ì›: {bge_embs.shape[1]}")

    # ê¸°ì¡´ ëª¨ë¸ (ko-sroberta)
    print("\n2ï¸âƒ£ ko-sroberta-multitask ëª¨ë¸")
    start = time.time()
    sbert_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
    sbert_embs = sbert_model.encode(test_texts)
    sbert_time = time.time() - start
    print(f"   ì²˜ë¦¬ ì‹œê°„: {sbert_time:.2f}ì´ˆ")
    print(f"   ì„ë² ë”© ì°¨ì›: {sbert_embs.shape[1]}")

    print(f"\nğŸ“Š ì†ë„ ë¹„êµ: BGE-M3ê°€ {sbert_time/bge_time:.2f}ë°° {'ë¹ ë¦„' if bge_time < sbert_time else 'ëŠë¦¼'}")


if __name__ == "__main__":
    print("ğŸš€ BGE-M3 ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ")
    print("="*60)

    # 1. ê¸°ë³¸ ì‚¬ìš©ë²•
    example_basic_usage()

    # 2. ë²¡í„° ê²€ìƒ‰
    example_search()

    # 3. ë‹¤êµ­ì–´ ì§€ì›
    example_multilingual()

    # 4. ì„±ëŠ¥ ë¹„êµ
    # example_performance_comparison()  # ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬

    print("\n" + "="*60)
    print("âœ… ëª¨ë“  ì˜ˆì‹œ ì‹¤í–‰ ì™„ë£Œ!")
    print("="*60)
