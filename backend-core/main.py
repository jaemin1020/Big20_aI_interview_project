from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging
import os
<<<<<<< HEAD
import shutil
from pathlib import Path
from sqlalchemy import text
from pydantic import BaseModel

# DB ì„¤ì •
from database import init_db, get_session
# DB í…Œì´ë¸” ëª¨ë“ˆ ì„í¬íŠ¸
from models import (
    User, UserCreate, UserLogin, UserRole, Company,
    Interview, InterviewCreate, InterviewResponse, InterviewStatus,
    Question, QuestionCategory, QuestionDifficulty,
    Transcript, TranscriptCreate, Speaker,
    EvaluationReport, EvaluationReportResponse,
    Resume, ResumeChunk
)
# ì¸ì¦ ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
# ì¸ì¦ ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from auth import get_password_hash, verify_password, create_access_token, get_current_user
from utils.common import validate_email, validate_username  # ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€
=======

from database import init_db
# ë¼ìš°í„° ì„í¬íŠ¸
from routes.auth import router as auth_router
from routes.companies import router as companies_router
from routes.interviews import router as interviews_router
from routes.transcripts import router as transcripts_router
from routes.resumes import router as resumes_router
from routes.users import router as users_router
>>>>>>> 5fe6f7adb33f16443747dc01fc10ed12295552be

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("Backend-Core")

app = FastAPI(title="AI Interview Backend v2.0")

# DB ì´ˆê¸°í™”
@app.on_event("startup")
def on_startup():
    init_db()
    logger.info("âœ… Database initialized with new schema")

# CORS ì„¤ì •
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:3000,http://127.0.0.1:3000").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¼ìš°í„° ë“±ë¡
app.include_router(auth_router)       # /auth
app.include_router(companies_router)  # /companies
app.include_router(interviews_router) # /interviews
app.include_router(transcripts_router)# /transcripts
app.include_router(resumes_router)    # /api/resumes
app.include_router(users_router)      # /users

<<<<<<< HEAD
# Celery ì„¤ì •
celery_app = Celery("ai_worker", broker="redis://redis:6379/0", backend="redis://redis:6379/0")

# ==================== Auth Endpoints ====================
# íšŒì›ê°€ì…
@app.post("/register")
async def register(user_data: UserCreate, db: Session = Depends(get_session)):
    # 1. ìœ íš¨ì„± ê²€ì‚¬ (ê¸¸ì´ ë° í¬ë§·)
    if not validate_username(user_data.username):
        raise HTTPException(
            status_code=400, 
            detail="ì•„ì´ë””ëŠ” 4~12ìì˜ ì˜ë¬¸ ì†Œë¬¸ì, ìˆ«ì, ë°‘ì¤„(_)ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )
    
    if not validate_email(user_data.email):
        raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë©”ì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

    # 2. ì¤‘ë³µ í™•ì¸
    stmt = select(User).where(
        (User.username == user_data.username) | (User.email == user_data.email)
    )
    existing_user = db.exec(stmt).first()
    if existing_user:
        raise HTTPException(status_code=400, detail="Username or email already exists")
    
    # ìƒˆ ì‚¬ìš©ì ìƒì„±
    new_user = User(
        email=user_data.email,
        username=user_data.username,
        role=user_data.role,
        password_hash=get_password_hash(user_data.password),
        full_name=user_data.full_name
    )
    db.add(new_user)
    db.commit()
    db.refresh(new_user)
    
    logger.info(f"New user registered: {new_user.username} ({new_user.role})")
    return {"id": new_user.id, "username": new_user.username, "email": new_user.email}

# ë¡œê·¸ì¸
@app.post("/token")
async def login(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_session)):
    # ì‚¬ìš©ì ì¸ì¦
    stmt = select(User).where(User.username == form_data.username)
    user = db.exec(stmt).first()
    # ë¹„ë°€ë²ˆí˜¸ í™•ì¸
    if not user or not verify_password(form_data.password, user.password_hash):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password"
        )
    # í† í° ìƒì„±
    access_token = create_access_token(data={"sub": user.username})
    logger.info(f"User logged in: {user.username}")
    return {"access_token": access_token, "token_type": "bearer"}

# ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ
@app.get("/users/me")
async def read_users_me(current_user: User = Depends(get_current_user)):
    return current_user

# ==================== Interview Endpoints ====================
# ë©´ì ‘ ìƒì„±
@app.post("/interviews", response_model=InterviewResponse)
async def create_interview(
    interview_data: InterviewCreate,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """ë©´ì ‘ ì„¸ì…˜ ìƒì„± ë° ì§ˆë¬¸ ìƒì„±"""
    
    # 1. Interview ë ˆì½”ë“œ ìƒì„± (ìƒíƒœ: SCHEDULED)
    new_interview = Interview(
        candidate_id=current_user.id,
        position=interview_data.position,
        company_id=interview_data.company_id,
        status=InterviewStatus.SCHEDULED,
        scheduled_time=interview_data.scheduled_time,
        start_time=datetime.utcnow()
    )
    # DBì— ì €ì¥
    db.add(new_interview)
    db.commit()
    db.refresh(new_interview)
    
    logger.info(f"Interview created: ID={new_interview.id}, Position={new_interview.position}")
    
    # 2. AI ì§ˆë¬¸ ìƒì„±
    # Backendê°€ ì§ì ‘ LLMì„ ëŒë¦¬ì§€ ì•Šìœ¼ë¯€ë¡œ, Celery Taskë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    generated_questions = []
    
    try:
        logger.info("Requesting question generation from AI-Worker...")
        # Celery íƒœìŠ¤í¬ í˜¸ì¶œ (ìµœëŒ€ 90ì´ˆ ëŒ€ê¸° - ëª¨ë¸ ë¡œë”© ì‹œê°„ ê³ ë ¤)
        task = celery_app.send_task(
            "tasks.question_generator.generate_questions",
            args=[interview_data.position, new_interview.id, 5]
        )
        # ë™ê¸°ì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¼ (UXìƒ ì§ˆë¬¸ì´ ë°”ë¡œ í•„ìš”í•¨)
        generated_questions = task.get(timeout=180)
        logger.info(f"Received {len(generated_questions)} questions from AI-Worker")
        
    except Exception as e:
        logger.warning(f"AI-Worker question generation failed ({e}). Using fallback questions.")
        # ì‹¤íŒ¨ ì‹œ í´ë°± ì§ˆë¬¸ ìƒì„±
        generated_questions = [
            f"{interview_data.position} ì§ë¬´ì— ì§€ì›í•˜ê²Œ ëœ ë™ê¸°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "ê°€ì¥ ë„ì „ì ì´ì—ˆë˜ í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ ê·¸ ê³¼ì •ì—ì„œ ì–»ì€ êµí›ˆì€ ë¬´ì—‡ì¸ê°€ìš”?",
            f"{interview_data.position}ë¡œì„œ ë³¸ì¸ì˜ ê°€ì¥ í° ê°•ì ê³¼ ë³´ì™„í•˜ê³  ì‹¶ì€ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê°ˆë“± ìƒí™©ì„ í•´ê²°í–ˆë˜ êµ¬ì²´ì ì¸ ì‚¬ë¡€ê°€ ìˆë‹¤ë©´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "í–¥í›„ 5ë…„ ë’¤ì˜ ì»¤ë¦¬ì–´ ëª©í‘œëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        ]

    # 3. Questions ë° Transcript í…Œì´ë¸”ì— ì €ì¥
    try:
        for i, q_text in enumerate(generated_questions):
            # 3-1. ì§ˆë¬¸ ì€í–‰ì— ì €ì¥
            question = Question(
                content=q_text,
                category=QuestionCategory.TECHNICAL if i < 3 else QuestionCategory.BEHAVIORAL,
                difficulty=QuestionDifficulty.MEDIUM,
                rubric_json={
                    "criteria": ["êµ¬ì²´ì„±", "ì§ë¬´ ì í•©ì„±", "ë…¼ë¦¬ë ¥"], 
                    "weight": {"content": 0.5, "communication": 0.5}
                },
                position=interview_data.position
            )
            db.add(question)
            db.commit()
            db.refresh(question)
            
            # 3-2. Transcriptì— AI ë°œí™”ë¡œ ê¸°ë¡
            transcript = Transcript(
                interview_id=new_interview.id,
                speaker=Speaker.AI,
                text=q_text,
                question_id=question.id,
                order=i
            )
            db.add(transcript)
        
        # ë©´ì ‘ ìƒíƒœ ì—…ë°ì´íŠ¸: LIVE
        new_interview.status = InterviewStatus.LIVE
        db.add(new_interview)
        db.commit()
        db.refresh(new_interview)
        
    except Exception as e:
        logger.error(f"Failed to save questions: {e}")
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë©´ì ‘ ì„¸ì…˜ì€ ë°˜í™˜ (ë¹ˆ ì§ˆë¬¸ ëª©ë¡ì¼ ìˆ˜ ìˆìŒ)
    
    return InterviewResponse(
        id=new_interview.id,
        candidate_id=new_interview.candidate_id,
        position=new_interview.position,
        status=new_interview.status,
        start_time=new_interview.start_time,
        end_time=new_interview.end_time,
        overall_score=new_interview.overall_score
    )

# ==================== Question Endpoints ====================

# ë©´ì ‘ ì§ˆë¬¸ ì¡°íšŒ
@app.get("/interviews/{interview_id}/questions")
async def get_interview_questions(
    interview_id: int,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """ë©´ì ‘ì˜ ì§ˆë¬¸ ëª©ë¡ ì¡°íšŒ (Transcriptì—ì„œ AI ë°œí™”ë§Œ í•„í„°ë§)"""
    stmt = select(Transcript).where(
        Transcript.interview_id == interview_id,
        Transcript.speaker == Speaker.AI
    ).order_by(Transcript.order)
    
    transcripts = db.exec(stmt).all()
    
    return [
        {
            "id": t.question_id,
            "content": t.text,
            "order": t.order,
            "timestamp": t.timestamp
        }
        for t in transcripts
    ]

# ==================== Transcript Endpoints ====================

# ëŒ€í™” ê¸°ë¡ ì €ì¥
@app.post("/transcripts")
async def create_transcript(
    transcript_data: TranscriptCreate,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """ì‹¤ì‹œê°„ ëŒ€í™” ê¸°ë¡ ì €ì¥ (STT ê²°ê³¼)"""
    
    transcript = Transcript(
        interview_id=transcript_data.interview_id,
        speaker=transcript_data.speaker,
        text=transcript_data.text,
        question_id=transcript_data.question_id
    )
    db.add(transcript)
    db.commit()
    db.refresh(transcript)
    
    logger.info(f"Transcript saved: Interview={transcript.interview_id}, Speaker={transcript.speaker}")
    
    # ì‚¬ìš©ì ë‹µë³€ì¸ ê²½ìš° AI í‰ê°€ ìš”ì²­
    if transcript.speaker == Speaker.USER:
        # í•´ë‹¹ ì§ˆë¬¸ ì¡°íšŒ
        question = db.get(Question, transcript.question_id)
        if question:
            celery_app.send_task(
                "tasks.evaluator.analyze_answer",
                args=[
                    transcript.id,
                    question.content,
                    transcript.text,
                    question.rubric_json,
                    question.id  # ì§ˆë¬¸ ID ì¶”ê°€ (í‰ê·  ì ìˆ˜ ì—…ë°ì´íŠ¸ìš©)
                ]
            )
            logger.info(f"Evaluation task sent for transcript {transcript.id}")
    
    return {"id": transcript.id, "status": "saved"}

# ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
@app.get("/interviews/{interview_id}/transcripts")
async def get_interview_transcripts(
    interview_id: int,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """ë©´ì ‘ì˜ ì „ì²´ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
    stmt = select(Transcript).where(
        Transcript.interview_id == interview_id
    ).order_by(Transcript.timestamp)
    
    transcripts = db.exec(stmt).all()
    
    return [
        {
            "id": t.id,
            "speaker": t.speaker,
            "text": t.text,
            "timestamp": t.timestamp,
            "sentiment_score": t.sentiment_score,
            "emotion": t.emotion
        }
        for t in transcripts
    ]

# ==================== Evaluation Endpoints ====================

# ë©´ì ‘ ì™„ë£Œ ì²˜ë¦¬ ë° ìµœì¢… í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
@app.post("/interviews/{interview_id}/complete")
async def complete_interview(
    interview_id: int,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """ë©´ì ‘ ì¢…ë£Œ ë° ìµœì¢… í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    interview = db.get(Interview, interview_id)
    if not interview:
        raise HTTPException(status_code=404, detail="Interview not found")
    
    # ë©´ì ‘ ì¢…ë£Œ ì²˜ë¦¬
    interview.status = InterviewStatus.COMPLETED
    interview.end_time = datetime.utcnow()
    db.add(interview)
    db.commit()
    
    # í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± íƒœìŠ¤í¬ ì „ë‹¬
    celery_app.send_task(
        "tasks.evaluator.generate_final_report",
        args=[interview_id]
    )
    
    logger.info(f"Interview {interview_id} completed. Report generation started.")
    return {"status": "completed", "interview_id": interview_id}

@app.get("/interviews/{interview_id}/report", response_model=EvaluationReportResponse)
async def get_evaluation_report(
    interview_id: int,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """í‰ê°€ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
    stmt = select(EvaluationReport).where(
        EvaluationReport.interview_id == interview_id
    )
    report = db.exec(stmt).first()
    
    if not report:
        raise HTTPException(status_code=404, detail="Report not yet available")
    
    return report

# ==================== Resume Endpoints ====================

# ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ì„¤ì •
UPLOAD_DIR = Path("./uploads/resumes")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

@app.post("/resumes/upload")
async def upload_resume(
    file: UploadFile = File(...),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """ì´ë ¥ì„œ íŒŒì¼ ì—…ë¡œë“œ (PDF, DOC, DOCX)"""
    
    # íŒŒì¼ í™•ì¥ì ê²€ì¦
    allowed_extensions = [".pdf", ".doc", ".docx"]
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"Unsupported file type. Allowed: {', '.join(allowed_extensions)}"
        )
    
    # íŒŒì¼ ì €ì¥ ê²½ë¡œ ìƒì„± (candidate_id_timestamp_filename)
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    safe_filename = f"{current_user.id}_{timestamp}_{file.filename}"
    file_path = UPLOAD_DIR / safe_filename
    
    try:
        # íŒŒì¼ ì €ì¥
        with file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Resume ë ˆì½”ë“œ ìƒì„±
        new_resume = Resume(
            candidate_id=current_user.id,
            file_name=file.filename,
            file_path=str(file_path),
            file_size=file_path.stat().st_size,
            processing_status="pending"
        )
        db.add(new_resume)
        db.commit()
        db.refresh(new_resume)
        
        logger.info(f"Resume uploaded: ID={new_resume.id}, User={current_user.username}, File={file.filename}")
        
        # Celery íƒœìŠ¤í¬ë¡œ ì´ë ¥ì„œ íŒŒì‹± ë° êµ¬ì¡°í™” ì‘ì—… ì „ë‹¬
        celery_app.send_task(
            "parse_resume_pdf",
            args=[new_resume.id, str(file_path)]
        )
        logger.info(f"Resume parsing task sent for ID={new_resume.id}")
        
        return {
            "id": new_resume.id,
            "file_name": new_resume.file_name,
            "file_size": new_resume.file_size,
            "status": "uploaded",
            "message": "Resume uploaded successfully. Processing will begin shortly."
        }
        
    except Exception as e:
        logger.error(f"Resume upload failed: {e}")
        # ì‹¤íŒ¨ ì‹œ íŒŒì¼ ì‚­ì œ
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail="File upload failed")


# ğŸ§ª í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸ë“¤ (ì¸ì¦ ë¶ˆí•„ìš”)
# ì£¼ì˜: êµ¬ì²´ì ì¸ ê²½ë¡œë¥¼ ë¨¼ì € ì •ì˜í•´ì•¼ FastAPI ë¼ìš°íŒ…ì´ ì œëŒ€ë¡œ ì‘ë™í•¨

# ğŸ§ª í…ŒìŠ¤íŠ¸ìš©: ì¸ì¦ ì—†ëŠ” ì´ë ¥ì„œ ìƒíƒœ ì¡°íšŒ
@app.get("/test/resumes/{resume_id}")
async def test_get_resume_status(
    resume_id: int,
    db: Session = Depends(get_session)
):
    """
    í…ŒìŠ¤íŠ¸ìš© ì´ë ¥ì„œ ìƒíƒœ ì¡°íšŒ (ì¸ì¦ ë¶ˆí•„ìš”)
    
    - ì„ë² ë”© ì²˜ë¦¬ ìƒíƒœ ë° ì²­í¬ ì •ë³´ í™•ì¸
    """
    return {"message": "Endpoint is ALIVE", "id": resume_id}


# ğŸ§ª í…ŒìŠ¤íŠ¸ìš©: ì¸ì¦ ì—†ëŠ” ì´ë ¥ì„œ ì—…ë¡œë“œ (ê°œë°œ/ë””ë²„ê¹…ìš©)
@app.post("/test/upload-resume")
async def test_upload_resume(
    file: UploadFile = File(...),
    db: Session = Depends(get_session)
):
    """
    í…ŒìŠ¤íŠ¸ìš© ì´ë ¥ì„œ ì—…ë¡œë“œ (ì¸ì¦ ë¶ˆí•„ìš”)
    
    - ê°œë°œ ë° ë””ë²„ê¹… ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©
    - ì„ë² ë”© ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°”ë¡œ í™•ì¸ ê°€ëŠ¥
    """
    # íŒŒì¼ ê²€ì¦
    if not file.filename.lower().endswith(('.pdf', '.doc', '.docx')):
        raise HTTPException(status_code=400, detail="Only PDF, DOC, DOCX files are allowed")
    
    # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ìê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
    from sqlmodel import select
    stmt = select(User).where(User.username == "test_user")
    test_user = db.exec(stmt).first()
    
    if not test_user:
        from auth import get_password_hash
        test_user = User(
            username="test_user",
            email="test@example.com",
            password_hash=get_password_hash("test1234"),
            full_name="í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì",
            role=UserRole.CANDIDATE
        )
        db.add(test_user)
        db.commit()
        db.refresh(test_user)
        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì‚¬ìš©ì ìƒì„± ì™„ë£Œ (ID: {test_user.id})")
    
    test_user_id = test_user.id
    
    try:
        # íŒŒì¼ ì €ì¥
        upload_dir = Path("./uploads/resumes")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = f"{timestamp}_{file.filename}"
        file_path = upload_dir / safe_filename
        
        with file_path.open("wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        file_size = file_path.stat().st_size
        
        # Resume ë ˆì½”ë“œ ìƒì„±
        resume = Resume(
            candidate_id=test_user_id,
            file_name=file.filename,
            file_path=str(file_path),
            file_size=file_size,
            processing_status="pending"
        )
        
        db.add(resume)
        db.commit()
        db.refresh(resume)
        
        logger.info(f"âœ… [TEST] Resume uploaded: {resume.id} by test_user")
        
        # Celery Task ì „ì†¡ (ai-workerë¡œ ì „ë‹¬)
        task = celery_app.send_task(
            "parse_resume_pdf",  # Workerì— ë“±ë¡ëœ task ì´ë¦„
            args=[resume.id, str(file_path)]
        )
        
        return {
            "message": "âœ… í…ŒìŠ¤íŠ¸ ì—…ë¡œë“œ ì„±ê³µ! ì„ë² ë”© ì²˜ë¦¬ ì¤‘...",
            "resume_id": resume.id,
            "file_name": file.filename,
            "file_size": file_size,
            "task_id": task.id,
            "status_check_url": f"/test/resumes/{resume.id}",
            "note": "âš ï¸ ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í…ŒìŠ¤íŠ¸ìš©ì…ë‹ˆë‹¤. ìš´ì˜ í™˜ê²½ì—ì„œëŠ” /resumes/uploadë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
        }
        
    except Exception as e:
        logger.error(f"Test resume upload failed: {e}")
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


# ğŸ§ª í…ŒìŠ¤íŠ¸ìš©: ì¸ì¦ ì—†ëŠ” ì´ë ¥ì„œ ê²€ìƒ‰
@app.post("/test/resumes/search")
async def test_search_resumes(
    query: str,
    top_k: int = 10,
    min_score: float = 0.5,
    db: Session = Depends(get_session)
):
    """
    í…ŒìŠ¤íŠ¸ìš© ì´ë ¥ì„œ ê²€ìƒ‰ (ì¸ì¦ ë¶ˆí•„ìš”)
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "Python ë°±ì—”ë“œ ê°œë°œì")
        top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸: 10)
        min_score: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ (0~1, ê¸°ë³¸: 0.5)
    """
    logger.info(f"ğŸ” [TEST] Resume search: query='{query}', top_k={top_k}")
    
    try:
        # 1. ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (Celery Task ì‚¬ìš©)
        task = celery_app.send_task(
            "generate_query_embedding",
            args=[query]
        )
        
        # ê²°ê³¼ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
        query_embedding = task.get(timeout=10)
        logger.info(f"âœ… Query embedding generated (dim: {len(query_embedding)})")
        
    except Exception as e:
        logger.error(f"âŒ Failed to generate query embedding: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to generate embedding: {str(e)}"
        )
    
    # 2. pgvectorë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰
    sql_query = text("""
        SELECT 
            rc.id as chunk_id,
            rc.resume_id,
            rc.content,
            rc.chunk_index,
            1 - (rc.embedding <=> CAST(:query_embedding AS vector)) as similarity_score,
            r.file_name,
            r.candidate_id,
            u.full_name as candidate_name,
            u.email as candidate_email
        FROM resume_chunks rc
        JOIN resumes r ON rc.resume_id = r.id
        JOIN users u ON r.candidate_id = u.id
        WHERE 
            r.processing_status = 'completed'
            AND rc.embedding IS NOT NULL
            AND 1 - (rc.embedding <=> CAST(:query_embedding AS vector)) >= :min_score
        ORDER BY rc.embedding <=> CAST(:query_embedding AS vector)
        LIMIT :top_k
    """)
    
    try:
        result = db.execute(
            sql_query,
            {
                "query_embedding": str(query_embedding),
                "min_score": min_score,
                "top_k": top_k
            }
        )
        
        chunks = result.fetchall()
        logger.info(f"ğŸ“Š Found {len(chunks)} matching chunks")
        
    except Exception as e:
        logger.error(f"âŒ Database search failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Search failed: {str(e)}"
        )
    
    # 3. Resumeë³„ë¡œ ê·¸ë£¹í™” (ì¤‘ë³µ ì œê±°)
    resume_map = {}
    for chunk in chunks:
        resume_id = chunk.resume_id
        
        if resume_id not in resume_map:
            resume_map[resume_id] = {
                "resume_id": resume_id,
                "file_name": chunk.file_name,
                "candidate_name": chunk.candidate_name,
                "candidate_email": chunk.candidate_email,
                "max_similarity": float(chunk.similarity_score),
                "matched_chunks": []
            }
        
        resume_map[resume_id]["matched_chunks"].append({
            "chunk_index": chunk.chunk_index,
            "content": chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content,
            "similarity_score": float(chunk.similarity_score)
        })
        
        # ìµœê³  ìœ ì‚¬ë„ ì—…ë°ì´íŠ¸
        if chunk.similarity_score > resume_map[resume_id]["max_similarity"]:
            resume_map[resume_id]["max_similarity"] = float(chunk.similarity_score)
    
    # 4. ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    results = sorted(
        resume_map.values(),
        key=lambda x: x["max_similarity"],
        reverse=True
    )
    
    logger.info(f"âœ… [TEST] Found {len(results)} resumes matching query")
    
    return {
        "query": query,
        "total_results": len(results),
        "results": results,
        "note": "âš ï¸ ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” í…ŒìŠ¤íŠ¸ìš©ì…ë‹ˆë‹¤."
    }




# ì´ë ¥ì„œ ìƒíƒœ ì¡°íšŒ (ë‹¨ì¼)
@app.get("/resumes/{resume_id}")
async def get_resume_status(
    resume_id: int,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """ì´ë ¥ì„œ ì²˜ë¦¬ ìƒíƒœ ë° ì •ë³´ ì¡°íšŒ"""
    resume = db.get(Resume, resume_id)
    if not resume:
        raise HTTPException(status_code=404, detail="Resume not found")
    
    # ê¶Œí•œ í™•ì¸: ë³¸ì¸ ë˜ëŠ” recruiter/adminë§Œ ì¡°íšŒ ê°€ëŠ¥
    if resume.candidate_id != current_user.id and current_user.role not in ["recruiter", "admin"]:
        raise HTTPException(status_code=403, detail="Access denied")
    
    logger.info(f"Resume {resume_id} status requested by {current_user.username}")
    
    return {
        "id": resume.id,
        "file_name": resume.file_name,
        "file_size": resume.file_size,
        "processing_status": resume.processing_status,
        "uploaded_at": resume.uploaded_at,
        "processed_at": resume.processed_at,
        "has_embedding": resume.embedding is not None,
        "has_structured_data": resume.structured_data is not None,
        "structured_data": resume.structured_data if resume.structured_data else {}
    }

# ì´ë ¥ì„œ ëª©ë¡ ì¡°íšŒ
@app.get("/resumes")
async def get_user_resumes(
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """í˜„ì¬ ì‚¬ìš©ìì˜ ì´ë ¥ì„œ ëª©ë¡ ì¡°íšŒ"""
    stmt = select(Resume).where(
        Resume.candidate_id == current_user.id
    ).order_by(Resume.uploaded_at.desc())
    
    resumes = db.exec(stmt).all()
    
    logger.info(f"Resume list requested by {current_user.username}: {len(resumes)} resumes")
    
    return [
        {
            "id": r.id,
            "file_name": r.file_name,
            "file_size": r.file_size,
            "processing_status": r.processing_status,
            "uploaded_at": r.uploaded_at,
            "processed_at": r.processed_at,
            "has_embedding": r.embedding is not None
        }
        for r in resumes
    ]


# ==================== Resume Search Endpoints (Phase 2) ====================

class ResumeSearchRequest(BaseModel):
    """ì´ë ¥ì„œ ê²€ìƒ‰ ìš”ì²­"""
    query: str
    top_k: int = 10
    min_score: float = 0.5


@app.post("/resumes/search")
async def search_resumes(
    request: ResumeSearchRequest,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ì´ë ¥ì„œ ê²€ìƒ‰
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "Python ë°±ì—”ë“œ ê°œë°œì")
        top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸: 10)
        min_score: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ (0~1, ê¸°ë³¸: 0.5)
        
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ìˆœ ì •ë ¬)
    """
    logger.info(f"ğŸ” Resume search: query='{request.query}', top_k={request.top_k}, user={current_user.id}")
    
    try:
        # 1. ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (Celery Task ì‚¬ìš©)
        task = celery_app.send_task(
            "generate_query_embedding",
            args=[request.query]
        )
        
        # ê²°ê³¼ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
        query_embedding = task.get(timeout=10)
        logger.info(f"âœ… Query embedding generated (dim: {len(query_embedding)})")
        
    except Exception as e:
        logger.error(f"âŒ Failed to generate query embedding: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to generate embedding: {str(e)}"
        )
    
    # 2. pgvectorë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰
    # <=> ì—°ì‚°ì: ì½”ì‚¬ì¸ ê±°ë¦¬ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
    # 1 - ì½”ì‚¬ì¸ ê±°ë¦¬ = ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    sql_query = text("""
        SELECT 
            rc.id as chunk_id,
            rc.resume_id,
            rc.content,
            rc.chunk_index,
            1 - (rc.embedding <=> CAST(:query_embedding AS vector)) as similarity_score,
            r.file_name,
            r.candidate_id,
            u.full_name as candidate_name,
            u.email as candidate_email
        FROM resume_chunks rc
        JOIN resumes r ON rc.resume_id = r.id
        JOIN users u ON r.candidate_id = u.id
        WHERE 
            r.processing_status = 'completed'
            AND rc.embedding IS NOT NULL
            AND 1 - (rc.embedding <=> CAST(:query_embedding AS vector)) >= :min_score
        ORDER BY rc.embedding <=> CAST(:query_embedding AS vector)
        LIMIT :top_k
    """)
    
    try:
        result = db.execute(
            sql_query,
            {
                "query_embedding": str(query_embedding),
                "min_score": request.min_score,
                "top_k": request.top_k
            }
        )
        
        chunks = result.fetchall()
        logger.info(f"ğŸ“Š Found {len(chunks)} matching chunks")
        
    except Exception as e:
        logger.error(f"âŒ Database search failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Search failed: {str(e)}"
        )
    
    # 3. Resumeë³„ë¡œ ê·¸ë£¹í™” (ì¤‘ë³µ ì œê±°)
    resume_map = {}
    for chunk in chunks:
        resume_id = chunk.resume_id
        
        if resume_id not in resume_map:
            resume_map[resume_id] = {
                "resume_id": resume_id,
                "file_name": chunk.file_name,
                "candidate_name": chunk.candidate_name,
                "candidate_email": chunk.candidate_email,
                "max_similarity": float(chunk.similarity_score),
                "matched_chunks": []
            }
        
        resume_map[resume_id]["matched_chunks"].append({
            "chunk_index": chunk.chunk_index,
            "content": chunk.content[:200] + "..." if len(chunk.content) > 200 else chunk.content,
            "similarity_score": float(chunk.similarity_score)
        })
        
        # ìµœê³  ìœ ì‚¬ë„ ì—…ë°ì´íŠ¸
        if chunk.similarity_score > resume_map[resume_id]["max_similarity"]:
            resume_map[resume_id]["max_similarity"] = float(chunk.similarity_score)
    
    # 4. ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    results = sorted(
        resume_map.values(),
        key=lambda x: x["max_similarity"],
        reverse=True
    )
    
    logger.info(f"âœ… Found {len(results)} resumes matching query")
    
    return {
        "query": request.query,
        "total_results": len(results),
        "results": results
    }


# ==================== Interview Context & RAG Search (Phase 2) ====================

@app.get("/interviews/{interview_id}/context")
async def get_interview_context(
    interview_id: int,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    ë©´ì ‘ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (Phase 2: RAG ì§ˆë¬¸ ìƒì„±ìš©)
    
    Returns:
        - company_id, company_name, company_ideal
        - position (ì§€ì› ì§ë¬´)
        - resume_id
    """
    logger.info(f"ğŸ¯ [Interview {interview_id}] ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ìš”ì²­")
    
    # 1. Interview ì¡°íšŒ
    interview = db.get(Interview, interview_id)
    if not interview:
        raise HTTPException(status_code=404, detail="Interview not found")
    
    # ê¶Œí•œ ì²´í¬
    if interview.candidate_id != current_user.id and current_user.role not in ["recruiter", "admin"]:
        raise HTTPException(status_code=403, detail="Access denied")
    
    # 2. Company ì •ë³´ ì¡°íšŒ
    company_data = None
    if interview.company_id:
        company = db.get(Company, interview.company_id)
        if company:
            company_data = {
                "company_id": company.id,
                "company_name": company.company_name,
                "company_ideal": company.ideal,
                "company_description": company.description
            }
    
    # 3. Resume ì •ë³´ ì¡°íšŒ
    resume_data = None
    if interview.resume_id:
        resume = db.get(Resume, interview.resume_id)
        if resume:
            resume_data = {
                "resume_id": resume.id,
                "file_name": resume.file_name,
                "processing_status": resume.processing_status
            }
    
    context = {
        "interview_id": interview.id,
        "position": interview.position,
        "company": company_data,
        "resume": resume_data,
        "status": interview.status
    }
    
    logger.info(f"âœ… [Interview {interview_id}] ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ")
    return context


class HybridSearchRequest(BaseModel):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìš”ì²­ (Phase 2)"""
    interview_id: int
    section_type: str  # 'skill_cert', 'career_project', 'cover_letter'
    query: str
    top_k: int = 5
    min_score: float = 0.5


@app.post("/search/hybrid")
async def hybrid_search(
    request: HybridSearchRequest,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Phase 2: ì„¹ì…˜ + íšŒì‚¬ + ì§ë¬´ í•„í„°ë§)
    
    ê²€ìƒ‰ ì „ëµ:
    1. Interviewì—ì„œ company_id, position ê°€ì ¸ì˜¤ê¸°
    2. ResumeChunkì—ì„œ section_typeìœ¼ë¡œ í•„í„°ë§
    3. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    4. ê²°ê³¼ ë°˜í™˜
    """
    logger.info(f"ğŸ” [Hybrid Search] Interview={request.interview_id}, Section={request.section_type}, Query='{request.query}'")
    
    # 1. Interview ì •ë³´ ì¡°íšŒ
    interview = db.get(Interview, request.interview_id)
    if not interview:
        raise HTTPException(status_code=404, detail="Interview not found")
    
    # ê¶Œí•œ ì²´í¬
    if interview.candidate_id != current_user.id and current_user.role not in ["recruiter", "admin"]:
        raise HTTPException(status_code=403, detail="Access denied")
    
    # 2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    try:
        task = celery_app.send_task(
            "generate_query_embedding",
            args=[request.query]
        )
        query_embedding = task.get(timeout=10)
        logger.info(f"âœ… Query embedding generated (dim: {len(query_embedding)})")
    except Exception as e:
        logger.error(f"âŒ Failed to generate query embedding: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to generate embedding: {str(e)}"
        )
    
    # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì„¹ì…˜ íƒ€ì… í•„í„°ë§)
    sql_query = text("""
        SELECT 
            rc.id as chunk_id,
            rc.resume_id,
            rc.content,
            rc.chunk_index,
            rc.section_type,
            1 - (rc.embedding <=> CAST(:query_embedding AS vector)) as similarity_score,
            r.file_name
        FROM resume_chunks rc
        JOIN resumes r ON rc.resume_id = r.id
        WHERE 
            r.id = :resume_id
            AND r.processing_status = 'completed'
            AND rc.embedding IS NOT NULL
            AND rc.section_type = :section_type
            AND 1 - (rc.embedding <=> CAST(:query_embedding AS vector)) >= :min_score
        ORDER BY rc.embedding <=> CAST(:query_embedding AS vector)
        LIMIT :top_k
    """)
    
    try:
        result = db.execute(
            sql_query,
            {
                "resume_id": interview.resume_id,
                "section_type": request.section_type,
                "query_embedding": str(query_embedding),
                "min_score": request.min_score,
                "top_k": request.top_k
            }
        )
        
        chunks = result.fetchall()
        logger.info(f"ğŸ“Š Found {len(chunks)} matching chunks (section={request.section_type})")
        
    except Exception as e:
        logger.error(f"âŒ Hybrid search failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Search failed: {str(e)}"
        )
    
    # 4. ê²°ê³¼ í¬ë§·íŒ…
    results = [
        {
            "chunk_id": chunk.chunk_id,
            "content": chunk.content[:300] + "..." if len(chunk.content) > 300 else chunk.content,
            "section_type": chunk.section_type,
            "similarity_score": float(chunk.similarity_score)
        }
        for chunk in chunks
    ]
    
    logger.info(f"âœ… [Hybrid Search] Returned {len(results)} results")
    
    return {
        "interview_id": request.interview_id,
        "section_type": request.section_type,
        "query": request.query,
        "total_results": len(results),
        "results": results
    }


# ==================== Recruiter Endpoints ====================

# ì „ì²´ ì¸í„°ë·° ëª©ë¡ ì¡°íšŒ
@app.get("/interviews")
async def get_all_interviews(
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    """ì „ì²´ ì¸í„°ë·° ëª©ë¡ ì¡°íšŒ (ë¦¬í¬ë£¨í„°ìš©)"""
    
    # ê¶Œí•œ ì²´í¬: recruiter ë˜ëŠ” adminë§Œ ì ‘ê·¼ ê°€ëŠ¥
    if current_user.role not in ["recruiter", "admin"]:
        # candidateëŠ” ìì‹ ì˜ ì¸í„°ë·°ë§Œ ì¡°íšŒ ê°€ëŠ¥
        stmt = select(Interview).where(
            Interview.candidate_id == current_user.id
        ).order_by(Interview.created_at.desc())
    else:
        # recruiter/adminì€ ì „ì²´ ì¡°íšŒ
        stmt = select(Interview).order_by(Interview.created_at.desc())
    
    interviews = db.exec(stmt).all()
    
    # ì‘ë‹µ ë°ì´í„° êµ¬ì„± (candidate ì •ë³´ í¬í•¨)
    result = []
    for interview in interviews:
        candidate = db.get(User, interview.candidate_id)
        result.append({
            "id": interview.id,
            "candidate_id": interview.candidate_id,
            "candidate_name": candidate.full_name if candidate else "Unknown",
            "position": interview.position,
            "status": interview.status,
            "created_at": interview.created_at,
            "start_time": interview.start_time,
            "end_time": interview.end_time,
            "overall_score": interview.overall_score
        })
    
    logger.info(f"Interviews list requested by {current_user.username} ({current_user.role}): {len(result)} records")
    return result

# ==================== Health Check ====================

# ì„œë²„ ìƒíƒœ í™•ì¸
=======
# Health Check
>>>>>>> 5fe6f7adb33f16443747dc01fc10ed12295552be
@app.get("/")
async def root():
    return {
        "service": "AI Interview Backend v2.0",
        "status": "running",
        "doc": "/docs"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)