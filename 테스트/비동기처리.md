# ⚡ AI 면접 시스템 비동기 병렬 처리 및 로직 최적화 계획서

현재 발생하고 있는 **"2단계 이후 면접 중단 현상"**과 **"느린 응답 속도"**를 근본적으로 해결하기 위한 기술적 개선 계획입니다.

---

## 1. 현황 및 문제 원인 분석

### 1-1. 자원 충돌 (VRAM 병목)

- **현상**: 답변 분석(`analyze_answer`)과 질문 생성(`generate_next_question`)이 각자 모델을 로드하려 시도.
- **원인**: 7.8B 규모의 EXAONE 모델은 GPU 메모리에 중복 로드될 수 없으며, 이 과정에서 태스크가 조용히 중단됨.

### 1-2. 직렬 처리의 한계 (UX 저하)

- **현상**: 비동기 호출(`delay`)을 사용함에도 불구하고 실제로는 순차적으로 실행됨.
- **원인**: 워커가 `solo` 풀 모드이며 인스턴스가 하나뿐이라, 앞에 진행 중인 '분석' 작업이 끝날 때까지 '질문 생성'이 시작되지 않음.
- **결과**: `분석 시간(약 60초) + 생성 시간(약 40초)`이 누적되어 프론트엔드 타임아웃(120초)을 위협함.

### 1-3. 단계 매칭 로직 결함

- **현상**: 특정 단계에서 다음 질문이 생성되지 않고 면접이 종료됨.
- **원인**: DB에 저장된 단계명과 `interview_scenario.py`에 정의된 키값이 불일치할 경우 진행이 멈추는 방어 로직 부재.

---

## 2. 해결 단계 및 구체적 계획

### 🛠 Phase 1. 로직 정상화 및 모델 통합 (Logic & Resource)

- **[모델 싱글톤화]**: `evaluator`와 `question_gen`이 각자 모델을 로드하지 않고, `utils/exaone_llm.py`의 단일 인스턴스만 사용하도록 수정.
- **[방어 로직 추가]**: `generate_next_question` 수행 시 단계 매칭 실패를 대비한 '공통 직무 질문' 폴백(Fallback) 메커니즘 상시 가동.
- **[VRAM 관리]**: 태스크 종료 후 명시적인 가비지 컬렉션(`gc.collect()`) 및 캐시 정리를 통해 실행 시점 메모리 안정성 확보.

### ⚡ Phase 2. 워커 역할 분리 및 큐(Queue) 설계 (Architecture)

- **[태스크 라우팅]**: Celery 설정을 통해 태스크의 성격에 따라 큐를 강제 배정.
  - `gpu_queue`: 질문 생성 (LLM GPU 가속 필수)
  - `cpu_queue`: 답변 분석 (CPU 경량 분석 혹은 CPU 모드 LLM 사용)
- **[분석 로직 경량화]**: `analyze_answer`에서 리소스가 부족할 경우 CPU에서도 빠르게 구동되는 '경량 평가 모드' 옵션 제공.

### 🐳 Phase 3. 인프라 물리적 분리 (Infra)

- **[Docker Compose 복제]**: `ai-worker` 서비스를 `ai-worker-gpu`와 `ai-worker-cpu`로 이원화.
- **[개별 자원 할당]**: GPU 워커에게만 NVIDIA 리소스를 할당하고, CPU 워커는 순수 연산 파워만 사용하도록 제한.
- **[병목 제거]**: 이제 분석 워커가 아무리 바빠도 질문 생성 워커는 자기 큐의 작업을 즉시 처리하여 전체 면접 흐름을 유지.

---

### 2-4. 모델 배치 전략 (Model Placement Strategy)

제한된 GPU 자원(VRAM)을 효율적으로 활용하기 위해 모델별 실행 환경을 다음과 같이 고정합니다.

| 모델명                       | 역할                               | 할당 자원            | 이유                                                 |
| :--------------------------- | :--------------------------------- | :------------------- | :--------------------------------------------------- |
| **EXAONE 3.5 (7.8B)**  | **질문 생성** (Question Gen) | **GPU (CUDA)** | 면접 흐름 유지를 위해 가장 빠른 응답 속도 필요       |
| **EXAONE 3.5 (7.8B)**  | **답변 분석** (Evaluation)   | **CPU**        | 백그라운드 작업이므로 상대적으로 느린 속도 허용 가능 |
| **KURE-v1**            | **RAG 임베딩** (Embedding)   | CPU                  | 벡터 연산 특성상 GPU 사용 시 극적인 효율 향상        |
| **Whisper-v3-turbo**   | **음성 인식** (STT)          | **CPU**        | 현재 속도로도 충분하며, GPU 메모리 점유 방지 필요    |
| **DeepFace/MediaPipe** | **감정/시선 분석** (Vision)  | **CPU**        | 가벼운 모델 위주로 구성하여 CPU 부하 최소화          |

---

## 3. 기대 효과 및 향후 변화

| 개선 전 (AS-IS)                              | 개선 후 (TO-BE)                                  |
| :------------------------------------------- | :----------------------------------------------- |
| **면접 성공률**: 2단계 이후 중단 잦음  | **15단계 무중단 완주 보장**                |
| **대기 시간**: 약 100초~120초 이상     | **약 30초~40초 (체감상 70% 단축)**         |
| **자원 효율**: GPU 중복 로드로 불안정  | **최저 자원(GPU 1장)으로 고성능 발휘**     |
| **장애 대응**: 로그 파악이 매우 난해함 | **GPU/CPU 워커 로그 분리로 명확한 디버깅** |

---

**작성자**: Antigravity (AI Coding Assistant)
**작성일**: 2026-02-10
