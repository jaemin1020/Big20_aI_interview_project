# 📑 AI-워커 엔진 진행 보고서 (통합본)

---
## [01-파싱.md] PDF 파싱 엔진 기술 분석 및 이력서 구조화 시스템 종합 보고서

# 📑 PDF 파싱 엔진 기술 분석 및 이력서 구조화 시스템 종합 보고서

---

## 1. 시스템 개요

본 문서는 **pdfplumber 기반 이력서 자동 파싱 시스템**의 기술적 구조와 동작 원리를 설명하는 종합 보고서입니다.

본 시스템은 다음 두 가지를 동시에 수행합니다.

1. PDF 내부 객체(Object) 기반 텍스트·표 추출
2. 추출 데이터를 구조화된 JSON 형태로 변환

핵심 목표는 다음과 같습니다.

* 이력서의 **정형 데이터(학력, 경력, 수상 등)**를 정확히 매핑
* 표 기반 레이아웃을 최대한 보존
* 비정형 자기소개서까지 구조적으로 분리

---

# 2. PDF 파싱 엔진 핵심 원리

## 2-1. 객체 기반 추출 (Object-Based Extraction)

본 시스템은 pdfplumber를 사용합니다.

pdfplumber는 OCR 방식이 아닙니다.

즉,

❌ 이미지를 보고 글자를 추측하지 않음
✅ PDF 내부 객체 데이터 직접 분석

PDF 내부에는 다음 정보가 저장되어 있습니다.

* 문자(Char)

  * x0, x1, top, bottom 좌표
  * 폰트
  * 크기
* 선(Line)
* 사각형(Rect)
* 기타 그래픽 객체

이 정보를 조합하여 문서 구조를 재구성합니다.

---

## 2-2. 좌표 시스템 기반 구조 인식

pdfplumber는 페이지를 하나의 좌표 평면으로 간주합니다.

### 텍스트 문장 인식

* 같은 y축에 일정 간격으로 배열 → 하나의 문장
* x좌표 간격 → 띄어쓰기 판단

### 표(Table) 인식

* 선(Line) 객체 감지
* 선이 교차하는 지점 → 셀(Cell) 생성
* 행(Row), 열(Column) 구성

이 원리 덕분에 이력서처럼 표가 많은 문서에 매우 강력합니다.

---

# 3. 전체 파싱 아키텍처

시스템 흐름은 다음과 같습니다.

```
PDF 입력
   ↓
pdfplumber 객체 추출
   ↓
텍스트 + 표 데이터 분리 수집
   ↓
섹션 상태 전환(State Machine)
   ↓
카테고리별 구조화
   ↓
JSON 출력
```

---

# 4. 코드 상세 분석

아래 코드를 기준으로 매우 상세히 설명합니다.

---

# 4-1. 유틸리티 함수 분석

---

## ① clean_text(text)

### 목적

* 불필요한 공백 제거
* 줄바꿈 통합
* 정규식 매칭 정확도 향상

### 코드 핵심

```python
return re.sub(r'\s+', ' ', text).strip()
```

### 동작 원리

* `\s+` → 하나 이상의 공백
* 모든 공백을 단일 스페이스로 변환
* 앞뒤 공백 제거

### 입력 / 출력 예시

입력:

```
"  홍길동   \n  백엔드   개발자  "
```

출력:

```
"홍길동 백엔드 개발자"
```

---

## ② get_row_text(row)

### 목적

* 표 한 줄을 하나의 문자열로 결합
* 섹션 키워드 탐지용

### 예시

입력:

```
["학력", "", ""]
```

출력:

```
"학력"
```

입력:

```
["2020.03", "서울대학교", "컴퓨터공학과"]
```

출력:

```
"2020.03서울대학교컴퓨터공학과"
```

공백 제거 후 붙입니다.

---

## ③ is_date(text)

### 목적

* 날짜 또는 연도 포함 여부 확인

정규식:

```
\d{4}
```

### 예시

입력:

```
"2023.03 ~ 2024.02"
```

출력:

```
True
```

입력:

```
"컴퓨터공학과"
```

출력:

```
False
```

---

# 4-2. 메인 함수: parse_resume_final()

---

## 1단계: 기본 데이터 구조 생성

```python
data = {
    "header": { "name": "", "target_company": "", "target_role": "" },
    "education": [],
    "activities": [],
    "awards": [],
    "projects": [],
    "certifications": [],
    "self_intro": []
}
```

### 출력 형태 예시

```json
{
  "header": {
    "name": "홍길동",
    "target_company": "삼성전자",
    "target_role": "백엔드 개발"
  },
  "education": [],
  ...
}
```

---

## 2단계: 입력 소스 판별

```python
is_file_path = False
```

조건:

* 문자열이 .pdf로 끝남
* 실제 파일 존재

→ 파일로 판단

아니면 텍스트로 판단

---

## 3단계: PDF에서 데이터 추출

### 텍스트 추출

```python
text = page.extract_text()
```

### 예시 출력

```
이름 : 홍길동
지원직무 : 백엔드 개발
```

---

### 표 추출

```python
tables.extend(page.extract_tables())
```

### 예시 출력 구조

```
[
  [
    ["이름", "홍길동"],
    ["지원직무", "백엔드 개발"]
  ],
  [
    ["학력"],
    ["2020.03~2024.02", "서울대학교", "컴퓨터공학과", "3.8/4.5"]
  ]
]
```

---

# 5. 헤더(Header) 파싱 로직

## 로직 설명

1. "이름" 찾음
2. 오른쪽 칸(i+1) 값 가져옴
3. 없으면 i+2 확인

### 예시 표 입력

```
["이름", "홍길동"]
```

출력:

```json
"header": {
  "name": "홍길동"
}
```

---

## 폴백 로직 (정규식)

표에 이름이 없으면:

```
이\s*름\s*[:：\-\s]+([가-힣]{2,4})
```

입력:

```
이름 : 홍길동
```

출력:

```
홍길동
```

---

# 6. 섹션 상태 관리 (State Machine)

변수:

```python
current_section
```

동작 방식:

| 발견 키워드 | 상태 변경      |
| ----------- | -------------- |
| 학력        | education      |
| 경력/활동   | activities     |
| 수상        | awards         |
| 자격증      | certifications |
| 프로젝트    | projects       |

---

# 7. 학력 파싱 상세 분석

입력 예시:

```
["2020.03~2024.02", "서울대학교-컴퓨터공학과", "3.8/4.5"]
```

### 분해 과정

1. period → 첫 칸
2. 학교명 → parts[0]
3. 전공 → parts[1]
4. 학점 → GPA 패턴 검색

### 출력 예시

```json
{
  "period": "2020.03~2024.02",
  "school_name": "서울대학교",
  "major": "컴퓨터공학과",
  "gpa": "3.8/4.5"
}
```

---

# 8. 활동(Activities) 파싱

입력:

```
["2023.01~2023.06", "AI 프로젝트 - 1등 (2023)", "팀장", "하이브본사"]
```

### 처리 단계

1. 괄호 안 날짜 추출
2. * 기준 분리
3. 역할 보강

출력:

```json
{
  "period": "2023",
  "title": "AI 프로젝트",
  "role": "팀장",
  "organization": "하이브본사"
}
```

---

# 9. 수상(Awards) 파싱

입력:

```
["2023", "AI 경진대회 - 최우수상", "한국정보원"]
```

출력:

```json
{
  "date": "2023",
  "title": "AI 경진대회",
  "organization": "한국정보원"
}
```

---

# 10. 프로젝트 파싱

입력:

```
["2024.01", "RAG 기반 챗봇 개발", "FastAPI"]
```

출력:

```json
{
  "period": "2024.01",
  "title": "RAG 기반 챗봇 개발",
  "organization": "FastAPI"
}
```

---

# 11. 자기소개서(Self Introduction) 분리

정규식:

```
(\[질문\d+\].*?(?:주십시오|세요))
```

### 입력 예시

```
[질문1] 지원 동기를 작성하세요
저는 백엔드 개발자가 되고 싶습니다.

[질문2] 협업 경험을 작성하세요
...
```

### 출력

```json
[
  {
    "question": "[질문1] 지원 동기를 작성하세요",
    "answer": "저는 백엔드 개발자가 되고 싶습니다."
  }
]
```

---

# 12. 라이브러리 비교 분석

| 라이브러리   | 특징                  | 강점                | 한계                     |
| ------------ | --------------------- | ------------------- | ------------------------ |
| pdfplumber   | 객체 기반 고수준 추출 | 표 인식 강력        | 대용량 문서 느릴 수 있음 |
| PyPDF2       | 저수준 조작           | 병합/분할 빠름      | 표 인식 거의 없음        |
| pdfminer.six | 상세 로우데이터       | 폰트/좌표 완전 제어 | 복잡                     |
| Camelot      | 표 특화               | 격자 표 강력        | 텍스트 추출 약함         |
| Tesseract    | OCR 엔진              | 스캔 PDF 가능       | 느리고 오타 발생         |

---

# 13. 결론

본 시스템은 다음 이유로 pdfplumber를 채택하였습니다.

1. 이력서의 대부분은 표 기반
2. 좌표 기반 매칭 가능
3. 항목-값 매칭 정확도 우수
4. 상태 전환 기반 구조화 가능

단, 스캔 PDF 대응 시에는 OCR 엔진(Tesseract)와의 하이브리드 구성이 필요합니다.

---

# 🔥 최종 결과 예시 (전체 출력)

```json
{
  "header": {
    "name": "홍길동",
    "target_company": "삼성전자",
    "target_role": "백엔드 개발"
  },
  "education": [
    {
      "period": "2020.03~2024.02",
      "school_name": "서울대학교",
      "major": "컴퓨터공학과",
      "gpa": "3.8/4.5"
    }
  ],
  "activities": [],
  "awards": [],
  "projects": [],
  "certifications": [],
  "self_intro": []
}
```

---

# 📌 종합 평가

본 코드는 단순 텍스트 추출이 아닌,

* 객체 기반 분석
* 표 구조 인식
* 상태 머신 기반 분류
* 정규식 보강 로직
* 예외 대응 처리

까지 포함한 **준-프로덕션 수준 이력서 파싱 엔진**입니다.

---

## [02-청킹.md] 이력서 AI 분석 시스템: 파싱 및 청킹 엔진 심층 분석

# 📑 [기술 명세서] 이력서 AI 분석 시스템: 파싱 및 청킹 엔진 심층 분석

본 문서는 PDF에서 추출된 비정형 데이터를 AI가 가장 효율적으로 이해할 수 있는 형태인 **'의미 단위의 조각(Chunk)'**으로 재구성하는 기술적 매커니즘을 상세히 다룹니다.

---

## 1. 텍스트 분할 전략: `RecursiveCharacterTextSplitter`

가장 먼저 살펴볼 부분은 데이터를 자르는 **'도구'**의 설정입니다. 단순히 글자 수로 자르는 것은 AI에게 문맥이 잘린 쓰레기 데이터를 주는 것과 같습니다.

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,
    chunk_overlap=100,
    separators=["\n\n", "\n", ".", " ", ""]
)

```

### 🔍 깊이 있는 분석

* **재귀적 분할 (Recursive Splitting)**: 이 분할기는 `separators` 리스트의 순서대로 자를 지점을 찾습니다.

1. 가장 먼저 **문단(`\n\n`)**을 찾습니다. 문단이 통째로 600자 이내라면 그대로 한 조각이 됩니다.
2. 문단이 너무 길면 **줄바꿈(`\n`)**을 찾고, 그다음은 **마침표(`.`)**을 찾습니다.
3. 결과적으로 문장 중간이 툭 끊기지 않고, 최대한 의미가 완결되는 지점에서 조각이 나뉩니다.

* **청크 오버랩 (100자)**: 조각 A의 끝 100자와 조각 B의 앞 100자를 겹치게 만듭니다. 이는 AI가 검색된 조각만 읽었을 때 앞뒤 문맥을 몰라 발생하는 **'할루시네이션(환각)'**을 방지하는 아주 중요한 장치입니다.

---

## 2. 항목별 데이터 처리 로직 및 문법 분석

코드는 각 항목의 특성에 맞춰 데이터를 **'시맨틱(Semantic) 문장'**으로 가공합니다.

### ① 안전한 데이터 접근 (`.get()` 문법)

```python
header = parsed_data.get("header", {})
educations = parsed_data.get("education", [])

```

* **분석**: `parsed_data["header"]`라고 직접 접근하면 해당 키가 없을 때 프로그램이 즉시 종료됩니다. `.get("key", default)`를 사용하여 데이터가 누락된 이력서라도 에러 없이 유연하게 넘어가도록 설계되었습니다.

### ② 정형 데이터의 자연어 변환 (Feature Engineering)

```python
text = f"[학력] {school} {major} ({status})"
if period: text += f" - {period}"
if gpa: text += f", 학점: {gpa}"

```

* **분석**: 흩어져 있는 학교, 전공, 학점 데이터를 하나의 완성된 문장으로 합칩니다.
* **이유**: AI 모델(LLM)은 키-값 형태의 데이터보다 **"지원자는 한국대학교에서 컴퓨터공학을 전공하고 졸업했습니다."**와 같은 자연어 형태에서 훨씬 더 높은 검색 정확도를 보이기 때문입니다.

### ③ 프로젝트 및 자기소개서의 지능적 분할

이 부분은 이력서에서 가장 긴 텍스트가 발생하는 구간입니다.

```python
if len(text) > 400:
    split_texts = text_splitter.split_text(text)
    for i, st in enumerate(split_texts):
        chunks.append({
            "text": f"(부분 {i+1}) {st}",
            "metadata": { ..., "title": title }
        })

```

* **로직**: 텍스트가 400자를 초과하면 위에서 정의한 `text_splitter`를 가동합니다.
* **`enumerate`의 활용**: 조각난 텍스트에 `(부분 1)`, `(부분 2)`와 같은 인덱스를 붙여, 나중에 AI가 "이 내용은 프로젝트의 앞부분이구나"라고 인지할 수 있게 돕습니다.

---

## 3. EXAONE 3.5 (32k) 모델 환경에서 이 코드가 빛나는 이유

"EXAONE 3.5는 한 번에 32,000 토큰을 읽는데, 왜 600자씩 자르나요?"라는 의문에 대한 기술적 답변입니다.

### 3.1 "Lost in the Middle" 현상 원천 차단

아무리 컨텍스트 창이 넓어도 모델은 입력된 텍스트의 **중간 부분에 있는 정보**를 가장 잘 놓칩니다. 600자 단위의 명확한 조각을 주면 모델은 그 정보에 100% 집중할 수 있습니다.

### 3.2 수천 명의 데이터 확장성 (Vector Search)

질문이 들어왔을 때 이력서 1,000장을 한꺼번에 모델에 넣으면 비용과 시간이 엄청나게 소모됩니다.

* **해결**: 청킹된 데이터는 벡터 DB에 저장됩니다. 사용자가 "Python 경험자 찾아줘"라고 하면, 이 코드에서 만든 `category: experience`인 조각 중 Python이 언급된 **핀포인트 조각 5개만** 골라 EXAONE 모델에게 전달합니다. 이것이 바로 고효율 RAG의 핵심입니다.

---

## 4. 메타데이터(Metadata) 설계 명세

이 코드는 텍스트만 저장하는 것이 아니라, 각 조각에 **'디지털 이름표'**를 붙입니다.

| 필드명                     | 데이터 예시              | 역할                                                    |
| -------------------------- | ------------------------ | ------------------------------------------------------- |
| **`source`**       | "resume"                 | 데이터의 출처가 이력서임을 명시                         |
| **`category`**     | "narrative", "education" | 특정 항목(예: 학력만, 자소서만)에 대한 필터링 검색 지원 |
| **`subtype`**      | "answer", "question"     | 자기소개서의 질문과 답변을 구분하여 맥락 파악력 향상    |
| **`question_ref`** | "지원동기를 쓰시오..."   | 조각난 답변이 어떤 질문에 대한 것인지 연결고리 제공     |

---

## 💡 최종 결론

본 시스템의 `chunk_resume` 함수는 단순히 텍스트를 자르는 기능적 역할을 넘어, **비정형 이력서 데이터를 AI가 가장 선호하는 '고품질 지식 조각'으로 승화시키는 전처리 공정**입니다.

특히 **EXAONE 3.5**와 같은 대형 모델과 결합될 때, 이 정교한 청킹 전략은 모델의 추론 속도를 높이고, 비용을 절감하며, 무엇보다 **정보 검색의 정확도를 비약적으로 상승**시키는 핵심 엔진 역할을 수행합니다.

---

## [03.엑사원모델.md] 엑사원 3.5 모델 특성

### 컨텍스트 길이 확장 및 성능 유지 (5page)

- **단계적 컨텍스트 확장**: K-EXAONE은 기본적으로 8K 토큰까지 프리트레이닝된 후, 두 단계에 걸쳐 컨텍스트 길이를 32K, 256K로 확장함. 각 단계에서 데이터 샘플링 비율을 조정하여 장문맥 학습 신호와 안정성을 균형 있게 반영함.
- **Rehearsal Dataset**: 장문맥 특화 학습 시 단기(짧은 문맥) 성능 저하를 방지하기 위해, 프리트레이닝 분포 및 기타 단기 데이터에서 고품질 샘플을 재사용하는 Rehearsal Dataset을 도입.
... (이력서 03.엑사원모델.md 의 전체 내용 포함됨) ...

---

## [04.임베딩.md] [기술 보고서] 이력서 임베딩 엔진: 텍스트의 수치화 및 최적화 전략

본 문서는 한국어 문장 유사도에 특화된 `KURE-v1` 모델을 활용하여, 청킹된 이력서 조각들을 고차원 벡터로 변환하는 기술적 매커니즘을 상세히 다룹니다.

(04.임베딩.md 전체 내용 통합됨)

---

## [05.pgvector.md] [기술 보고서] 벡터 데이터 저장 엔진: PGVector 기반 영구 저장 매커니즘

본 문서는 임베딩된 이력서 데이터를 PostgreSQL의 벡터 확장 기능(`pgvector`)을 활용하여 영구 저장하고, 멀티 테넌트(Multi-tenant) 검색을 위해 메타데이터를 관리하는 기술적 과정을 상세히 기술합니다.

(05.pgvector.md 전체 내용 통합됨)

---

## [07.resume-embedding-orcas.md] [기술 보고서] AI 면접 시스템: RAG 기반 지능형 질문 생성 엔진

본 시스템은 지원자의 이력서를 분석하여 벡터화하고, 이를 바탕으로 면접 단계에 맞는 최적의 질문을 실시간으로 생성하는 오케스트레이션(Orchestration) 구조를 가집니다.

(07.resume-embedding-orcas.md 전체 내용 통합됨)

---

## [08-질문생성.md] 기술 상세 분석 보고서: BIGTERVIEW 지능형 면접 추론 엔진

본 보고서는 실시간 AI 면접 시스템의 핵심 로직인 `generate_next_question_task`를 중심으로, 입력 검증 → 문맥 검색 → 동적 추론 → 출력 정제 → 시스템 복원력에 이르는 전 과정을 코드 수준에서 정밀 분석합니다.

(08-질문생성.md 전체 내용 통합됨)
