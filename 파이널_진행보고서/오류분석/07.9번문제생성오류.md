# 9, 11, 13번 인재상 기반 질문 생성 오류 분석 보고서

## 1. 개요
최근 면접 진행 중 9번(협업/소통), 11번(책임감/가치관), 13번(성능/창의성) 등 '인재상 기반 Narrative' 단계에서 AI 면접관이 직접적인 질문이 아닌, 질문의 의도나 가이드를 설명하는 형식의 부적절한 텍스트를 생성하는 현상이 발견되었습니다. 본 보고서는 해당 오류의 기술적 원인을 분석하고 구체적인 해결 방안을 제시합니다.

---

## 2. 오류 현상 분석
**사례 (9번 질문)**:
> "지원자가 이전에 협업 과정에서 창의적인 해결책을 제시하여 팀 목표 달성에 기여했다고 답변했다면, 구체적인 상황에서 예상치 못한 난관이 발생했을 때, 창의적인 접근법을 유지하면서도 팀원들의 의견을 어떻게 조율하여 프로젝트를 성공적으로 이끌었는지 설명해주세요. 이 질문은 지원자가 창의성과 동시에 팀워크 및 문제 해결 능력을 균형 있게 발휘한 구체적인 사례를 요구하여, 회사의 인재상과의 일치성을 심층적으로 검증합니다."

**핵심 문제**: 
AI가 면접관으로서 지원자에게 묻는 '단일 질문'을 내놓지 않고, "지원자가 ~라고 답변했다면"과 같은 가설적 상황과 "이 질문은 ~를 의도합니다"라는 가이드 성격의 설명을 포함하여 출력함.

---

## 3. 기술적 원인 규명
### (1) 프롬프트의 '꼬리질문' 고정 지시 (역할 충돌)
- **현상**: `ai-worker/tasks/question_generator.py`의 `PROMPT_TEMPLATE`에 "가장 예리한 꼬리질문 하나를 생성하십시오"라는 지시가 모든 단계에 고정되어 있음.
- **원인**: 9, 11, 13번은 이전 대화와 별개로 새로운 주제를 시작하는 '신규 질문' 단계임에도 불구하고, 시스템이 AI에게 "이전 답변을 분석해서 꼬리질문을 던져라"라고 잘못 명령함. LLM은 이 모순을 해결하기 위해 이전 답변과 신규 주제를 억지로 엮으려다 가설적/가이드 형태의 답변을 생성하게 됨.

### (2) 정제 로직의 '물음표(?)' 의존성 (정제 실패)
- **현상**: 질문 본문 뒤에 붙는 사족을 잘라내는 로직이 물음표(`?`) 부호를 기준으로 작동함.
- **원인**: 9번 단계의 가이드에 "물음표를 절대 사용하지 말고 '~주세요.'로 끝내라"라는 강한 제약이 걸려 있어 질문 내에 물음표가 생성되지 않았고, 이로 인해 정제 로직이 작동하지 않아 뒷부분의 가이드 설명이 그대로 노출됨.

### (3) LLM의 페르소나 이탈 (인터뷰어 vs 가이드 제작자)
- **원인**: 위 두 가지 제약 사항이 충돌하면서 EXAONE 모델이 본인을 '면접관'이 아닌 '면접 시나리오 설계자'로 오인하여 질문의 의도와 가정을 설명하는 텍스트를 출력함.

### (4) 11번 단계의 특수성 망각
- **원인**: 11번(책임감/가치관)은 이력서(자기소개서)의 특정 문장을 RAG로 가져와서 인용해야 하는 특수한 단계임에도 불구하고, 일반적인 인재상 질문과 동일한 범주로 묶여 처리됨으로써 RAG 데이터 활용도가 저하됨.

---

## 4. 단계별 해결 방안 (수정 계획)

### [해결 1] PROMPT_TEMPLATE 구조 개선 (ai-worker/tasks/question_generator.py)
고정된 "꼬리질문 생성" 문구를 지우고 상황에 맞는 핵심 미션인 `{mode_task_instruction}` 변수를 도입합니다.
- **9, 13번 (일반 인재상)**: "지원자의 이전 답변에 얽매이지 말고, 회사의 인재상 가치를 검증하기 위한 **새로운 질문**을 던지십시오."
- **11번 (자기소개서 인용)**: "**[지원자 자기소개서 답변]에서 인상적인 문장을 하나 인용**하여 질문을 시작하고, 이를 인재상과 연결하여 가치관을 묻는 질문을 생성하십시오."
- **일반 꼬리질문**: "이전 답변의 논리적 허점을 찌르는 **예리한 꼬리질문**을 생성하십시오."

### [해결 2] 텍스트 정제(Cleaning) 로직 보강
물음표(`?`) 유무와 상관없이 질문 뒤에 붙는 사족을 제거하는 정규식을 더 강력하게 수정합니다.
- `re.sub(r'(이\s*질문은|의도는|~라고\s*답변했다면|설명하십시오|검증합니다).*', '', final_content)` 구문을 추가하여 AI가 스스로 적은 해설 부분을 원천 차단합니다.

### [해결 3] mode_instruction 로직 강화
9번(협업/소통) 단계일 때 "이전 답변을 요약하지 말고, 즉시 다음 주제인 '협업'에 대해 질문을 시작하십시오"라는 지침을 명시적으로 추가하여, AI가 9번을 꼬리질문으로 착각하지 않도록 방어 로직을 삽입합니다.

### [해결 4] 시나리오 가이드 최적화 (backup-core/config/)
AI가 '가이드'를 질문 본문으로 착각하지 않도록, `guide` 문구에서 "~를 확인하십시오" 같은 분석적 표현을 "**~에 대해 질문하십시오**"와 같은 직접적인 명령형으로 다듬어 LLM의 행동 지침을 명확히 합니다.

## 5. 최종 수정 내역 (완료 ✅)
- **일자**: 2026-02-28
- **수정 사항**:
    1. **프롬프트 동적 제어 (`{mode_task_instruction}`)**: 
        - 43~44행의 고정된 "꼬리질문 생성" 지시문을 제거하고 변수화 완료.
        - **9, 13번**: "이전 답변에 얽매이지 말고 새로운 주제로 질문하십시오" 지시 주입.
        - **11번**: "자기소개서 문장을 인용하여 가치관을 물어보라"는 전용 미션 주입.
    2. **정제(Cleaning) 로직 보강**:
        - 물음표(`?`) 유무와 상관없이 `(이\s*질문은|의도는|~라고\s*답변했다면|검증합니다|의도함|확인합니다|요구하여).*` 패턴을 감지하여 뒷부분을 일괄 삭제하는 정규식 적용.
    3. **11번 RAG 데이터 활용 최적화**: 
        - 11번 단계에서 자기소개서 [질문1] 데이터를 정상적으로 불러와 질문 서두에 배치하도록 인스트럭션과 로직을 매칭함.

---
**최종 상태**: ✅ 수정 완료 및 테스트 준비 중
**보고서 작성일**: 2026-02-28
**작성자**: Antigravity (AI Assistant)
