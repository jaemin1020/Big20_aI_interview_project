# 답변 상세 평가 프로세스 개선 보고서

## 1. 개요
현재 AI 면접 시스템에서는 사용자가 답변을 완료할 때마다 실시간으로 답변 분석 및 평가(`total_score`, `rubric_score`)를 수행하고 있습니다. 이 방식은 면접 종료 즉시 리포트를 볼 수 있다는 장점이 있으나, GPU 자원 점유로 인한 다음 질문 생성 지연 및 면접 흐름 저하라는 치명적인 단점이 발견되었습니다. 이를 해결하기 위해 **'실시간 개별 평가'** 모델에서 **'면접 종료 후 일괄 평가'** 모델로 아키텍처를 개선하였습니다.

## 2. 문제 분석
- **GPU 자원 경합**: `analyze_answer`(답변 분석)와 `generate_next_question`(다음 질문 생성)이 동시에 GPU 워커에 요청되면서, 다음 질문이 나오기까지 대기 시간이 길어지는 현상 발생.
- **실시간성 저해**: 사용자는 질문에 대한 답변을 마친 후 즉시 다음 질문을 듣기를 기대하나, 평가 로직이 병목이 되어 사용자 경험(UX)이 저하됨.
- **데이터 미완성 리스트**: 면접 도중 오류나 네트워크 문제로 개별 평가가 누락될 경우, 최종 리포트의 정합성이 떨어질 위험이 있음.

## 3. 해결 방안 (개선 사항)
면접 중에는 **질문 생성**에만 자원을 집중하고, 개별 답변에 대한 **정밀 평가(`rubric_score` 포함)**는 면접이 모두 종료된 시점(`complete_interview`)에 한꺼번에 처리하도록 프로세스를 변경하였습니다.

### 주요 개선 메커니즘:
1.  **실시간 호출 중단**: `transcripts` 저장 시점에 수행하던 `analyze_answer` 비동기 태스크 호출을 제거.
2.  **배치 평가(Batch Evaluation) 도입**: 최종 리포트 생성(`generate_final_report`) 태스크 시작 직후, 평가되지 않은 모든 답변을 선제적으로 일괄 분석.
3.  **데이터 정합성 확보**: `total_score`와 `rubric_score`가 모두 채워진 것을 확인한 후 최종 리포트 요약 로직을 실행하여 리포트 품질 보증.

## 4. 코드 수정 내역

### 4.1. `backend-core/routes/transcripts.py`
- 사용자의 발화가 저장될 때마다 실행되던 AI 평가 호출을 주석 처리하여 면접 중 GPU 부하를 제거하였습니다.
- **변경 전**: 답변 저장 시 `analyze_answer` 즉시 호출.
- **변경 후**: 질문 생성 요청만 수행하고 평가는 면접 종료 시점으로 위임.

### 4.2. `ai-worker/tasks/evaluator.py`
- `generate_final_report` 함수 내부에 **Pre-processing(사전 평가)** 로직을 추가하였습니다.
- **로직 상세**:
    - 해당 면접의 모든 `Transcript` 중 `Speaker.USER`가 발화한 답변을 필터링.
    - 그 중 `total_score` 또는 `rubric_score` 칼럼이 `NULL`인 대상을 식별.
    - 식별된 답변들에 대해 `analyze_answer`를 동기적으로 호출하여 DB에 점수와 루브릭 정보를 즉시 업데이트.
    - 모든 답변의 평가가 완료된 후, 최신 데이터를 다시 로드하여 최종 리포트 요약 프로세스로 전달.

## 5. 기대 효과 및 향후 계획
- **면접 체감 속도 향상**: 면접 중 AI의 응답 속도가 현저히 빨라져 자연스러운 대화 흐름이 가능해집니다.
- **데이터 완결성**: 리포트 조회 전 모든 답변에 대한 정밀 진단(`rubric_score`)이 완료됨을 보장하므로, 분석 결과의 신뢰도가 상승합니다.
- **자원 최적화**: GPU 자원을 면접 단계별로(질문 생성 -> 일괄 평가) 효율적으로 배분하여 서버 부하를 안정화시킵니다.

---
**보고서 작성일**: 2026-02-28
**작성자**: Antigravity (AI Assistant)
