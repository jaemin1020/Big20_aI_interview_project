# 👔 면접 대비: LangChain(랭체인) 활용 기술 문서 (Q&A 형식)

본 문서는 우리 프로젝트에서 LangChain을 어떻게 활용했는지와 프레임워크 전반에 대한 기술적 질문을 정리한 자료입니다.

---

### Q1. 이 프로젝트에서 LangChain 프레임워크를 도입한 주요 이유는 무엇인가요?
**A1.** **'추상화(Abstraction)'와 '생태계 활용(Ecosystem Integration)'**을 위해 도입했습니다. 
특정 LLM 모델(예: OpenAI, EXAONE)에 의존하지 않고 인터페이스를 통일하여 유연하게 모델을 교체할 수 있는 환경을 구축했습니다. 특히 **RAG(Retrieval-Augmented Generation)**를 위한 벡터 데이터베이스(PGVector)와의 쉬운 연동, 복잡한 프롬프트 템플릿의 체계적 관리, 구조화된 출력(JSON)을 파싱하는 과정을 표준화하기 위해 채택했습니다.

---

### Q2. 프로젝트의 어떤 부분에서 LangChain의 어떤 구성 요소들을 사용했나요?
**A2.** 크게 **세 가지 핵심 영역**에서 활용했습니다.
1.  **질문 생성 (`PromptTemplate` & `StrOutputParser`):** 지원자의 직무와 자소서 문맥을 반영하는 동적인 프롬프트를 생성하고, LLM의 답변에서 불필요한 사족을 제거한 정제된 텍스트를 추출하는 데 사용했습니다.
2.  **RAG 기반 엔진 (`PGVector` & `HuggingFaceEmbeddings`):** 이력서 데이터를 벡터화하고, 지원자의 답변과 관련 있는 프로젝트 경험을 실시간으로 검색하는 Retriever를 구축하는 데 핵심적으로 사용했습니다.
3.  **답변 평가 (`JsonOutputParser`):** AI가 지원자의 답변을 여러 가지 루브릭 지표(기술력, 소통 능력 등)에 맞춰 채점할 때, 결과물을 정확한 JSON 형식으로 출력하고 직렬화하는 데 활용했습니다.

---

### Q3. 프로젝트에서 사용 중인 EXAONE(GGUF) 모델은 LangChain에서 기본 제공하지 않는데, 어떻게 연동했나요?
**A3.** LangChain의 **커스텀 LLM 인터페이스(`langchain_core.language_models.llms.LLM`)를 상속받아 직접 클래스를 구현**했습니다. 
`_call` 메서드를 오버라이딩하여 `llama-cpp-python`을 통해 로드된 로컬 EXAONE 모델을 호출하도록 설계했으며, 이를 통해 기존 LangChain의 `PromptTemplate`이나 `Chain` 로직을 코드 수정 없이 그대로 모델에 적용할 수 있는 **호환성(LCEL 호환성)**을 확보했습니다.

---

### Q4. LCEL(LangChain Expression Language)의 장점은 무엇이라고 생각하시나요?
**A4.** **'선언적 프로그래밍'**과 **'투명성'**입니다. 
`prompt | model | parser`와 같이 직관적인 파이프 기호를 사용하여 복잡한 로직을 한 줄로 표현할 수 있어 코드의 가독성이 비약적으로 높아집니다. 또한, 같은 로직을 동기(Sync) 방식과 비동기(Async) 방식으로 손쉽게 전환할 수 있으며, 중간 데이터의 흐름을 추적하기 쉬워 디버깅이 용이하다는 장점이 있습니다.

---

### Q5. LangChain을 사용하면서 겪은 문제나 한계점이 있었나요?
**A5.** 추상화가 높은 만큼 **내부 동작 방식(Black-box)을 완전히 제어하기 어려운 경우**가 있었습니다. 
예를 들어, 특정 모델에서 프롬프트가 잘리는 현상이나 토큰 제한 문제 발생 시 LangChain 내부 소스 코드를 분석해야만 원인을 알 수 있는 경우가 있었습니다. 이를 해결하기 위해 `LangSmith`를 연동하여 실제 LLM에 전달되는 최종 프롬프트와 페이로드를 실시간으로 모니터링하며 문제를 해결했습니다.

---

### 💡 [면접 전략] 랭체인 관련 핵심 키워드 정리
*   **Prompt Engineering:** `{context}`와 `{question}` 등을 조합하여 모델의 답변 품질을 높이는 기법.
*   **Structured Output:** LLM의 자유로운 답변을 JSON 등으로 강제하여 시스템 연동성을 높이는 기법.
*   **VectorStore (RAG):** 지식 베이스를 벡터화하여 모델의 할루시네이션(환각)을 방지하고 정확한 정보를 제공하는 아키텍처.
*   **Chain of Thought (CoT):** 답변 평가 시 AI가 단계별로 사고하게 하여 채점의 논리적 근거를 확보하는 전략.
