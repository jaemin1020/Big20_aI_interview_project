# 👔 면접 대비: 프로젝트 종합 소개 및 기술 Q&A (데이터/AI 엔지니어링 중심)

본 문서는 지원하시는 **Python 기반 데이터 파이프라인 및 RAG 엔지니어** 직무에 맞춰, 프로젝트의 기술적 핵심 가치를 재구성한 자료입니다.

---

### Q1. 이 프로젝트(Big20 AI Interview)에 대해 종합적으로 소개해 주십시오.
**A1.** **"실시간 멀티모달 분석 지능형 면접 시스템"**으로, 지원자의 역량에 맞춰 실시간으로 질문을 생성하고 평가하는 풀스택 AI 솔루션입니다. 시스템 전체 아키텍처는 다음과 같은 핵심 요소들이 유기적으로 결합되어 있습니다.

1.  **Frontend (React & WebRTC)**: 사용자 인터페이스를 담당하며, 실시간 영상/음성 스트리밍을 통해 지원자의 표정, 시선, 음성 데이터를 수집하고 인터랙티브한 면접 환경을 제공합니다.
2.  **Backend (FastAPI & Redis)**: 고성능 비동기 API 서버로, 전체 면접 비즈니스 로직과 데이터 흐름을 제어합니다. Redis를 메시지 브로커로 활용하여 무거운 AI 연산을 비동기 태스크로 분산 처리합니다.
3.  **AI Workers (GPU/CPU 분리 설계)**: 자원의 효율적 관리를 위해 워커를 이원화했습니다.
    *   **GPU 워커:** 거대 언어 모델(EXAONE-3.5) 추론, RAG용 벡터 임베딩 생성 등 고사양 연산을 전담합니다.
    *   **CPU 워커:** 음성 인식/합성(STT, TTS), 감정 분석, 시선 추적 등 실시간 처리가 중요한 멀티미디어 분석을 담당합니다.
4.  **Database (PostgreSQL & PGVector)**: 지원자의 기본 정보 및 면접 기록을 저장하는 관계형 데이터베이스 역할과 함께, 이력서 데이터를 벡터화하여 저장하고 실시간으로 유사 문맥을 검색하는 벡터 데이터베이스 기능을 동시에 수행합니다.

---

### Q2. 본인이 담당한 '가장 복잡한 데이터 처리' 과정은 무엇이었나요? (데이터 전처리/파이프라인)
**A2.** **"비정형 PDF 이력서에서 의미 있는 데이터를 추출하여 벡터 DB에 적재하는 ETL 파이프라인"**입니다.
단순히 전체 텍스트를 임베딩하는 것이 아니라, **하이브리드 파싱 전략**을 사용했습니다.
1.  **Parsing:** `pdfplumber`로 표와 텍스트를 구분하고, 특정 키워드(학력, 경력, 활동 등)를 기준으로 섹션을 분리하는 파서를 직접 개발했습니다.
2.  **Chunking:** `RecursiveCharacterTextSplitter`를 활용하되, 섹션의 의미가 끊기지 않도록 문맥 보호(Contextual Chunking)를 적용했습니다.
3.  **Optimization:** 임베딩 연산이 30초 이상 소요될 때 발생하는 **DB 트랜잭션 락(Transaction Lock)** 문제를 해결하기 위해, DB 세션을 '조회'와 '저장' 시점으로 짧게 쪼개는 **세션 최적화**를 진행했습니다.

---

### Q3. 프로젝트 진행 중 겪었던 가장 큰 기술적 문제는 무엇이었고, 어떻게 해결했나요? (핵심!)
**A3.** **"분산 환경에서의 AI 리소스 관리 및 데이터 파이프라인 최적화"** 과정에서 세 가지 핵심 난제를 해결했습니다.

#### 1) 라이브러리 내부 결함 및 버전 호환성 해결 (RAG/Vector DB)
*   **문제상황**: 특정 면접 단계에서 자소서 기반 RAG 검색 시 `PGVector.__init__() missing 1 required positional argument` 에러가 발생하며 질문 생성이 실패하는 현상이 있었습니다.
*   **원인분석**: LangChain의 `PGVector` 라이브러리 업데이트 과정에서 내부 데코레이터가 인자 전달 방식을 엄격하게 제한하여, 기존 키워드 인자(`kwargs`) 방식이 충돌을 일으킨 것이었습니다.
*   **해결조치**: 라이브러리 내부 소스 코드를 분석하여 **위치 인자(Positional Arguments) 기반의 초기화**로 코드를 전면 수정하고, `connection_string`과 `engine` 객체를 하이브리드 방식으로 전달하여 안정성을 확보했습니다.
*   **결과**: 시스템 강결합 문제를 해결하고 100% 신뢰할 수 있는 RAG 검색 환경을 구축했습니다.

#### 2) Redis 기반 분산 락을 통한 자원 경합 해결 (Distributed System)
*   **문제상황**: 프론트엔드의 상태 폴링(Polling) 요청이 겹치면서 동일한 질문에 대해 TTS 생성 태스크가 중복 폭주하여 서버 자원이 낭비되었습니다.
*   **원인분석**: API 서버가 비동기 작업의 완료 여부를 확인하기 전, 중복된 요청이 큐(Queue)에 쌓이는 레이스 컨디션(Race Condition)이 발생했습니다.
*   **해결조치**: Redis의 **`SET NX` (Atomic Lock)** 기능을 도입하여, 특정 태스크가 처리 중일 때는 Lock을 걸어 중복 진입을 차단했습니다. 또한 워커 단에서도 파일 존재 여부를 최종 확인하는 멱등성 로직을 추가했습니다.
*   **결과**: 무분별한 AI 연산 호출을 차단하여 서버 부하를 획기적으로 줄였습니다.

#### 3) 무거운 추론 작업을 위한 DB 세션 및 트랜잭션 최적화 (Data Pipeline)
*   **문제상황**: 이력서 임베딩 시 약 30~40초가 소요되는 동안 `transaction in progress` 경고가 발생하며 DB 락 현상이 나타났습니다.
*   **원인분석**: 긴 시간 소요되는 임베딩 연산 과정 동안 DB 세션을 계속 점유하고 있었기 때문입니다.
*   **해결조치**: 세션 관리 로직을 리팩토링하여 '데이터 추출 - 연산 - 결과 저장' 단계를 분리하고, 무거운 연산은 세션 외부에서 수행하도록 최적화했습니다.
*   **결과**: 임베딩 중에도 다른 DB 작업이 원활하게 수행되도록 파이프라인 안정성을 높였습니다.

#### 4) RAG 기반 질문 생성의 품질 저하 및 할루시네이션 해결 (Prompt Engineering)
*   **문제상황**: RAG로 이력서 문맥을 제공했음에도 불구하고, AI가 자소서 내용과 무관한 일반적인 질문만 반복하거나 존재하지 않는 기술 스택을 언급(Hallucination)하는 문제가 있었습니다.
*   **원인분석**: 프롬프트의 지시사항이 모호하여 모델이 리트리버로 가져온 컨텍스트보다 학습된 일반 데이터를 우선시했기 때문입니다.
*   **해결조치**: **프롬프트 엔지니어링 고도화**를 진행했습니다.
    *   **Persona 부착**: '냉철한 시니어 기술 면접관' 페르소나를 부여하고 사고 단계(Chain of Thought)를 명시했습니다.
    *   **Constraint 강화**: "반드시 제공된 이력서의 프로젝트 경험에서만 근거를 찾을 것", "불필요한 사족(서론/결론) 제거" 등 엄격한 제약 조건을 추가했습니다.
    *   **Few-shot 적용**: 실제 좋은 꼬리 질문과 나쁜 질문의 사례를 예시로 제공했습니다.
*   **결과**: 질문의 구체성이 비약적으로 상승했으며, 지원자의 실제 프로젝트 경험을 찌르는 예리한 질문이 생성되어 면접의 질이 향상되었습니다.

---

### Q4. LangChain이나 LangGraph 같은 프레임워크를 어떻게 활용했나요?
**A4.** LangChain의 **커스텀 LLM 인터페이스**를 직접 상속받아 로컬 환경의 GGUF 모델(EXAONE)을 완벽하게 연동했습니다. 
*   **LangChain:** 프롬프트 템플릿, 출력 파서(`JsonOutputParser`), 그리고 벡터 스토어 리트리버를 체인(Chain)으로 연결하여 시스템의 선언적 가독성을 높였습니다.
*   **Flow Control:** 최근에는 지원자의 답변 점수를 실시간 분석하여 "질문 난이도를 자동으로 조절하거나 격려 멘트를 주입"하는 **상태 기반 흐름 제어(Stateful Flow Control)** 로직을 구현했으며, 이를 향후 더 복잡한 시나리오를 위해 **LangGraph** 구조로 고도화할 계획을 가지고 있습니다.

---

### Q5. 벡터 DB(PGVector)를 선택한 이유와 활용 전략은 무엇인가요?
**A5.** 별도의 벡터 전용 DB를 도입하는 것보다, **기존 관계형 DB(PostgreSQL)와의 트랜잭션 정합성**을 유지하는 것이 프로젝트의 신뢰성 면에서 유리하다고 판단했습니다.
*   **전략:** `knn` 검색 시의 거리 측정 알고리즘(Cosine Similarity)을 최적화하고, `resume_id`와 `chunk_type`과 같은 메타데이터 필터링을 병행하여 검색 결과의 정확도(Precision)를 높였습니다. 또한 하드웨어 제약 내에서 `connection pooling`을 공유하여 수십 개의 동시 요청에도 세션이 누수되지 않도록 관리했습니다.

---

### 💡 [면접 마무리 발언 추천]
> "저는 단순히 AI 모델을 호출하는 것에 그치지 않고, **데이터의 전처리부터 벡터 DB 적재, 그리고 Celery를 활용한 비동기 파이프라인 설계**까지 전체적인 **Data-to-AI Lifecycle**을 Python으로 직접 구축해 보았습니다. 이러한 경험을 바탕으로 귀사의 데이터 파이프라인을 더욱 견고하고 효율적으로 관리할 준비가 되어 있습니다."
