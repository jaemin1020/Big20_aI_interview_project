# 🚨 시스템 로그 심층 분석 리포트 및 핵심 조치 사항

현재 시스템은 뛰어난 AI 추론 성능을 보이고 있으나, **통신 구조와 인프라 파이프라인의 비효율**로 인해 심각한 자원 낭비와 UX 저하가 발생하고 있습니다. 시급히 해결해야 할 4가지 핵심 문제를 우선순위별로 정리했습니다.

---

## 🥇 1순위: 프론트엔드 무한 Polling 및 TTS 중복 덮어쓰기 (자원 낭비)

프론트엔드가 파일 생성을 기다리지 못하고 짧은 주기로 API를 계속 호출하며, 백엔드는 이를 방어하지 못해 동일한 작업을 워커에 중복으로 지시하고 있습니다.

### 📌 결정적 증거 로그 (질문 14221번 처리 과정)

 `log

# 1. 첫 번째 TTS 요청이 큐에 들어가 연산 중 (21:37:13 시작)

ai-worker-cpu | 21:37:13.142 Task tasks.tts.synthesize[d0438...] received

# 2. 프론트엔드 Polling -> 파일 없음 -> 백엔드가 2차 중복 요청 발사

backend       | 21:37:18.247 [WARNING] Interview-Router: ⏳ [TTS Missing] ID: 14221
backend       | 21:37:18.276 [INFO] Interview-Router: 🔊 [TTS] 비동기 음성 생성 요청 완료
ai-worker-cpu | 21:37:18.325 Task tasks.tts.synthesize[aede3...] received

# 3. 첫 번째 작업 완료 (파일 생성)

ai-worker-cpu | 21:37:19.040 ✅ 음성 합성 완료 (소요시간: 5886.85ms)
ai-worker-cpu | 21:37:19.153 💾 [파일 저장 성공] 경로: /app/uploads/tts/q_14221.wav

# 4. 두 번째 중복 작업 완료 (멀쩡한 파일을 똑같이 다시 연산해서 덮어씀)

ai-worker-cpu | 21:37:21.950 ✅ 음성 합성 완료 (소요시간: 2916.39ms)
ai-worker-cpu | 21:37:22.033 💾 [파일 저장 성공] 경로: /app/uploads/tts/q_14221.wav
 `

### 💡 문제점 및 해결책

* **문제점:** CPU 연산 자원이 100% 헛공회전하고 있습니다. 동시 접속자가 늘어나면 큐가 폭발하여 서버가 다운될 수 있습니다.
* **해결책:** 백엔드에서 Celery로 태스크를 보내기 전, **Redis 분산 락(Lock)**을 설정하여 이미 진행 중인 동일 질문(ID)에 대한 중복 요청을 원천 차단해야 합니다.

---

## 🥈 2순위: 33분에 달하는 최종 리포트 생성 속도 (최악의 UX)

면접이 모두 종료된 후, 사용자의 답변 17개를 순차적(for문)으로 평가하면서 엄청난 병목이 발생하고 있습니다.

### 📌 결정적 증거 로그 (면접 종료 직후)

 `log

# 1. 22:27:57 면접 종료 및 최종 리포트 생성 시작

ai-worker-gpu | 22:27:57.194 Generating Final Report for Interview 153
ai-worker-gpu | 22:27:57.203 🧐 Evaluating 17 individual answers before final report...

# 2. 첫 번째 답변 채점에 약 72초 소요

ai-worker-gpu | 22:27:57.209    - Evaluating Transcript 1754 (Stage: intro)
ai-worker-gpu | 22:29:09.946 ✅ [DB_UPDATE] Transcript(id=1754) scores updated: total=78.0

# 3. (중략) 17개 답변을 하나씩 순서대로 묵묵히 채점...

# 4. 23:01:10 최종 완료 (약 33분 소요)

ai-worker-gpu | 23:01:10.149 ✅ 인터뷰 153에 대한 최종 리포트 생성 완료
ai-worker-gpu | 23:01:10.151 Task tasks.evaluator.generate_final_report[...] succeeded in 1993.068621585s: None
 `

### 💡 문제점 및 해결책

* **문제점:** 사용자가 면접 결과를 보기 위해 33분 동안 빈 화면을 대기해야 하므로, 서비스 이탈의 결정적 원인이 됩니다.
* **해결책:** 17개의 평가 작업을 직렬로 처리하지 않고, Celery의 `group` 또는 `chord`를 활용하여 가용 GPU/CPU 자원 내에서 **병렬 처리(Parallel Processing)**하도록 로직을 개편해야 합니다.

---

## 🥉 3순위: DB 트랜잭션 충돌 및 누수 (데이터베이스 셧다운 위험)

특히 RAG 엔진이 작동하여 자소서를 검색할 때마다 DB 세션 충돌이 발생하고 있습니다.

### 📌 결정적 증거 로그 (RAG 검색 구간)

 `log

# 1. AI가 이력서 내용 검색(RAG) 시작

ai-worker-gpu | 21:42:39.812 🔍 [RAG 검색 시작] Query: '협업 사례, 팀 프로젝트 중 갈등 조율...'

# 2. 즉시 DB 컨테이너에서 트랜잭션 경고 발생

db            | 21:42:39.838 UTC [64] WARNING:  there is already a transaction in progress

# 3. 검색 자체는 성공

ai-worker-gpu | 21:42:40.038 ✅ 검색 완료: 2개의 문맥을 발견했습니다.
 `

### 💡 문제점 및 해결책

* **문제점:** SQLAlchemy 등의 ORM에서 이전 쿼리 실행 후 세션(Session)을 닫지 않고(`close` 또는 `commit`) 계속 재사용하고 있다는 의미입니다. 동시 트랜잭션이 쌓이면 DB 데드락이 발생합니다.
* **해결책:** 백엔드 및 워커의 DB 접근 로직에서 `with SessionLocal() as db:` 패턴을 엄격히 적용하거나, 예외 발생 시 `finally: db.close()`가 보장되도록 세션 라이프사이클을 수정해야 합니다.

---

## 🏅 4순위: 도커 워커 간 시간 동기화 에러 (안정성 저하)

시스템 내부 시간이 틀어져 워커 간의 상태 체크(Heartbeat)가 실패하고 있습니다.

### 📌 결정적 증거 로그

 `log ai-worker-gpu | 21:18:47.682 [INFO/MainProcess] missed heartbeat from celery@73b98015b394 ai-worker-gpu | 21:18:47.684 [WARNING/MainProcess] Substantial drift from celery@73b98015b394 may mean clocks are out of sync.  Current drift is 33 seconds.  [orig: 2026-03-01 21:18:47.684341 recv: 2026-03-01 21:18:14.206504] ` 

### 💡 문제점 및 해결책

* **문제점:** CPU와 GPU 컨테이너 간의 시간이 33초 어긋나 있습니다. Celery는 30초 이상 하트비트가 지연되면 워커가 죽은 것으로 간주하여 작업을 재할당하거나 강제 종료할 수 있습니다.
* **해결책:** 개발 환경(WSL2 등)의 호스트 OS와 Docker 간의 시간 동기화 버그입니다. 호스트 터미널에서 `sudo hwclock -s`를 실행하여 시스템 시간을 강제로 맞추거나 Docker 데몬을 재시작해야 합니다.
