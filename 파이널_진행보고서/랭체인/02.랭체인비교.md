# 02. 랭체인 vs Native 기술 비교 분석 리포트

본 문서는 프로젝트 아키텍처 설계 과정에서 진행된 랭체인(LangChain) 프레임워크와 Native 방식(직접 구현)의 성능 및 효율성 비교 결과를 정리합니다.

## 1. 비교의 핵심 요약

| 비교 항목                  | LangChain 방식                   | Native (직접 구현) 방식         | 결과 및 시사점 |
| :------------------------- | :------------------------------- | :------------------------------ | :------------- |
| **정확도 (Quality)** | 동일 (KURE-v1 사용)              | 동일 (KURE-v1 사용)             |                |
| **초기화 속도**      | 모델 래퍼 로딩 오버헤드 존재     | 라이브러리 직접 로딩으로 빠름   |                |
| **데이터 처리 효율** | `Document` 객체 변환 비용 발생 | Raw 데이터(List/Dict) 즉시 사용 |                |
| **하드웨어 제어권**  | 추상화 계층에 의한 간접 제어     | GPU/메모리 명시적 직접 제어     |                |

---

## 2. 세부 성능 벤치마크 (Benchmark)

`benchmarks/performance_test.py` 스크립트를 통해 측정할 실제 데이터 기록란입니다.

### ① 객체 변환 및 검색 오버헤드 (Retrieval Overhead)

DB 검색 결과를 애플리케이션 데이터로 변환할 때의 시간 차이입니다.

| 측정 항목                     | Native 방식 (Raw) | LangChain (Document) | 차이 (배수) |
| :---------------------------- | :---------------- | :------------------- | :---------- |
| **변환 속도 (1,000건)** |                   |                      |             |

### ② 임베딩 초기화 및 연산 (Embedding Performance)

모델 로딩 및 문장 벡터화에 걸리는 시간입니다.

| 측정 항목                       | Native 방식 | LangChain 방식 | 차이 (배수) |
| :------------------------------ | :---------- | :------------- | :---------- |
| **모델 초기화(Loading)**  |             |                |             |
| **문장 임베딩(Encoding)** |             |                |             |

---

## 3. 테스트 데이터 기반 기술 분석 (작성 예정)

> **[안내]** 위 벤치마크 스크립트를 실행한 후, 출력되는 수치를 바탕으로 최종 시사점과 결론을 작성할 예정입니다.

### ❓ 무엇을 측정하고 비교할 것인가?

* 정확도가 아닌 **'응답 지연 시간(Latency)'**과 **'시스템 자원 효율성'**을 수치로 검증합니다.
* 프레임워크의 추상화 계층이 실제 '실시간 응답성'에 미치는 물리적 영향력을 파악합니다.
