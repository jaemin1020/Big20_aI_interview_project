# 01. 랭체인(LangChain) 도입 배경 및 모듈별 적용 현황

## 1. 랭체인 도입 배경: AI 워크플로우의 표준화와 확장성 확보

현대적인 LLM 애플리케이션, 특히 '에이전트형 AI 면접관'과 같이 복잡한 RAG(검색 증강 생성) 시스템에서는 단순한 모델 호출보다 **각 컴포넌트 간의 유기적인 연결과 상태 관리**가 훨씬 중요합니다. 본 프로젝트는 다음 세 가지 전략적 목표를 달성하기 위해 랭체인을 시스템의 중추로 채택하였습니다.

### ① 오케스트레이션(Orchestration)의 단일화

데이터 추출(PDF), 변환(Embedding), 저장(VectorStore), 추론(LLM)으로 이어지는 파편화된 복잡한 과정을 하나의 **선언적 파이프라인(LCEL)**으로 통합하여, 데이터의 흐름을 투명하게 관리하고 디버깅 효율을 극대화했습니다.

### ② 기술 부채 방지 및 모델 독립성 (Future-Proofing)

특정 모델 규격에 종속되지 않는 랭체인의 추상화 계층을 활용함으로써, 향후 로컬 GGUF 모델에서 클라우드 기반 API로, 혹은 새로운 벡터 데이터베이스로 교체하더라도 **비즈니스 로직의 수정 없이 인프라만 교체**할 수 있는 탄력적인 구조를 완성했습니다.

### ③ 출력 데이터의 결정론적 제어 (Reliable Output)

생성형 AI의 고질적인 문제인 '비정형 응답'을 랭체인의 `OutputParser`와 `Pydantic`으로 통제하여, 면접 점수와 피드백 등 핵심 데이터를 **DB에 즉시 삽입 가능한 정밀한 JSON 구조**로 수급하는 신뢰성을 확보했습니다.

---

## 2. 모듈별 랭체인 적용 현황 (Full Framework Integration)

본 프로젝트는 데이터 전처리부터 최종 추론까지 AI 파이프라인의 전 과정을 랭체인 생태계로 통합하였습니다.

### ① 데이터 전처리 모듈 (`tasks/chunking.py`)

> **활용 기능**: `RecursiveCharacterTextSplitter`
> **해결 과제**: 고정된 길이로 자를 경우 문맥이 끊기는 문제를 방지하기 위해, 문단(sep="\n\n") -> 문장(sep=".") -> 단어(sep=" ") 순으로 지능적으로 텍스트를 분할합니다.

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

def chunk_resume(parsed_data):
    # 의미 단위를 보존하며 최대 600자 내외로 조각(Chunk) 생성
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    # 실제 적용: 자소서와 같이 긴 텍스트를 논리적으로 쪼개어 RAG 정확도 향상
    split_texts = text_splitter.split_text(long_answer)
```

### ② 벡터 변환 모듈 (`tasks/embedding.py`)

> **활용 기능**: `HuggingFaceEmbeddings`
> **해결 과제**: 로컬 GPU/CPU 환경에 최적화된 `KURE-v1` 모델을 랭체인 표준 API로 감싸, 시스템의 다른 부분에서 모델의 복잡한 로드 과정을 신경 쓰지 않고 벡터화할 수 있게 합니다.

```python
from langchain_huggingface import HuggingFaceEmbeddings

def get_embedder(device='cuda'):
    # 로컬 모델 경로 및 장치 설정을 랭체인 인터페이스로 단일화
    return HuggingFaceEmbeddings(
        model_name="nlpai-lab/KURE-v1",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )
```

### ③ 데이터 저장 모듈 (`tasks/pgvector_store.py`)

> **활용 기능**: `PGVector`, `Document`
> **해결 과제**: 파싱된 데이터 조각들을 랭체인의 표준 `Document` 규격으로 포장하고, 단 한 줄의 명령으로 벡터화와 DB 저장을 동시에 수행합니다.

```python
from langchain_community.vectorstores import PGVector
from langchain_core.documents import Document

def store_embeddings(resume_id, chunks):
    # 데이터를 랭체인 표준 규격인 Document 객체로 포장
    documents = [Document(page_content=c['text'], metadata=c['metadata']) for c in chunks]
  
    # 벡터화와 저장을 단일 API로 처리 (자동 인덱스 생성)
    vector_store = PGVector.from_documents(
        embedding=get_embedder(),
        documents=documents,
        collection_name="resume_all_embeddings",
        connection_string=DATABASE_URL
    )
```

### ④ 컨텍스트 검색 모듈 (`tasks/rag_retrieval.py`)

> **활용 기능**: `PGVector.as_retriever()`, `similarity_search_with_score`
> **해결 과제**: 질문 생성 시 필요한 이력서 문맥과 질문 은행 데이터를 동일한 인터페이스로 검색하여, 추론 엔진(Chain)에 일관된 데이터를 공급합니다.

```python
def retrieve_context(query, resume_id, top_k=10):
    # 랭체인 검색기 객체 생성
    vector_store = PGVector(connection_string=URL, embedding_function=get_embedder())
  
    # 유사도 기반 검색 수행 및 관련 문맥 반환
    docs_with_scores = vector_store.similarity_search_with_score(
        query, k=top_k, filter={"resume_id": resume_id}
    )
    return docs_with_scores
```

### ⑤ 질문 생성 모듈 (`tasks/question_generator.py`)

> **활용 기능**: `PromptTemplate`, `LCEL (Chain)`, `StrOutputParser`
> **해결 과제**: 면접관의 페르소나와 준수 수칙을 담은 복잡한 프롬프트를 직관적인 체인 구조(`|`)로 실행하여 코드 가독성과 디버깅 효율을 높입니다.

```python
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 프롬프트 -> 모델 -> 출력 파서를 하나의 파이프라인으로 연결
chain = PromptTemplate.from_template(TEMPLATE) | get_exaone_llm() | StrOutputParser()

# 실시간 문맥 정보를 주입하여 질문 생성 실행
question = chain.invoke({"context": context, "stage": "기술면접"})
```

### ⑥ 답변 평가 모듈 (`tasks/evaluator.py`)

> **활용 기능**: `JsonOutputParser`, `Pydantic (BaseModel)`
> **해결 과제**: AI가 생성한 자유 형식의 평가 답변을 미리 정의된 `Schema`에 맞춰 강제로 JSON 포맷으로 변환, DB 저장 프로세스를 완벽히 자동화합니다.

```python
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field

class AnswerEval(BaseModel):
    total_score: int = Field(description="역량 점수")
    feedback: str = Field(description="평가 사유")

# 모델의 답변을 즉시 파이썬 딕셔너리 객체로 변환
parser = JsonOutputParser(pydantic_object=AnswerEval)
eval_chain = prompt | llm | parser
result = eval_chain.invoke({"answer": user_answer}) # result는 즉시 dict 타입
```

## 3. 개발 전략: 생산성과 유지보수를 위한 Full LangChain 통합

본 프로젝트는 **서비스의 안정적인 운영과 지속 가능한 코드 관리**에 초점을 맞추었습니다. 벤치마크 테스트를 통해 프레임워크 도입에 따른 오버헤드가 실시간 서비스 품질에 미치는 영향이 극히 미미함을 정량적으로 확인하였으며, 이에 따라 모든 AI 데이터 경로를 랭체인으로 단일화하는 전략적 결정을 내렸습니다.

### 📍 프레임워크 중심 아키텍처의 당위성

1. **개발 생산성의 극대화**: 랭체인이 제공하는 수많은 사전 정의된 컴포넌트(`Splitter`, `VectorStore`, `Parser`)를 활용하여, 핵심 비즈니스 로직(면접 시나리오 설계 등)에 개발 리소스를 집중할 수 있었습니다.
2. **팀 협업의 표준화**: 각 개발자가 각기 다른 방식으로 Native 코드를 짜는 대신, 랭체인이라는 공통 언어를 통해 AI 파이프라인을 설계함으로써 코드 가독성을 높이고 인수인계 및 협업의 장벽을 제거했습니다.
3. **검증된 신뢰성**: 전 세계 수만 개의 프로젝트에서 검증된 랭체인의 엔진을 활용함으로써, 직접 구현 시 발생할 수 있는 잠재적인 엣지 케이스(Edge Case) 에러를 사전에 차단했습니다.

### 📑 전략적 도입 분석 요약 (Pragmatic Framework Adoption)

> **1. 관리 효율성 (Governance)**: 전 과정이 랭체인 규격을 따름으로써 시스템 전반의 AI 워크플로우에 대한 강력한 통제권 확보.
> **2. 신속한 대응 (Agility)**: 새로운 모델이나 기술이 등장할 때, 프레임워크의 업데이트만으로 최신 트렌드를 즉시 프로젝트에 반영할 수 있는 구조 구축.
> **3. 최종 결론**: "미세한 연산 속도의 이점보다, 프레임워크가 제공하는 '압도적인 생산성'과 '시스템의 안정성'이 현대적인 AI 서비스를 구축하는 데 있어 더욱 중요한 핵심 성공 요인임을 입증하였습니다."

---

## 4. 도입 전/후 코드 비교 (대표 사례: 질문 생성)

본 프로젝트의 **질문 생성 모듈 (`tasks/question_generator.py`)**을 기준으로 랭체인 도입 전(예상)과 도입 후(실제)의 코드를 비교하였습니다.

### [Before] Native Implementation (추상화 미적용 시 예상)

> 복잡한 문자열 포맷팅과 모델 호출 로직이 섞여 유지보수가 어렵고, 모델 변경 시 모든 API 규격을 새로 맞춰야 함.

```python
# 1. 수동 프롬프트 포맷팅
full_prompt = PROMPT_TEMPLATE.replace("{context}", context_text) \
                             .replace("{stage_name}", stage_name) \
                             .replace("{guide}", guide)
                         
# 2. 모델별 전용 SDK 또는 API 호출 (모델 변경 시 이 부분 전체 수정 필요)
import requests
response = requests.post("http://local-llm-api/generate", json={
    "prompt": full_prompt,
    "max_tokens": 512,
    "stop": ["\n"]
})
raw_text = response.json()["choices"][0]["text"]

# 3. 수동 결과 정제 (Regex 활용)
final_content = raw_text.strip().replace("\"", "")
```

### [After] LangChain Architecture (실제 프로젝트 모듈 적용)

> **LCEL(파이프 연산자)**을 사용하여 로직을 선언적으로 정의하며, `get_exaone_llm()`을 통해 어떤 모델이든 동일한 인터페이스로 호출 가능.

```python
# [tasks/question_generator.py 실제 적용 코드]

# 1. 랭체인 기반 표준화된 인터페이스 정의
llm = get_exaone_llm()  # 로컬 EXAONE 모델을 랭체인 표준 객체로 로드
prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
chain = prompt | llm | StrOutputParser() # LCEL 파이프라인 형성

# 2. 변수 주입 및 체인 실행 (동기/비동기/배치 모두 동일 문법)
final_content = chain.invoke({
    "context": context_text,
    "stage_name": next_stage['display_name'],
    "company_ideal": company_ideal,
    "guide": guide_formatted,
    "mode_instruction": mode_instruction,
    "target_role": target_role
})
```

---

## 4. 도입 전/후 아키텍처 비교 요약

> **[Before] Native Implementation**
>
> - 프롬프트 문자열과 파이썬 로직이 뒤섞인 비대화(Bloated)된 함수
> - 모델 호출 실패 시 복잡한 재시도 로직 직접 구현 필요
> - RAG 구현 시 문서 분할 및 검색 로직 수동 관리

> **[After] LangChain Architecture**
>
> - **모듈화**: 프롬프트, 모델, 로직이 각각 독립된 블록으로 관리됨
> - **안정성**: 내장된 체인 실행 옵션으로 배치 처리 및 비동기 호출 용이
> - **확장성**: 추후 LangGraph 등을 활용한 복잡한 에이전트 구조로의 전환 기반 마련
