# 📌 LLM 프롬프트 우선순위 분석 (실제 코드 템플릿 기준)

## 0️⃣ 가장 강력한 계층: 🔒 출력 규칙 블록

```
### [출력 규칙 - 반드시 준수]
1. 인사말 금지
2. 서두 금지
3. 물음표로 끝나는 단일 문장
4. 하십시오체
```

### 왜 최상위인가?

* “반드시 준수”
* 번호로 구조화됨
* 명확한 금지형 문장
* 출력 형식 직접 통제

LLM은 이런 **명시적, 절대적, 구조화된 제약**을 가장 높은 확률 가중치로 반영해.

👉 실제 체감 우선순위: ★★★★★ (절대 상위)

이 블록이 사실상 전체 출력을 “락(lock)” 걸고 있음.

---

## 1️⃣ 두 번째 계층: 🧠 실시간 핵심 임무 블록

```
- 수행 과업: {mode_task_instruction}
- 실행 상세: {mode_instruction}
- 전역 제약: {global_constraint}
```

여기가 실질적인 질문 생성 로직을 지배해.

### 내부 우선순위

1. global_constraint
2. mode_instruction
3. mode_task_instruction

왜냐하면:

* global_constraint = 금지/형식 제약 → 강함
* mode_instruction = 구조 통제
* mode_task_instruction = 내용의 날카로움

형식 > 구조 > 내용

👉 전체 우선순위: ★★★★☆

---

## 2️⃣ 세 번째 계층: 🎭 면접 전략 및 페르소나

```
- 평가 대상 직무
- 핵심 인재상
- 면접 단계 (guide 포함)
```

이 영역은 **톤과 방향성 조정용 메타 정보**야.

이건 “어떤 질문을 만들 것인가”에 영향을 주지만
“어떻게 출력할 것인가”를 통제하지는 못해.

👉 우선순위: ★★★☆☆

---

## 3️⃣ 네 번째 계층: 📚 참고 문맥 (context)

지원자의 이전 답변 내용.

이건 질문 소재를 제공하는 역할이야.
하지만 출력 규칙을 깨뜨릴 정도로 강하지는 않음.

👉 우선순위: ★★☆☆☆

---

# 🔥 최종 실제 작동 우선순위

LLM이 이 템플릿을 받을 때 실제로 따를 가능성이 높은 순서는:

```
1. [출력 규칙 - 반드시 준수]
2. global_constraint
3. mode_instruction
4. mode_task_instruction
5. 면접 전략 및 페르소나
6. context
```

---

# 📌 중요한 포인트

### 💥 이 템플릿의 핵심 특징

출력 규칙이 맨 아래에 있지만
실제로는 **가장 강력한 통제 블록**으로 작동한다.

왜?

* "반드시 준수"
* 번호로 분리
* 구체적
* 출력 형식 직접 지정

LLM은 이런 구조화된 절대 조건을 강하게 따르는 경향이 있음.

---

# ⚠️ 실제 현업에서 생기는 현상

만약 아래처럼 충돌이 나면:

* global_constraint: 물음표 쓰지 마라
* 출력 규칙: 물음표로 끝내라

👉 거의 항상 **출력 규칙 쪽이 이김**

왜냐하면:

* 더 구체적이고
* 더 직접적으로 출력에 연결되어 있고
* 더 명확하게 “반드시”라고 명시되어 있음

---

# 🧠 결론

이 코드 템플릿에서의 실질적 우선순위는:

---

# 🔍 랭스미스(LangSmith) 출력 vs 실제 프롬프트 비교

위의 우선순위 분석이 중요한 진짜 이유는 **랭스미스에서 보는 것과 LLM이 읽는 것이 다르기 때문**입니다.

### 1. 랭스미스 "Inputs" 탭 (표면적인 모습)
랭스미스 디버깅 시 개발자에게 보이는 `target_role`, `company_ideal`, `context` 등은 개별적인 **'변수 리스트'**입니다.
*   **특징**: 각 항목이 독립적인 객체처럼 나열됨 (`v context`, `v mode_instruction` 등).
*   **착각**: AI가 이 항목들을 순서 없이 골고루 참고할 것이라 착각하기 쉬움.

### 2. 실제 코드 및 LLM 호출 (실질적인 모습)
`question_generator.py`에서 `chain.invoke()`가 실행되는 순간, 이 모든 변수는 하나의 **거대한 텍스트 덩어리**로 합쳐집니다.

```python
# 실제 내부 동작 (Formatting)
final_prompt = PROMPT_TEMPLATE.format(
    target_role="백엔드 개발",
    company_ideal="열정, 창의, 혁신...",
    context="[이전 질문: ... 지원자 답변: ...]",
    ...
)
```

### 3. 왜 랭스미스 화면과 다르게 작동하는가?

1.  **순서의 힘 (Recency Bias)**: 텍스트가 하나로 합쳐지면 LLM은 이를 위에서 아래로 읽습니다. 랭스미스에서는 변수들이 따로 놀지만, 실제로는 **[출력 규칙]이 가장 마지막에 위치**하여 AI의 머릿속에 질문을 내뱉기 직전 '가장 강렬한 인상'을 남깁니다.
2.  **구조의 힘**: 랭스미스 상의 `context`는 단순히 데이터일 뿐이지만, 실제 프롬프트 속에서는 `### [참고 문맥]`이라는 제목 아래에 배치됩니다. 이 제목(Header)들이 LLM에게 "여기는 데이터 영역이고, 저기 아래는 너가 지켜야 할 법(Rule) 영역이야"라는 **위계를 전달**합니다.

### 🎯 결론
랭스미스에서 보여주는 **'변수(Input)'**들은 질문의 식재료일 뿐이고, 실제 질문의 맛(결과물)은 **'프롬프트 템플릿(Recipe)'**의 하단 배치를 통한 **우선순위 역전 설계**에서 결정됩니다.

따라서 랭스미스 로그 분석 시 변수 내용만 볼 것이 아니라, **해당 변수가 프롬프트 템플릿의 어느 위치(상단vs하단)에 박혀서 전달되는지**를 반드시 확인해야 합니다.
