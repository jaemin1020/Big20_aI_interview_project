
---

# 📑 [기술 보고서] 이력서 임베딩 엔진: 텍스트의 수치화 및 최적화 전략

본 문서는 한국어 문장 유사도에 특화된 `KURE-v1` 모델을 활용하여, 청킹된 이력서 조각들을 고차원 벡터로 변환하는 기술적 매커니즘을 상세히 다룹니다.

---

## 1. 임베딩(Embedding)의 본질: "언어를 숫자로"

인공지능은 텍스트를 직접 읽지 못합니다. 대신 텍스트를 수백 개의 숫자 리스트인 **벡터(Vector)**로 변환하여 **'의미의 공간'**에 점을 찍습니다.

* **원리**: 비슷한 의미를 가진 문장(예: "Python 개발 경험", "파이썬 프로젝트 수행")은 이 공간에서 서로 가까운 거리에 위치하게 됩니다.
* **활용**: 나중에 사용자가 질문을 던지면, 질문의 좌표와 가장 가까운 곳에 있는 이력서 조각을 찾아내는 **시맨틱 검색(Semantic Search)**이 가능해집니다.

---

## 2. 자원 최적화 전략: 싱글톤(Singleton) 패턴

모델 로딩은 메모리를 기가바이트(GB) 단위로 점유하고 수 초 이상의 시간을 소모하는 **'비싼 작업'**입니다.

```python
_embedder = None # 전역 변수

def get_embedder(device):
    global _embedder
    if _embedder is None: # 처음 한 번만 실행
        _embedder = HuggingFaceEmbeddings(...)
    return _embedder

```

### 🔍 깊이 있는 분석

* **메모리 보호**: 만약 함수를 호출할 때마다 모델을 새로 로드한다면, 동시 접속자가 많아질 때 서버의 RAM이 즉시 고갈되어 시스템이 붕괴(OOM, Out of Memory)됩니다.
* **지연 시간(Latency) 최소화**: 첫 호출 이후에는 메모리에 상주(Resident) 중인 모델을 즉시 반환하므로, 실제 임베딩 처리 속도가 비약적으로 빨라집니다.

---

## 3. 핵심 로직 상세 분석 (`embed_chunks`)

실제 조각들을 벡터로 바꾸는 과정에서 사용된 기술적 포인트들입니다.

### ① 하드웨어 가속 자동 감지

```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'

```

* **로직**: 시스템에 NVIDIA GPU가 있는지 확인합니다. GPU(`cuda`)가 있다면 수백 개의 문장을 병렬로 처리하여 속도를 수십 배 높이고, 없다면 안정적인 `cpu` 모드로 작동합니다.

### ② 수학적 정규화 (Normalization)

```python
encode_kwargs={'normalize_embeddings': True}

```

* **수학적 의미**: 생성된 벡터의 길이를 모두 **1**로 맞추는 작업입니다.
* **이유**: 검색 시 사용하는 **코사인 유사도(Cosine Similarity)** 계산을 단순화하고 정확도를 높이기 위해서입니다. 벡터의 길이와 상관없이 '방향'만으로 유사도를 판별하게 하여, 문장의 길이에 의한 왜곡을 방지합니다.

$$
\text{Similarity} = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}
$$

### ③ 데이터 결합 (Mapping)

```python
embedded_result.append({
    "text": c["text"],
    "vector": vectors[i] # AI가 계산한 좌표값
})

```

* **분석**: 벡터 자체는 숫자 덩어리일 뿐이라 사람이 읽을 수 없습니다. 따라서 **"이 숫자가 어떤 글자에서 나왔는지"**를 알 수 있도록 원본 텍스트 및 메타데이터와 한 세트로 묶어주는 과정이 필수적입니다.

---

## 4. `KURE-v1` 모델의 특성 분석

본 시스템에서 채택한 `nlpai-lab/KURE-v1`은 한국어 검색 및 문장 유사도 성능이 매우 뛰어난 모델입니다.

| 특징                     | 설명                                   | 효과                                               |
| ------------------------ | -------------------------------------- | -------------------------------------------------- |
| **한국어 특화**    | 한국어의 구어체와 문어체를 모두 학습   | 이력서 특유의 딱딱한 말투도 정확히 이해함          |
| **Pure Embedding** | 접두어(Instruction) 없이도 고성능 발휘 | 데이터 전처리 복잡도를 낮추고 원문의 의미를 보존함 |
| **고차원 벡터**    | 문장을 수백 개의 숫자로 정밀하게 묘사  | 미세한 의미 차이(예: '팀원'과 '팀장')를 구분해냄   |

---

## 5. 결론 및 RAG 시스템에서의 역할

이 코드가 완성됨으로써, 앞서 수행한 **파싱(추출)**과 **청킹(분할)** 데이터는 비로소 **'기계가 읽을 수 있는 지식'**으로 탈바꿈했습니다. 이제 이 데이터는 벡터 데이터베이스(ChromaDB, Pinecone 등)에 저장되어, **EXAONE 3.5** 모델이 답변을 생성할 때 가장 신뢰할 수 있는 근거 자료로 활용될 준비를 마쳤습니다.

---
