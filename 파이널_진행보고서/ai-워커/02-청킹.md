
# 📑 [기술 명세서] 이력서 AI 분석 시스템: 파싱 및 청킹 엔진 심층 분석

본 문서는 PDF에서 추출된 비정형 데이터를 AI가 가장 효율적으로 이해할 수 있는 형태인 **'의미 단위의 조각(Chunk)'**으로 재구성하는 기술적 매커니즘을 상세히 다룹니다.

---

## 1. 텍스트 분할 전략: `RecursiveCharacterTextSplitter`

가장 먼저 살펴볼 부분은 데이터를 자르는 **'도구'**의 설정입니다. 단순히 글자 수로 자르는 것은 AI에게 문맥이 잘린 쓰레기 데이터를 주는 것과 같습니다.

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,
    chunk_overlap=100,
    separators=["\n\n", "\n", ".", " ", ""]
)

```

### 🔍 깊이 있는 분석

* **재귀적 분할 (Recursive Splitting)**: 이 분할기는 `separators` 리스트의 순서대로 자를 지점을 찾습니다.

1. 가장 먼저 **문단(`\n\n`)**을 찾습니다. 문단이 통째로 600자 이내라면 그대로 한 조각이 됩니다.
2. 문단이 너무 길면 **줄바꿈(`\n`)**을 찾고, 그다음은 **마침표(`.`)**를 찾습니다.
3. 결과적으로 문장 중간이 툭 끊기지 않고, 최대한 의미가 완결되는 지점에서 조각이 나뉩니다.

* **청크 오버랩 (100자)**: 조각 A의 끝 100자와 조각 B의 앞 100자를 겹치게 만듭니다. 이는 AI가 검색된 조각만 읽었을 때 앞뒤 문맥을 몰라 발생하는 **'할루시네이션(환각)'**을 방지하는 아주 중요한 장치입니다.

---

## 2. 항목별 데이터 처리 로직 및 문법 분석

코드는 각 항목의 특성에 맞춰 데이터를 **'시맨틱(Semantic) 문장'**으로 가공합니다.

### ① 안전한 데이터 접근 (`.get()` 문법)

```python
header = parsed_data.get("header", {})
educations = parsed_data.get("education", [])

```

* **분석**: `parsed_data["header"]`라고 직접 접근하면 해당 키가 없을 때 프로그램이 즉시 종료됩니다. `.get("key", default)`를 사용하여 데이터가 누락된 이력서라도 에러 없이 유연하게 넘어가도록 설계되었습니다.

### ② 정형 데이터의 자연어 변환 (Feature Engineering)

```python
text = f"[학력] {school} {major} ({status})"
if period: text += f" - {period}"
if gpa: text += f", 학점: {gpa}"

```

* **분석**: 흩어져 있는 학교, 전공, 학점 데이터를 하나의 완성된 문장으로 합칩니다.
* **이유**: AI 모델(LLM)은 키-값 형태의 데이터보다 **"지원자는 한국대학교에서 컴퓨터공학을 전공하고 졸업했습니다."**와 같은 자연어 형태에서 훨씬 더 높은 검색 정확도를 보이기 때문입니다.

### ③ 프로젝트 및 자기소개서의 지능적 분할

이 부분은 이력서에서 가장 긴 텍스트가 발생하는 구간입니다.

```python
if len(text) > 400:
    split_texts = text_splitter.split_text(text)
    for i, st in enumerate(split_texts):
        chunks.append({
            "text": f"(부분 {i+1}) {st}",
            "metadata": { ..., "title": title }
        })

```

* **로직**: 텍스트가 400자를 초과하면 위에서 정의한 `text_splitter`를 가동합니다.
* **`enumerate`의 활용**: 조각난 텍스트에 `(부분 1)`, `(부분 2)`와 같은 인덱스를 붙여, 나중에 AI가 "이 내용은 프로젝트의 앞부분이구나"라고 인지할 수 있게 돕습니다.

---

## 3. EXAONE 3.5 (32k) 모델 환경에서 이 코드가 빛나는 이유

"EXAONE 3.5는 한 번에 32,000 토큰을 읽는데, 왜 600자씩 자르나요?"라는 의문에 대한 기술적 답변입니다.

### 3.1 "Lost in the Middle" 현상 원천 차단

아무리 컨텍스트 창이 넓어도 모델은 입력된 텍스트의 **중간 부분에 있는 정보**를 가장 잘 놓칩니다. 600자 단위의 명확한 조각을 주면 모델은 그 정보에 100% 집중할 수 있습니다.

### 3.2 수천 명의 데이터 확장성 (Vector Search)

질문이 들어왔을 때 이력서 1,000장을 한꺼번에 모델에 넣으면 비용과 시간이 엄청나게 소모됩니다.

* **해결**: 청킹된 데이터는 벡터 DB에 저장됩니다. 사용자가 "Python 경험자 찾아줘"라고 하면, 이 코드에서 만든 `category: experience`인 조각 중 Python이 언급된 **핀포인트 조각 5개만** 골라 EXAONE 모델에게 전달합니다. 이것이 바로 고효율 RAG의 핵심입니다.

---

## 4. 메타데이터(Metadata) 설계 명세

이 코드는 텍스트만 저장하는 것이 아니라, 각 조각에 **'디지털 이름표'**를 붙입니다.

| 필드명                     | 데이터 예시              | 역할                                                    |
| -------------------------- | ------------------------ | ------------------------------------------------------- |
| **`source`**       | "resume"                 | 데이터의 출처가 이력서임을 명시                         |
| **`category`**     | "narrative", "education" | 특정 항목(예: 학력만, 자소서만)에 대한 필터링 검색 지원 |
| **`subtype`**      | "answer", "question"     | 자기소개서의 질문과 답변을 구분하여 맥락 파악력 향상    |
| **`question_ref`** | "지원동기를 쓰시오..."   | 조각난 답변이 어떤 질문에 대한 것인지 연결고리 제공     |

---

## 💡 최종 결론

본 시스템의 `chunk_resume` 함수는 단순히 텍스트를 자르는 기능적 역할을 넘어, **비정형 이력서 데이터를 AI가 가장 선호하는 '고품질 지식 조각'으로 승화시키는 전처리 공정**입니다.

특히 **EXAONE 3.5**와 같은 대형 모델과 결합될 때, 이 정교한 청킹 전략은 모델의 추론 속도를 높이고, 비용을 절감하며, 무엇보다 **정보 검색의 정확도를 비약적으로 상승**시키는 핵심 엔진 역할을 수행합니다.
