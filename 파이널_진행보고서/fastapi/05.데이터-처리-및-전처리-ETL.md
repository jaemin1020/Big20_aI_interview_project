# 📊 데이터 처리 및 전처리(ETL) 파이프라인 보고서

---

## 1. 개요
본 보고서는 지원자의 이력서(PDF)를 AI가 이해할 수 있는 형태의 지식 베이스로 변환하는 **데이터 전처리 및 ETL(Extract, Transform, Load)** 과정을 기술한다.

---

## 2. 데이터 처리 아키텍처

1.  **Extract (추출)**: PDF 파일에서 텍스트 및 구조 정보를 추출.
2.  **Transform (변환)**: 비정형 텍스트를 JSON 구조로 변환 후, 의미 단위로 분할(Chunking).
3.  **Load (적재)**: 분할된 데이터에 메타데이터를 부여하여 벡터 DB에 저장.

---

## 3. 현 프로젝트 적용 사례 (코드 및 로직)

### 3.1 PDF 구조화 및 파싱 (tasks/parse_resume.py)
단순한 텍스트 추출이 아니라, LLM을 활용해 학력, 경력, 프로젝트, 자기소개서 등 섹션별로 구조화된 JSON 데이터를 생성한다.

### 3.2 의미 기반 텍스트 분할 (tasks/chunking.py)
AI가 한 번에 읽기 적당한 크기로 데이터를 자르되, 문맥이 끊기지 않도록 **RecursiveCharacterTextSplitter**를 사용한다.

```python
# [청킹 설정 사례]
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,      # 한 조각 최대 600자
    chunk_overlap=100,   # 조각 간 100자 겹침 (문맥 유지)
    separators=["\n\n", "\n", ".", " "] # 단락 -> 문장 순으로 분할 시도
)
```

### 3.3 섹션별 맞춤형 메타데이터 부여
검색의 정확도를 높이기 위해 각 데이터 조각에 '태그'를 붙인다.

```python
# [메타데이터 부여 사례]
chunks.append({
    "type": "project",
    "text": f"[프로젝트] 명칭: {title} \n상세: {desc}",
    "metadata": { 
        "source": "resume", 
        "category": "project", # 어떤 영역인지 명시
        "title": title         # 나중에 필터링 시 활용 가능
    }
})
```

---

## 4. 종합 평가 및 강점

| 항목 | 일반적인 텍스트 분할 | 우리 프로젝트 (의미론적 전처리) |
| :--- | :--- | :--- |
| **분할 방식** | 단순히 글자 수로 자름 | **구조(JSON) 파싱 후 논리적 단위로 분할** |
| **문맥 보존** | 앞뒤 내용이 잘림 | **Overlap 설정을 통해 문맥 연속성 확보** |
| **검색 효율** | 전체 데이터에서 난잡한 검색 | **카테고리별(경력, 학력 등) 타겟 검색 가능** |
| **데이터 품질** | 불필요한 특수문자 포함 | **정규표현식 및 LLM 정제를 거친 깔끔한 데이터** |

---

## 5. 결론
우리 프로젝트의 ETL 파이프라인은 단순히 데이터를 '옮기는' 것이 아니라, AI 면접관이 **"지원자의 특정 프로젝트 경험을 콕 집어 질문"**할 수 있도록 데이터를 **"가공하고 의미를 부여"**하는 데 초점이 맞춰져 있다. 이는 최종적으로 생성되는 질문의 고도화된 품질로 이어진다.

---

**[작성 위치]**
*   파일 경로: `C:\big20\Big20_aI_interview_project\파이널_진행보고서\백엔드\05.데이터-처리-및-전처리-ETL.md`
