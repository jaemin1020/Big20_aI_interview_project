services:
  # 1. Database: PostgreSQL + pgvector
  db:
    image: pgvector/pgvector:pg18
    container_name: interview_db
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./infra/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
      - postgres_data:/var/lib/postgresql # PostgreSQL 18+ 권장 경로
      - ./backend-core/data:/data
    ports:
      - "5432:5432" # 김린만 15432 , 다른모든분은 5432쓰시면 됩니다.

    networks:
      - interview_network

  # 2. Redis: Task Queue & Session Cache
  redis:
    image: redis:7-alpine
    container_name: interview_redis
    restart: always
    ports:
      - "6379:6379"
    networks:
      - interview_network

  # 3. Backend Core: FastAPI
  backend:
    build: ./backend-core
    container_name: interview_backend
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN}
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-true}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-Big20-AI-Interview}
    depends_on:
      - redis
      - db
    volumes:
      - ./backend-core:/app
    networks:
      - interview_network

  # 4-1. AI Worker GPU: 질문 생성 전용 (EXAONE GPU 로드)
  ai-worker-gpu:
    build:
      context: ./ai-worker
      dockerfile: Dockerfile
    working_dir: /app
    container_name: interview_worker_gpu
    # gpu_queue 전용 (배달 사고 방지를 위해 기본 celery 큐는 CPU 워커가 전담)
    command: celery -A main.app worker --loglevel=info -Q gpu_queue --pool=solo
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - CELERY_BROKER_URL=redis://redis:6379/0
      - N_GPU_LAYERS=-1
      - USE_GPU=true
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN}
      - HF_HOME=/app/models/.cache
      - DEEPFACE_HOME=/app/models/.deepface
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-true}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-Big20-AI-Interview}
    depends_on:
      - redis
      - db
    volumes:
      - ./ai-worker:/app
      - ./ai-worker/models:/app/models
      - ./backend-core/uploads:/app/uploads
      - ./backend-core:/backend-core
    networks:
      - interview_network

  # 4-2. AI Worker CPU: 답변 분석 및 기타 전처리 (EXAONE CPU 로드)
  ai-worker-cpu:
    build:
      context: ./ai-worker
      dockerfile: Dockerfile
    working_dir: /app
    container_name: interview_worker_cpu
    # cpu_queue와 기본 celery 큐를 함께 처리 (STT 동시 처리를 위해 threads 풀 사용)
    command: celery -A main.app worker --loglevel=info -Q cpu_queue,celery --pool=threads --concurrency=4
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 16G
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - CELERY_BROKER_URL=redis://redis:6379/0
      - N_GPU_LAYERS=0
      - USE_GPU=false
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN}
      - HF_HOME=/app/models/.cache
      - DEEPFACE_HOME=/app/models/.deepface
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-true}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT:-https://api.smith.langchain.com}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-Big20-AI-Interview}
    depends_on:
      - redis
      - db
    volumes:
      - ./ai-worker:/app
      - ./ai-worker/models:/app/models
      - ./backend-core/uploads:/app/uploads
      - ./backend-core:/backend-core
    networks:
      - interview_network

  # 5. Media Server: WebRTC & STT
  media-server:
    build: ./media-server
    container_name: interview_media
    ports:
      - "8080:8080"
      - "50000-50050:50000-50050/udp" # [추가] WebRTC Media Ports (UDP)
    environment:
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - BACKEND_URL=http://backend:8000
      - REDIS_URL=redis://redis:6379/0
      - PYTHONUNBUFFERED=1
    depends_on:
      - backend
      - redis
    volumes:
      - ./media-server:/app
    networks:
      - interview_network

  # 6. Frontend: React (Vite/CRA)
  frontend:
    build:
      context: ./frontend
    container_name: interview_react_web
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8000
      - VITE_DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
    depends_on:
      - backend
    networks:
      - interview_network

networks:

  interview_network:
    driver: bridge

volumes:
  postgres_data:
