import json
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter

def chunk_resume(parsed_data):
    chunks = []
    print("\n[STEP4] ë°ì´í„° ì²­í‚¹(Chunking) ì‹œì‘...")

    # ====================================================
    # âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
    # chunk_size=500: ìì†Œì„œ ë“± ê¸´ ê¸€ì„ ìë¥¼ ë•Œ ì‚¬ìš©
    # chunk_overlap=50: ë¬¸ë§¥ ìœ ì§€
    # ====================================================
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=70,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    # ----------------------------------------------------
    # 1. í—¤ë” (Header)
    # ----------------------------------------------------
    header = parsed_data.get("header", {})
    if header:
        name = header.get("name", "")
        role = header.get("target_role", "")
        company = header.get("target_company", "")
        chunks.append({
            "type": "header",
            "text": f"[í”„ë¡œí•„] ì´ë¦„: {name}, ì§€ì›ì§ë¬´: {role}, ì§€ì›íšŒì‚¬: {company}",
            "metadata": { "source": "resume", "category": "profile" }
        })

    # ----------------------------------------------------
    # 2. í•™ë ¥ (Education)
    # ----------------------------------------------------
    educations = parsed_data.get("education", [])
    for edu in educations:
        school = edu.get("school_name", "")
        major = edu.get("major", "")
        period = edu.get("period", "")
        gpa = edu.get("gpa", "")
        status = edu.get("status", "")
        
        text = f"[í•™ë ¥] {school} {major} ({status})"
        if period: text += f" - {period}"
        if gpa: text += f", í•™ì : {gpa}"
        
        chunks.append({
            "type": "education",
            "text": text,
            "metadata": { "source": "resume", "category": "education", "school": school }
        })

    # ----------------------------------------------------
    # 3. í™œë™ ë° ê²½ë ¥ (Activities)
    # ----------------------------------------------------
    activities = parsed_data.get("activities", [])
    for act in activities:
        org = act.get("organization", "")
        role = act.get("role", "")
        content = act.get("content", "")
        period = act.get("period", "")
        
        text = f"[í™œë™] {content}"
        if org: text += f" ({org})"
        if role: text += f" - {role}"
        if period: text += f" [{period}]"

        chunks.append({
            "type": "activity",
            "text": text,
            "metadata": { "source": "resume", "category": "activity", "org": org }
        })

    # ----------------------------------------------------
    # 4. ìˆ˜ìƒ (Awards)
    # ----------------------------------------------------
    awards = parsed_data.get("awards", [])
    for awd in awards:
        title = awd.get("title", "")
        grade = awd.get("grade", "")
        org = awd.get("organization", "")
        date = awd.get("date", "")
        
        text = f"[ìˆ˜ìƒ] {title}"
        if grade: text += f" ({grade})"
        if org: text += f" - {org}"
        if date: text += f" [{date}]"

        chunks.append({
            "type": "award",
            "text": text,
            "metadata": { "source": "resume", "category": "award" }
        })

    # ----------------------------------------------------
    # 5. í”„ë¡œì íŠ¸ (Projects)
    # ----------------------------------------------------
    projects = parsed_data.get("projects", [])
    for proj in projects:
        title = proj.get("title", "")
        period = proj.get("period", "")
        desc = proj.get("description", "") # ê¸°ê´€ ì •ë³´ ë“±ì´ ë“¤ì–´ìˆìŒ
        
        text = f"[í”„ë¡œì íŠ¸] {title}"
        if period: text += f" ({period})"
        if desc: text += f" - {desc}"

        chunks.append({
            "type": "project",
            "text": text,
            "metadata": { "source": "resume", "category": "project" }
        })

    # ----------------------------------------------------
    # 6. ìê²©ì¦ (Certifications)
    # ----------------------------------------------------
    certs = parsed_data.get("certifications", [])
    for cert in certs:
        title = cert.get("title", "")
        date = cert.get("date", "")
        org = cert.get("organization", "")

        text = f"[ìê²©ì¦] {title}"
        if date: text += f" ({date})"
        if org: text += f" - {org}"
        
        chunks.append({
            "type": "certification",
            "text": text,
            "metadata": { "source": "resume", "category": "certification" }
        })

    # ----------------------------------------------------
    # 7. ìê¸°ì†Œê°œì„œ (Self Intro) - ğŸ”¥ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  ğŸ”¥
    # ----------------------------------------------------
    self_intros = parsed_data.get("self_intro", [])
    for idx, intro in enumerate(self_intros):
        question = intro.get("question", "")
        answer = intro.get("answer", "")
        
        # ì§ˆë¬¸(Question) ìì²´ë„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì €ì¥ (ê²€ìƒ‰ ìš©ì´ì„±)
        chunks.append({
            "type": "narrative_q",
            "text": f"[ìì†Œì„œ ì§ˆë¬¸{idx+1}] {question}",
            "metadata": { "source": "resume", "category": "narrative", "subtype": "question" }
        })

        # ë‹µë³€(Answer)ì´ ê¸¸ë©´ ìª¼ê°œì„œ ì €ì¥
        if answer:
            split_texts = text_splitter.split_text(answer)
            for i, split_text in enumerate(split_texts):
                chunks.append({
                    "type": "narrative_a",
                    "text": f"[ìì†Œì„œ ë‹µë³€{idx+1}-{i+1}] {split_text}",
                    "metadata": {
                        "source": "resume",
                        "category": "narrative",
                        "subtype": "answer",
                        "question_ref": question[:20] + "..." # ì–´ë–¤ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì¸ì§€ ì‚´ì§ í‘œì‹œ
                    }
                })

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print(f"\nâœ… ì´ {len(chunks)}ê°œì˜ ì²­í¬(Chunk) ìƒì„± ì™„ë£Œ")
    return chunks

# -----------------------------------------------------------
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ (íŒŒì¼ë¡œ ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€ë¨)
# -----------------------------------------------------------
if __name__ == "__main__":
    import os
    from step2_parse_resume import parse_resume_final
    
    # í…ŒìŠ¤íŠ¸í•  íŒŒì¼ ê²½ë¡œ í™•ì¸
    target_pdf = "resume.pdf"
    if not os.path.exists(target_pdf):
        target_pdf = "/app/resume.pdf"
    
    if os.path.exists(target_pdf):
        print(f"ğŸ“‚ íŒŒì¼ ë¡œë“œ: {target_pdf}")
        
        # 1. íŒŒì‹± (Step 2)
        parsed_data = parse_resume_final(target_pdf)
        
        if parsed_data:
            # 2. ì²­í‚¹ (Step 4)
            chunks = chunk_resume(parsed_data)
            
            # ----------------------------------------------------
            # [ì¶”ê°€ë¨] ê²°ê³¼ë¥¼ ëˆˆìœ¼ë¡œ í™•ì¸í•˜ê¸° ìœ„í•´ íŒŒì¼ë¡œ ì €ì¥
            # ----------------------------------------------------
            output_file = "chunked_result.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(chunks, f, indent=2, ensure_ascii=False)
            
            print(f"\nâœ… ì €ì¥ ì™„ë£Œ! '{output_file}' íŒŒì¼ì„ ì—´ì–´ì„œ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”.")
            
            # (ì„ íƒ) í™”ë©´ì—ëŠ” 3ê°œë§Œ ë§›ë³´ê¸°ë¡œ ì¶œë ¥
            print("\n--- [ì²­í¬ ì˜ˆì‹œ (ìƒìœ„ 3ê°œ)] ---")
            for c in chunks[:3]:
                print(json.dumps(c, indent=2, ensure_ascii=False))
        else:
            print("âŒ íŒŒì‹± ë°ì´í„° ì—†ìŒ")
    else:
        print("âŒ í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")