
import json
import os
import sys
from pathlib import Path

# ai-worker ë° backend-core ê²½ë¡œ ì„¤ì •
# ì´ë¥¼ í†µí•´ backend-coreì˜ models_db.pyë¥¼ ì§ì ‘ ì„í¬íŠ¸í•˜ì—¬ ìŠ¤í‚¤ë§ˆ ì¤‘ë³µ ì •ì˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
current_dir = Path(__file__).parent
backend_core_path = current_dir.parent.parent / "backend-core"
ai_worker_path = current_dir.parent

sys.path.append(str(backend_core_path))
sys.path.append(str(ai_worker_path))

from abc import ABC
from sqlmodel import Session, create_engine, select
# backend-core/db_models.py ì—ì„œ ì„í¬íŠ¸
from db_models import Question, AnswerBank, QuestionCategory, QuestionDifficulty, Company
# vector_utilsì—ì„œ ì¤‘ì•™ ê´€ë¦¬í˜• EmbeddingGenerator ì‚¬ìš©
from utils.vector_utils import get_embedding_generator
from datetime import datetime

# ì‹¤í–‰ ì˜ˆì‹œ
# docker exec -it interview_worker python scripts/import_all_with_embeddings.py

# ==========================================
# ë°ì´í„° êµ¬ì¡° (Data Structure)
# ==========================================
# 
# ì…ë ¥ JSON í˜•ì‹:
# [
#     {
#         "question": "ì§ˆë¬¸ ë‚´ìš©",
#         "answer": "ë‹µë³€ ë‚´ìš©",
#         "subcategory": "ì†Œë¶„ë¥˜ (ì˜ˆ: ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹)",  # Optional
#         "industry": "ì‚°ì—… (ì˜ˆ: IT/ì†Œí”„íŠ¸ì›¨ì–´, AI/ë°ì´í„°)",  # Optional
#         "company": "íšŒì‚¬ëª… (ì˜ˆ: ì‚¼ì„±ì „ì, ì¹´ì¹´ì˜¤)",  # Optional
#         "position": "ì§ë¬´ (ì˜ˆ: Backend ê°œë°œì)"  # Optional
#     }
# ]
#
# DB ì €ì¥ ë§¤í•‘:
# - subcategory â†’ question_type (ì§ˆë¬¸ ì„¸ë¶€ ë¶„ë¥˜)
# - industry â†’ Question.industry (ì‚°ì—… ë¶„ì•¼, NULL í—ˆìš©)
# - company â†’ Question.company (íšŒì‚¬ëª…, NULL í—ˆìš©)
# - position â†’ Question.position (ì§ë¬´, NULL í—ˆìš©)
# - ì†Œë¶„ë¥˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ QuestionCategory ìë™ ë¶„ë¥˜
# - NULL ê°’ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ê¸°ë³¸ê°’ ì—†ìŒ)
#
# ==========================================

# ==========================================
# Configuration
# ==========================================

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql+psycopg://admin:1234@interview_db:5432/interview_db")

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ (backend-coreì˜ data ë””ë ‰í† ë¦¬ ì°¸ì¡°)
# Docker ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ ê²½ë¡œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ volume mount í™•ì¸ í•„ìš”
# ì—¬ê¸°ì„œëŠ” ìƒëŒ€ ê²½ë¡œë¡œ ì ‘ê·¼ ì‹œë„. ì‹¤íŒ¨ ì‹œ ì ˆëŒ€ ê²½ë¡œ í™•ì¸ í•„ìš”.

POSSIBLE_DATA_DIRS = [
    "../backend-core/data", 
    "/backend-core/data",
    "c:/big20/git/Big20_aI_interview_project/backend-core/data",
    "/app/data", # Maybe mounted here
    "/data"
]

DATA_FILE_NAME_OLD = "preprocessed_data.json"
DATA_FILE_NAME_CORP = "corp_data.json"

def find_file(filename):
    """
    ì£¼ì–´ì§„ íŒŒì¼ëª…ì„ ê°€ëŠ¥í•œ ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ìŒ
    
    Args:
        filename (str): ì°¾ì„ íŒŒì¼ëª…
    
    Returns:
        str: íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” None
    
    Raises:
        ValueError: íŒŒì¼ì´ ë°œê²¬ë˜ì§€ ì•Šì„ ê²½ìš°
    
    ìƒì„±ì: ejm
    ìƒì„±ì¼ì: 2026-02-04
    """
    for directory in POSSIBLE_DATA_DIRS:
        filepath = os.path.join(directory, filename)
        if os.path.exists(filepath):
            return filepath
    return None

DATA_FILE_OLD = find_file(DATA_FILE_NAME_OLD) or "preprocessed_data.json"
DATA_FILE_CORP = find_file(DATA_FILE_NAME_CORP) or "corp_data.json"


# ==========================================
# Script
# ==========================================

def get_engine():
    """
    ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ì„ ìƒì„±í•˜ê³  í…ŒìŠ¤íŠ¸
    
    Returns:
        engine: ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„
    
    Raises:
        Exception: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨
    
    ìƒì„±ì: ejm
    ìƒì„±ì¼ì: 2026-02-04
    """
    try:
        engine = create_engine(DATABASE_URL)
        with Session(engine) as session:
            pass
        return engine
    except Exception as e:
        print(f"âŒ Database connection failed: {e}")
        return None

def import_questions(session, file_path, source_name, generator):
    """
    JSON íŒŒì¼ì—ì„œ ì§ˆë¬¸ì„ ì½ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    
    Args:
        session (_type_): ë°ì´í„°ë² ì´ìŠ¤ì„¸ì…˜
        file_path (_type_): íŒŒì¼ìœ„ì¹˜
        source_name (_type_): ë°ì´í„°ì†ŒìŠ¤ëª…
        generator (_type_): ì„ë² ë”© ìƒì„±ê¸°

    Raises:
        Exception: íŒŒì¼ì´ ë°œê²¬ë˜ì§€ ì•Šì„ ê²½ìš°
        
    Returns:
        None
    
    ìƒì„±ì: ejm
    ìƒì„±ì¼ì: 2026-02-04
    """
    if not os.path.exists(file_path):
        print(f"âš ï¸ Warning: File not found at {file_path}. Skipping {source_name}.")
        return

    print(f"ğŸ“‚ Reading {source_name} from: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ Error reading {source_name}: {e}")
        return

    print(f"ğŸš€ Importing {len(data)} items from {source_name}...")
    
    count = 0
    skipped = 0
    duplicates = 0

    def classify_question(text):
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ë¶„ë¥˜ (ë°ì´í„°ì— ì¹´í…Œê³ ë¦¬ê°€ ì—†ì„ ê²½ìš° ì‚¬ìš©)
        
        Args:
            text (str): ë¶„ë¥˜í•  í…ìŠ¤íŠ¸
        
        Returns:
            tuple: (ì¹´í…Œê³ ë¦¬, ë¶„ë¥˜ëª…)
        
        Raises:
            ValueError: í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì„ ê²½ìš°

        ìƒì„±ì: ejm
        ìƒì„±ì¼ì: 2026-02-04
        """
        text = text.lower()
        
        # 1. ì¸ì„±/ë¬¸í™” ì í•©ì„± (CULTURAL_FIT)
        if any(w in text for w in ["ì§€ì›ë™ê¸°", "ì…ì‚¬", "í¬ë¶€", "ì¥ì ", "ë‹¨ì ", "ê°ˆë“±", "í˜‘ì—…", "ì†Œí†µ", "íŒ€ì›Œí¬", "ì‹¤íŒ¨", "ì„±ê³µ", "ì¡´ê²½", "ë¬¸í™”", "why"]):
            return QuestionCategory.CULTURAL_FIT, "ì¸ì„±ë©´ì ‘"
            
        # 2. ì§ë¬´/ê²½í—˜ (BEHAVIORAL)
        if any(w in text for w in ["í”„ë¡œì íŠ¸", "ê²½í—˜", "ì—­ëŸ‰", "ê¸°ì—¬", "í•´ê²°", "ì§ë¬´", "ì»¤ë¦¬ì–´", "ì–´ë–»ê²Œ"]):
            return QuestionCategory.BEHAVIORAL, "ì§ë¬´ê²½í—˜"
            
        # 3. ê¸°ìˆ  (TECHNICAL) - Default
        return QuestionCategory.TECHNICAL, "ì§ë¬´ì§€ì‹"
    
    def auto_classify_by_subcategory(subcategory, text):
        """ì†Œë¶„ë¥˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
        
        Args:
            subcategory (str): ì†Œë¶„ë¥˜
            text (str): ë¶„ë¥˜í•  í…ìŠ¤íŠ¸
        
        Returns:
            QuestionCategory: ì¹´í…Œê³ ë¦¬
        
        Raises:
            ValueError: í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì„ ê²½ìš°
        
        ìƒì„±ì: ejm
        ìƒì„±ì¼ì: 2026-02-04
        """
        subcategory_lower = subcategory.lower()
        
        # ì¸ì„±/ë¬¸í™” ê´€ë ¨ ì†Œë¶„ë¥˜
        cultural_keywords = ["ì¸ì„±", "ë¬¸í™”", "ê°€ì¹˜ê´€", "íƒœë„", "ì„±ê²©", "í˜‘ì—…", "ì†Œí†µ"]
        if any(keyword in subcategory_lower for keyword in cultural_keywords):
            return QuestionCategory.CULTURAL_FIT
        
        # ê²½í—˜/í–‰ë™ ê´€ë ¨ ì†Œë¶„ë¥˜
        behavioral_keywords = ["ê²½í—˜", "í”„ë¡œì íŠ¸", "ì‹¤ë¬´", "ì‚¬ë¡€", "ìƒí™©", "ë¬¸ì œí•´ê²°"]
        if any(keyword in subcategory_lower for keyword in behavioral_keywords):
            return QuestionCategory.BEHAVIORAL
        
        # ê¸°ìˆ  ê´€ë ¨ ì†Œë¶„ë¥˜ (ê¸°ë³¸ê°’)
        # ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹, í”„ë¡œê·¸ë˜ë°, ì•Œê³ ë¦¬ì¦˜ ë“±
        return QuestionCategory.TECHNICAL

    for item in data:
        q_text = item.get("question") or item.get("ì§ˆë¬¸")
        # answer_cleaned ìš°ì„ , ì—†ìœ¼ë©´ answer/ë‹µë³€ ì‚¬ìš©
        a_text = item.get("answer_cleaned") or item.get("answer") or item.get("ë‹µë³€")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (NULL ê°’ ê·¸ëŒ€ë¡œ ìœ ì§€)
        subcategory = item.get("subcategory") or item.get("ì†Œë¶„ë¥˜")  # ê¸°ë³¸ê°’ ì—†ìŒ
        industry_value = item.get("industry") or item.get("ì‚°ì—…")  # ê¸°ë³¸ê°’ ì—†ìŒ
        company_value = item.get("company") or item.get("íšŒì‚¬")  # ê¸°ë³¸ê°’ ì—†ìŒ
        position_value = item.get("position") or item.get("ì§ë¬´")  # ê¸°ë³¸ê°’ ì—†ìŒ

        if not q_text or not a_text:
            skipped += 1
            continue

        # Check for duplicates
        statement = select(Question).where(Question.content == q_text)
        existing_q = session.exec(statement).first()
        
        if existing_q:
            duplicates += 1
            # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ
            continue

        # 1. Category Parsing
        # ì†Œë¶„ë¥˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
        category_str = item.get("QuestionCategory", "").lower()
        try:
            category = QuestionCategory(category_str)
        except ValueError:
            # ì†Œë¶„ë¥˜ ê¸°ë°˜ ìë™ ë¶„ë¥˜ (ì†Œë¶„ë¥˜ê°€ ìˆì„ ë•Œë§Œ)
            if subcategory:
                category = auto_classify_by_subcategory(subcategory, q_text)
            else:
                category, _ = classify_question(q_text)

        # 2. Difficulty Parsing
        difficulty_str = item.get("QuestionDifficulty", "").lower()
        try:
            difficulty = QuestionDifficulty(difficulty_str)
        except ValueError:
            difficulty = QuestionDifficulty.MEDIUM

        # 3. Question Type Parsing
        # ì†Œë¶„ë¥˜ë¥¼ question_typeìœ¼ë¡œ ì‚¬ìš© (ì—†ìœ¼ë©´ ìë™ ë¶„ë¥˜)
        q_type = item.get("QUESTION_TYPE") or subcategory
        if not q_type:
            _, q_type = classify_question(q_text)

        # Embedding ìƒì„± (Query ëª¨ë“œ ì‚¬ìš© ê¶Œì¥? Question ìì²´ëŠ” DBì— ì €ì¥ë˜ì–´ ê²€ìƒ‰ë¨(Passage ì„±ê²©ë„ ìˆìŒ)
        # í•˜ì§€ë§Œ ì§ˆë¬¸-ì§ˆë¬¸ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì—ëŠ” ë‘˜ ë‹¤ Query ë˜ëŠ” ë‘˜ ë‹¤ Passageë¡œ ë§ì¶°ì•¼ í•¨.
        # ë²¡í„° DB ê²€ìƒ‰ ì‹œ ìœ ì € ì¿¼ë¦¬ëŠ” "query:", DB ë¬¸ì„œëŠ” "passage:"ë¥¼ ë¶™ì—¬ ì €ì¥í•˜ëŠ” ë¹„ëŒ€ì¹­ ë°©ì‹ì´ ì¼ë°˜ì .
        # ì—¬ê¸°ì„œëŠ” Questionì„ 'ê²€ìƒ‰ ëŒ€ìƒ'ìœ¼ë¡œ ì €ì¥í•˜ë¯€ë¡œ "passage:" ì ‘ë‘ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥.
        # ë‚˜ì¤‘ì— ìœ ì €ê°€ ì§ˆë¬¸ì„ ê²€ìƒ‰í•  ë•Œ "query:"ë¥¼ ë¶™ì—¬ì„œ ê²€ìƒ‰.
        q_embedding = generator.encode_passage(q_text)

        # Create Question
        # DB í•„ë“œì— ì§ì ‘ ì €ì¥ (NULL ê°’ ê·¸ëŒ€ë¡œ ìœ ì§€)
        question = Question(
            content=q_text,
            category=category,
            difficulty=difficulty,
            rubric_json={"keywords": []},  # ê¸°ë³¸ rubricë§Œ ì €ì¥
            question_type=q_type, 
            usage_count=0,
            is_active=True,
            embedding=q_embedding,  # ì„ë² ë”© ì €ì¥
            company=company_value,  # Noneì´ë©´ NULLë¡œ ì €ì¥
            industry=industry_value,  # Noneì´ë©´ NULLë¡œ ì €ì¥
            position=position_value  # Noneì´ë©´ NULLë¡œ ì €ì¥
        )
        session.add(question)
        session.flush() # To get ID

        # Answer Embedding (Passage)
        a_embedding = generator.encode_passage(a_text)

        # Create AnswerBank
        answer = AnswerBank(
            question_id=question.id,
            answer_text=a_text,
            score=100.0,
            reference_count=0,
            is_active=True,
            embedding=a_embedding,  # ì„ë² ë”© ì €ì¥
            company=company_value,
            industry=industry_value,
            position=position_value
        )
        session.add(answer)
        count += 1
        
        if count % 100 == 0:
            print(f"   - {count} items processed...")

    try:
        session.commit()
        print(f"âœ… Finished {source_name}: Imported {count}, Duplicates {duplicates}, Skipped {skipped}")
    except Exception as e:
        session.rollback()
        print(f"âŒ Failed to commit {source_name}: {e}")

def import_companies(session, file_path, generator):
    """
    íšŒì‚¬ì •ë³´ë¥¼ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜

    Args:
        session (Session): DB ì„¸ì…˜
        file_path (str): JSON íŒŒì¼ ê²½ë¡œ
        generator (EmbeddingGenerator): ì„ë² ë”© ìƒì„±ê¸°
    
    Raises:
        Exception: íŒŒì¼ì´ ë°œê²¬ë˜ì§€ ì•Šì„ ê²½ìš°

    Returns:
        None
    """
    if not os.path.exists(file_path):
        print(f"âš ï¸ Warning: File not found at {file_path}. Skipping Companies.")
        return

    print(f"ğŸ“‚ Reading Companies from: {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ Error reading Companies: {e}")
        return

    print(f"ğŸš€ Importing {len(data)} companies...")
    
    count = 0
    duplicates = 0

    for item in data:
        name = item.get("name")
        code = item.get("code")
        ideal = item.get("ideal") or ""
        desc = item.get("description") or ""

        if not name or not code:
            continue

        code_str = str(code).strip()

        # Check for duplicates
        statement = select(Company).where(Company.id == code_str)
        existing_c = session.exec(statement).first()

        if existing_c:
            duplicates += 1
            # Update existing if needed, or skip
            # ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µ
            continue
            
        # Embedding text
        text_for_embedding = f"{ideal} {desc}".strip()
        # Company Infoë„ ê²€ìƒ‰ ëŒ€ìƒ -> Passage
        embedding = generator.encode_passage(text_for_embedding)

        company = Company(
            id=code_str,
            company_name=name,
            ideal=ideal,
            description=desc,
            embedding=embedding # ì„ë² ë”© ì €ì¥
        )
        session.add(company)
        count += 1
        
        if count % 50 == 0:
            print(f"   - {count} companies processed...")

    try:
        session.commit()
        print(f"âœ… Finished Companies: Imported {count}, Duplicates {duplicates}")
    except Exception as e:
        session.rollback()
        print(f"âŒ Failed to commit Companies: {e}")


def main():
    print("ğŸš€ Starting Data Import with Embeddings...")
    
    # Init Embedding Generator
    print("ğŸ“¦ Loading embedding model (KURE-v1)...")
    generator = get_embedding_generator()
    print("âœ… Model loaded.")

    engine = get_engine()
    if not engine:
        return

    with Session(engine) as session:
        # Import Questions
        if DATA_FILE_OLD:
             import_questions(session, DATA_FILE_OLD, "General Questions", generator)
        else:
            print("âš ï¸ Questions data file not found.")

        print("-" * 40)

        # Import Companies
        if DATA_FILE_CORP:
            import_companies(session, DATA_FILE_CORP, generator)
        else:
             print("âš ï¸ Corp data file not found.")

    print("=" * 40)
    print("ğŸ‰ All imports completed.")

if __name__ == "__main__":
    main()