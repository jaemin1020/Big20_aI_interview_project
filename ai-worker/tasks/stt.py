<<<<<<< HEAD
from celery import shared_task
from faster_whisper import WhisperModel
=======

>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d
import os
import logging
import base64
import tempfile
<<<<<<< HEAD
import torch
=======
from celery import shared_task
from faster_whisper import WhisperModel
>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d

logger = logging.getLogger("STT-Task")

# Ï†ÑÏó≠ Î™®Îç∏ Î≥ÄÏàò
stt_model = None
<<<<<<< HEAD
MODEL_SIZE = os.getenv("WHISPER_MODEL_SIZE", "large-v3-turbo") 

def load_stt_model():
    """
    Faster-Whisper Î™®Îç∏ Î°úÎìú
    """
    global stt_pipeline
    
    # [ÏµúÏ†ÅÌôî] GPU ÏõåÏª§(ÏßàÎ¨∏ ÏÉùÏÑ± Ï†ÑÏö©)Îäî STT Î™®Îç∏ÏùÑ Î°úÎìúÌï† ÌïÑÏöîÍ∞Ä ÏóÜÏùå
    gpu_layers = int(os.getenv("N_GPU_LAYERS", "-1"))
    if gpu_layers == -1:
        logger.info("‚è© [SKIP] GPU Worker detected. Skipping Whisper Pipeline loading.")
        return

    try:
        # cuDNN ÏóêÎü¨ Î∞©ÏßÄÎ•º ÏúÑÌï¥ CPU ÏÇ¨Ïö© Í∞ïÏ†ú
        device = "cpu" 
        torch_dtype = torch.float32

        logger.info(f"üöÄ [LOADING] Whisper Pipeline ({MODEL_ID}) on {device}...")
        
        stt_pipeline = pipeline(
            "automatic-speech-recognition",
            model=MODEL_ID,
            torch_dtype=torch_dtype,
            device=device,
            chunk_length_s=30,
        )
        logger.info("‚úÖ Whisper Pipeline loaded successfully.")
    except Exception as e:
        logger.error(f"‚ùå Failed to load Whisper Pipeline: {e}")
        stt_pipeline = None

# Î™®Îìà Î°úÎìú Ïãú Ï†ÑÏó≠ Ìò∏Ï∂ú Ï†úÍ±∞ (Ïã§Ï†ú ÌÉúÏä§ÌÅ¨ ÏàòÌñâ Ïãú Î°úÎìúÌïòÎèÑÎ°ù ÏàòÏ†ï)
# load_stt_pipeline()
=======
# Î™®Îç∏ ÏÇ¨Ïù¥Ï¶à: tiny, base, small, medium, large-v3-turbo. 
# CPU ÌôòÍ≤ΩÏùÑ ÏúÑÌï¥ Í∏∞Î≥∏Í∞íÏùÄ 'medium' ÎòêÎäî 'small' Í∂åÏû•.
MODEL_SIZE = os.getenv("WHISPER_MODEL_SIZE", "large-v3-turbo")

def load_stt_model():
    """
    Faster-Whisper Î™®Îç∏ÏùÑ Î°úÎìúÌï©ÎãàÎã§. (Ïã±Í∏ÄÌÜ§ Ìå®ÌÑ¥)
    Compute Type: int8 (CPU ÏÑ±Îä• ÏµúÏ†ÅÌôî)
    """
    global stt_model
    
    if stt_model is not None:
        return

    try:
        device = "cpu"
        # CPUÏóêÏÑú int8 ÏñëÏûêÌôî ÏÇ¨Ïö© Ïãú ÏÜçÎèÑ ÎåÄÌè≠ Ìñ•ÏÉÅ
        compute_type = "int8" 
        
        logger.info(f"üöÄ [LOADING] Faster-Whisper ({MODEL_SIZE}) on {device} (compute_type={compute_type})...")
        
        # Î™®Îç∏ Î°úÎìú (ÏµúÏ¥à Ïã§Ìñâ Ïãú Îã§Ïö¥Î°úÎìúÎê®)
        stt_model = WhisperModel(MODEL_SIZE, device=device, compute_type=compute_type)
        
        logger.info("‚úÖ Faster-Whisper loaded successfully.")
    except Exception as e:
        logger.error(f"‚ùå Failed to load Faster-Whisper: {e}", exc_info=True)
        stt_model = None
>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d

@shared_task(name="tasks.stt.recognize")
def recognize_audio_task(audio_b64: str):
    """
<<<<<<< HEAD
    Faster-WhisperÎ•º ÏÇ¨Ïö©Ìïú ÌÜµÌï© STT Task (ÌååÏùº/Ï≤≠ÌÅ¨)
    
    Args:
        audio_b64: Base64 encoded audio string
        
    Returns:
        dict: {"status": "success", "text": "..."}
    """
    global stt_model
    
    if stt_model is None:
        load_stt_model()
        if stt_model is None:
             return {"status": "error", "message": "Model loading failed"}
=======
    Faster-WhisperÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ïò§ÎîîÏò§(Base64)Î•º ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.
    Args:
        audio_b64 (str): Base64 Ïù∏ÏΩîÎî©Îêú Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞ (Ìó§Îçî Ìè¨Ìï®Îê† Ïàò ÏûàÏùå)
    """
    global stt_model
    
    # Î™®Îç∏ Î°úÎìú (ÏßÄÏó∞ Î°úÎî©)
    if stt_model is None:
        load_stt_model()
        if stt_model is None:
             return {"status": "error", "message": "STT Model loading failed"}
>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d

    temp_path = None
    try:
        if not audio_b64:
            return {"status": "error", "message": "Empty audio data"}
            
<<<<<<< HEAD
        # Base64 decoding & Save to Temp File
        audio_bytes = base64.b64decode(audio_b64)
        
        # ÌååÏùº Ï†ÄÏû• (faster-whisperÎäî ÌååÏùº Í≤ΩÎ°ú ÎòêÎäî binary stream ÏßÄÏõê)
=======
        # Base64 Ìó§Îçî Ï≤òÎ¶¨ (data:audio/webm;base64,...)
        if "," in audio_b64:
            audio_b64 = audio_b64.split(",")[1]
            
        try:
            audio_bytes = base64.b64decode(audio_b64)
        except Exception as e:
            return {"status": "error", "message": f"Base64 decode failed: {e}"}
        
        # ÏûÑÏãú ÌååÏùº Ï†ÄÏû• (faster-whisperÎäî ÌååÏùº Í≤ΩÎ°ú ÏûÖÎ†• Í∂åÏû•)
        # suffixÎäî webmÏúºÎ°ú Í∞ÄÏ†ïÌïòÎÇò, ffmpegÍ∞Ä ÏïåÏïÑÏÑú Ï≤òÎ¶¨Ìï®
>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d
        with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as tmp:
            tmp.write(audio_bytes)
            temp_path = tmp.name
        
        # Inference
<<<<<<< HEAD
        segments, info = stt_model.transcribe(
            temp_path, 
            beam_size=5, 
            language="ko", # ÌïúÍµ≠Ïñ¥ Í∞ïÏ†ú
            vad_filter=True # ÏùåÏÑ± ÌôúÎèô Í∞êÏßÄ ÏÇ¨Ïö©
=======
        # segmentsÎäî generatorÏù¥ÎØÄÎ°ú ÏàúÌöåÌï¥Ïïº Ïã§Ï†ú Ï∂îÎ°†Ïù¥ ÏàòÌñâÎê®
        segments, info = stt_model.transcribe(
            temp_path, 
            beam_size=5, 
            language="ko", 
            vad_filter=True, # ÏùåÏÑ± Íµ¨Í∞Ñ Í∞êÏßÄ ÌôúÏÑ±Ìôî (Î¨¥Ïùå Ï†úÍ±∞)
            vad_parameters=dict(min_silence_duration_ms=500)
>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d
        )
        
        full_text = ""
        for segment in segments:
            full_text += segment.text
        
        full_text = full_text.strip()
<<<<<<< HEAD
        logger.info(f"STT Success: {len(full_text)} chars")
=======
        logger.info(f"STT Success: {len(full_text)} chars. Preview: {full_text[:50]}")
>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d
        
        return {"status": "success", "text": full_text}
        
    except Exception as e:
        logger.error(f"STT Error: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}
        
    finally:
<<<<<<< HEAD
        # ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
=======
        # ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨
>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except OSError:
                pass
