from celery import shared_task
from faster_whisper import WhisperModel
import os
import logging
import base64
import tempfile
import torch

logger = logging.getLogger("STT-Task")

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜
stt_model = None
MODEL_SIZE = os.getenv("WHISPER_MODEL_SIZE", "large-v3-turbo") 

def load_stt_model():
    """
    Faster-Whisper ëª¨ë¸ ë¡œë“œ
    """
    global stt_pipeline
    
    # [ìµœì í™”] GPU ì›Œì»¤(ì§ˆë¬¸ ìƒì„± ì „ìš©)ëŠ” STT ëª¨ë¸ì„ ë¡œë“œí•  í•„ìš”ê°€ ì—†ìŒ
    gpu_layers = int(os.getenv("N_GPU_LAYERS", "-1"))
    if gpu_layers == -1:
        logger.info("â© [SKIP] GPU Worker detected. Skipping Whisper Pipeline loading.")
        return

    try:
        # cuDNN ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ CPU ì‚¬ìš© ê°•ì œ
        device = "cpu" 
        torch_dtype = torch.float32

        logger.info(f"ğŸš€ [LOADING] Whisper Pipeline ({MODEL_ID}) on {device}...")
        
        stt_pipeline = pipeline(
            "automatic-speech-recognition",
            model=MODEL_ID,
            torch_dtype=torch_dtype,
            device=device,
            chunk_length_s=30,
        )
        logger.info("âœ… Whisper Pipeline loaded successfully.")
    except Exception as e:
        logger.error(f"âŒ Failed to load Whisper Pipeline: {e}")
        stt_pipeline = None

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì „ì—­ í˜¸ì¶œ ì œê±° (ì‹¤ì œ íƒœìŠ¤í¬ ìˆ˜í–‰ ì‹œ ë¡œë“œí•˜ë„ë¡ ìˆ˜ì •)
# load_stt_pipeline()

@shared_task(name="tasks.stt.recognize")
def recognize_audio_task(audio_b64: str):
    """
    Faster-Whisperë¥¼ ì‚¬ìš©í•œ í†µí•© STT Task (íŒŒì¼/ì²­í¬)
    
    Args:
        audio_b64: Base64 encoded audio string
        
    Returns:
        dict: {"status": "success", "text": "..."}
    """
    global stt_model
    
    if stt_model is None:
        load_stt_model()
        if stt_model is None:
             return {"status": "error", "message": "Model loading failed"}

    temp_path = None
    try:
        if not audio_b64:
            return {"status": "error", "message": "Empty audio data"}
            
        # Base64 decoding & Save to Temp File
        audio_bytes = base64.b64decode(audio_b64)
        
        # íŒŒì¼ ì €ì¥ (faster-whisperëŠ” íŒŒì¼ ê²½ë¡œ ë˜ëŠ” binary stream ì§€ì›)
        with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as tmp:
            tmp.write(audio_bytes)
            temp_path = tmp.name
        
        # Inference
        segments, info = stt_model.transcribe(
            temp_path, 
            beam_size=5, 
            language="ko", # í•œêµ­ì–´ ê°•ì œ
            vad_filter=True # ìŒì„± í™œë™ ê°ì§€ ì‚¬ìš©
        )
        
        full_text = ""
        for segment in segments:
            full_text += segment.text
        
        full_text = full_text.strip()
        logger.info(f"STT Success: {len(full_text)} chars")
        
        return {"status": "success", "text": full_text}
        
    except Exception as e:
        logger.error(f"STT Error: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}
        
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if temp_path and os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except OSError:
                pass
