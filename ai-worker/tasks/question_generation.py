import sys
import os
import time
import gc 
import logging
import torch
from celery import shared_task
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager
from langchain_core.prompts import PromptTemplate

logger = logging.getLogger("AI-Worker-QuestionGen")

# -----------------------------------------------------------
# [1. ëª¨ë¸ ë° ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

model_path = local_path if os.path.exists(local_path) else docker_path

# ğŸš¨ DB ì¡°íšŒë¥¼ ìœ„í•´ ì¶”ê°€
try:
    from db import engine
    from sqlalchemy import text as sql_text
except ImportError:
    engine = None

# -----------------------------------------------------------
# [2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]
# -----------------------------------------------------------
PROMPT_TEMPLATE = """[|system|]
ë„ˆëŠ” 15ë…„ ì°¨ ë² í…Œë‘ {position} ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤. 
ì§€ê¸ˆì€ **ë©´ì ‘ì´ í•œì°½ ì§„í–‰ ì¤‘ì¸ ìƒí™©**ì´ë‹¤. (ìê¸°ì†Œê°œëŠ” ì´ë¯¸ ëë‚¬ë‹¤.)
ì œê³µëœ [ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±°] ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ë‹¨ê³„({stage})ì— ë§ëŠ” **í•µì‹¬ì ì¸ ì§ˆë¬¸ 1ê°œ**ë§Œ ë˜ì ¸ë¼.

[ì‘ì„± ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­] 
1. **"ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤" ì ˆëŒ€ ê¸ˆì§€.**
2. **"(ì ì‹œ ì¹¨ë¬µ)", "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤"** ê°™ì€ ëŒ€ë³¸ìš© ì§€ë¬¸ì„ ì“°ì§€ ë§ˆë¼.
3. **[í”„ë¡œì íŠ¸], [íšŒì‚¬ ëª…]** ê°™ì€ ìë¦¬í‘œì‹œì(Placeholder)ë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ë§ê³ , ê·¼ê±° ë°ì´í„°ì— ìˆëŠ” ì‹¤ì œ ëª…ì¹­ì„ ì¨ë¼.
4. ì§ˆë¬¸ ì•ë’¤ì— ì‚¬ì¡±ì„ ë¶™ì´ì§€ ë§ê³  **ì§ˆë¬¸ë§Œ ë”± í•œ ë¬¸ì¥(ìµœëŒ€ ë‘ ë¬¸ì¥)**ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.

[ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ]
1. ì‹œì‘ì€ ë°˜ë“œì‹œ **"{name}ë‹˜,"** ìœ¼ë¡œ ë¶€ë¥´ë©° ì‹œì‘í•  ê²ƒ.
2. ì§ˆë¬¸ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ í•µì‹¬ë§Œ ëª…í™•íˆ ë¬¼ì–´ë³¼ ê²ƒ. ê¼¬ì•„ë‚´ì§€ ë§ê³  ì •ê³µë²•ìœ¼ë¡œ ë¬¼ì–´ë³¼ ê²ƒ.
3. ë§íˆ¬ëŠ” ì •ì¤‘í•˜ê²Œ(..í•˜ì…¨ë‚˜ìš”?, ..ë¶€íƒë“œë¦½ë‹ˆë‹¤.) ìœ ì§€í•  ê²ƒ.
[|endofturn|]
[|user|]
# í‰ê°€ ë‹¨ê³„: {stage}
# í‰ê°€ ì˜ë„: {guide}
# ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±° (RAG):
{context}

# ìš”ì²­:
ìœ„ì˜ ê·¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ {name} ì§€ì›ìì—ê²Œ **êµ¬ì²´ì ì´ê³  ë‹¨ë„ì§ì…ì ì¸** ì§ˆë¬¸ì„ ë˜ì ¸ì¤˜.
[|endofturn|]
[|assistant|]
"""

# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# -----------------------------------------------------------
# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# -----------------------------------------------------------
def generate_human_like_question(exaone, name, position, stage, guide, context_list):
    """
    ExaoneLLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ìƒì„±
    """
    if not context_list:
        return f"âŒ (ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•´ ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"

    texts = [item['text'] for item in context_list] if isinstance(context_list[0], dict) else context_list
    context_text = "\n".join([f"- {txt}" for txt in texts])
    
    try:
        # ExaoneLLMì˜ generate_questions ë©”ì„œë“œ í™œìš© (ë‹¨ì¼ ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•´ count=1)
        # ë³´ë‹¤ ì •êµí•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•´ ì§ì ‘ generate í˜¸ì¶œ ê°€ëŠ¥í•˜ë‚˜, ì—¬ê¸°ì„œëŠ” ì¼ê´€ì„±ì„ ìœ„í•´ ë©í•‘
        system_msg = f"ë‹¹ì‹ ì€ 15ë…„ ì°¨ ë² í…Œë‘ {position} ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤. ì§€ê¸ˆì€ ë©´ì ‘ì´ í•œì°½ ì§„í–‰ ì¤‘ì¸ ìƒí™©ì´ë‹¤."
        user_msg = f"""ì§€ì›ì {name}ë‹˜ì—ê²Œ {stage} ë‹¨ê³„ì˜ ë©´ì ‘ ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”.
í‰ê°€ ì˜ë„: {guide}
ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±° (RAG):
{context_text}

[ìš”êµ¬ì‚¬í•­]
1. ì‹œì‘ì€ ë°˜ë“œì‹œ "{name}ë‹˜," ìœ¼ë¡œ ë¶€ë¥¼ ê²ƒ.
2. ì´ë ¥ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ìš´ **ê°„ê²°í•œ ì§ˆë¬¸ 1ê°œ**ë§Œ ë˜ì§ˆ ê²ƒ.
3. ë°˜ë“œì‹œ **150ì ì´ë‚´(ë‘ ë¬¸ì¥ ì´ë‚´)**ë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ ë¬¼ì–´ë³¼ ê²ƒ. ì‚¬ì¡± ê¸ˆì§€.
"""
        prompt = exaone._create_prompt(system_msg, user_msg)
        output = exaone.llm(
            prompt,
            max_tokens=512,
            stop=["[|endofturn|]", "[|user|]"],
            temperature=0.4,
            echo=False
        )
        return output['choices'][0]['text'].strip()
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ë©´ì ‘ì„ ì´ì–´ê°€ê² ìŠµë‹ˆë‹¤. {name}ë‹˜, ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤."

# -----------------------------------------------------------
# [4. Celery Task] - ê¸°ì¡´ ì¼ê´„ ìƒì„± íƒœìŠ¤í¬ (í•„ìš” ì‹œ ìœ ì§€)
# -----------------------------------------------------------
@shared_task(name="tasks.question_generation.generate_questions")
def generate_questions_task(position, interview_id, count=5, resume_id=1):
    from utils.exaone_llm import get_exaone_llm
    exaone = get_exaone_llm()
    
    # ... (ìƒëµ ê°€ëŠ¥í•˜ë‚˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ ì‹œì—ëŠ” exaone.generate_questions ì‚¬ìš© ê¶Œì¥)
    return exaone.generate_questions(position, count=count)

# -----------------------------------------------------------
# [5. Celery Task] - ì‹¤ì‹œê°„ 1ê°œì”© ìƒì„±í•˜ëŠ” íƒœìŠ¤í¬ (ìˆ˜ì • ì™„ë£Œ)
# -----------------------------------------------------------
@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    logger.info(f"ğŸ”¥ [START] generate_next_question_task for Interview {interview_id}")
    from db import engine, Session, select, save_generated_question
    from models import Interview, Transcript, Speaker, Question
    from config.interview_scenario import get_stage_by_name, get_next_stage
    from utils.exaone_llm import get_exaone_llm
    
    with Session(engine) as session:
        interview = session.get(Interview, interview_id)
        if not interview: return
            
        # ğŸ” ë§ˆì§€ë§‰ ë‹¨ê³„ íƒì§€ ìµœì í™” (ìˆœì„œ ê¸°ë°˜ì´ ì•„ë‹Œ ID ê¸°ë°˜ ìµœì‹  ë°ì´í„° ì¡°íšŒ)
        stmt = select(Transcript).where(
            Transcript.interview_id == interview_id,
            Transcript.speaker == Speaker.AI
        ).order_by(Transcript.id.desc()) # IDê°€ ê°€ì¥ í° ê²ƒì´ ì ˆëŒ€ì ìœ¼ë¡œ ìµœì‹ 
        last_ai_transcript = session.exec(stmt).first()
        
        last_stage_name = None
        if last_ai_transcript and last_ai_transcript.question_id:
            last_q = session.get(Question, last_ai_transcript.question_id)
            if last_q:
                # 1ìˆœìœ„: DBì— ì €ì¥ëœ íƒ€ì… ì •ë³´ ì‚¬ìš©
                last_stage_name = last_q.question_type
                
                # 2ìˆœìœ„ (Fallback): ì €ì¥ëœ íƒ€ì…ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ë‚´ìš©ìœ¼ë¡œ ìœ ì¶” (ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ê°€)
                if not last_stage_name:
                    content = last_q.content
                    if "ìê¸°ì†Œê°œ" in content: last_stage_name = "intro"
                    elif "ì§€ì› ë™ê¸°" in content or "ì§€ì›í•˜ê²Œ ëœ" in content: last_stage_name = "motivation"
                    elif "ê¸°ìˆ " in content or "ìŠ¤í‚¬" in content or "ë„êµ¬" in content: last_stage_name = "skill"
                    elif "í”„ë¡œì íŠ¸" in content or "ê²½í—˜" in content: last_stage_name = "experience"
                    elif "ì–´ë ¤ì›€" in content or "í•´ê²°" in content: last_stage_name = "problem_solving"
        
        if not last_stage_name:
            last_stage_name = "intro"

        logger.info(f"Detected Last Stage: {last_stage_name}")
        
        next_stage_data = get_next_stage(last_stage_name)
        if not next_stage_data:
            logger.info("Scenario Completed.")
            return {"status": "completed"}
            
        stage_name = next_stage_data["stage"]
        stage_type = next_stage_data.get("type", "ai")
        
        if stage_type == "template" or stage_type == "final":
            from utils.interview_helpers import get_candidate_info
            from db import Resume
            resume = session.get(Resume, interview.resume_id)
            c_info = get_candidate_info(resume.structured_data if resume else {})
            tmpl = next_stage_data.get("template", "{candidate_name}ë‹˜, ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤.")
            content = tmpl.format(candidate_name=c_info.get("candidate_name", "ì§€ì›ì"), target_role=interview.position)
            
            # QuestionCategory Enumì— 'general'ì´ ì—†ìœ¼ë¯€ë¡œ 'behavioral' ì‚¬ìš©
            save_generated_question(interview_id, content, "behavioral", stage_name, "")
            return {"status": "success", "stage": stage_name}

        # AI ìƒì„± ë£¨í‹´
        try:
            exaone = get_exaone_llm()
            
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì¼ë°˜ AI ì§ˆë¬¸ vs ê¼¬ë¦¬ì§ˆë¬¸)
            contexts = []
            if stage_type == "followup":
                # ê¼¬ë¦¬ì§ˆë¬¸ì˜ ê²½ìš° RAG ëŒ€ì‹  'ì§ì „ ë‹µë³€'ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                user_stmt = select(Transcript).where(
                    Transcript.interview_id == interview_id,
                    Transcript.speaker == Speaker.USER
                ).order_by(Transcript.id.desc())
                last_user_ans = session.exec(user_stmt).first()
                if last_user_ans:
                    contexts = [{"text": f"ì´ì „ ë‹µë³€: {last_user_ans.text}", "meta": {"category": "followup"}}]
                    logger.info(f"ğŸ“Œ Follow-up context prepared from last answer.")
            
            # RAG ê²€ìƒ‰ (ê¼¬ë¦¬ì§ˆë¬¸ì´ ì•„ë‹ˆê±°ë‚˜, ê¼¬ë¦¬ì§ˆë¬¸ì¸ë° ì»¨í…ìŠ¤íŠ¸ë¥¼ ëª» ì°¾ì€ ê²½ìš°)
            if not contexts:
                from .rag_retrieval import retrieve_context
                query_tmpl = next_stage_data.get("query_template", "{target_role}")
                query = query_tmpl.format(target_role=interview.position)
                contexts = retrieve_context(query, resume_id=interview.resume_id, top_k=3)
            
            from utils.interview_helpers import get_candidate_info
            from db import Resume
            resume = session.get(Resume, interview.resume_id)
            c_info = get_candidate_info(resume.structured_data if resume else {})
            
            content = generate_human_like_question(
                exaone, c_info.get("candidate_name", "ì§€ì›ì"), interview.position, 
                stage_name, next_stage_data.get("guide", ""), contexts
            )
            
            # ì‹œë‚˜ë¦¬ì˜¤ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ DB Enumì— ë§ê²Œ ë§¤í•‘
            category_raw = next_stage_data.get("category", "technical")
            category_map = {
                "certification": "technical",
                "project": "technical",
                "narrative": "behavioral",
                "problem_solving": "situational"
            }
            db_category = category_map.get(category_raw, "technical")
            
            save_generated_question(interview_id, content, db_category, stage_name, next_stage_data.get("guide", ""))
            return {"status": "success", "stage": stage_name, "question": content}
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
        finally:
            gc.collect()
