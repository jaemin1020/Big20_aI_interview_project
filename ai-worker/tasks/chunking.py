import json
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter # [ë¬¸ë²•] ê¸´ ê¸€ì„ ë˜‘ë˜‘í•˜ê²Œ ìë¥´ëŠ” ë„êµ¬

def chunk_resume(parsed_data):
    """
    êµ¬ì¡°í™”ëœ ì´ë ¥ì„œ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ AI ê²€ìƒ‰ìš© ì¡°ê°(Chunk)ë“¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    chunks = []
    print("\n[STEP4] ë°ì´í„° ì²­í‚¹(Chunking) ì‹œì‘...")

    # ====================================================
    # 1. í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • (Text Splitter)
    # [í•´ì„] ìì†Œì„œ ë‹µë³€ì²˜ëŸ¼ ê¸´ ê¸€ì„ ì–´ë–»ê²Œ ìë¥¼ì§€ ì •í•˜ëŠ” 'ê°€ì´ë“œë¼ì¸'ì…ë‹ˆë‹¤.
    # ====================================================
    # [ë¬¸ë²•] RecursiveCharacterTextSplitter: 
    # chunk_size=600: í•œ ì¡°ê°ì„ ìµœëŒ€ 600ìë¡œ ë§ì¶¤
    # chunk_overlap=100: ì¡°ê°ë¼ë¦¬ 100ì ì •ë„ ê²¹ì¹˜ê²Œ í•´ì„œ ì•ë’¤ ë¬¸ë§¥ì´ ì•ˆ ëŠê¸°ê²Œ í•¨
    # separators: ìë¥¼ ë•Œ ê¸°ì¤€ (ë¬¸ë‹¨ -> ë¬¸ì¥ -> ë‹¨ì–´ ìˆœì„œë¡œ ì‹œë„)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    # ----------------------------------------------------
    # 2. í—¤ë” ì •ë³´ (Profile)
    # ----------------------------------------------------
    header = parsed_data.get("header", {}) # [ë¬¸ë²•] .get("key", {}): í‚¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ëŒë ¤ì¤˜ì„œ ì—ëŸ¬ë¥¼ ë°©ì§€í•¨
    if header:
        name = header.get("name", "")
        role = header.get("target_role", "")
        company = header.get("target_company", "")
        
        # [í•´ì„] ì´ë¦„ê³¼ ì§ë¬´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ ê²€ìƒ‰ì´ ì˜ ë˜ê²Œ í•©ë‹ˆë‹¤.
        chunks.append({
            "type": "header",
            "text": f"[í”„ë¡œí•„] ì´ë¦„: {name}, ì§€ì›ì§ë¬´: {role}, ì§€ì›íšŒì‚¬: {company}",
            "metadata": { "source": "resume", "category": "profile" } # [í•´ì„] ë‚˜ì¤‘ì— ì¶œì²˜ë¥¼ ì•Œ ìˆ˜ ìˆê²Œ ê¼¬ë¦¬í‘œë¥¼ ë‹µë‹ˆë‹¤.
        })

    # ----------------------------------------------------
    # 3. í•™ë ¥ (Education)
    # ----------------------------------------------------
    educations = parsed_data.get("education", [])
    for edu in educations: # [ë¬¸ë²•] í•™ë ¥ì´ ì—¬ëŸ¬ ê°œ(í•™ì‚¬, ì„ì‚¬ ë“±)ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°˜ë³µë¬¸ì„ ëŒë¦½ë‹ˆë‹¤.
        school = edu.get("school_name", "")
        major = edu.get("major", "")
        period = edu.get("period", "")
        gpa = edu.get("gpa", "")
        status = edu.get("status", "")
        
        # [í•´ì„] í©ì–´ì§„ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ì½ê¸° ì¢‹ì€ ë¬¸ì¥ìœ¼ë¡œ í•©ì¹©ë‹ˆë‹¤.
        text = f"[í•™ë ¥] {school} {major} ({status})"
        if period: text += f" - {period}"
        if gpa: text += f", í•™ì : {gpa}"
        
        chunks.append({
            "type": "education",
            "text": text,
            "metadata": { "source": "resume", "category": "education", "school": school }
        })

    # ----------------------------------------------------
    # 4. ëŒ€ì™¸í™œë™ (Activities)
    # ----------------------------------------------------
    activities = parsed_data.get("activities", [])
    for act in activities:
        org = act.get("organization", "")
        role = act.get("role", "")
        period = act.get("period", "")
        desc = act.get("description", "")
        
        text = f"[ëŒ€ì™¸í™œë™] ê¸°ê´€: {org}, ì—­í• : {role}"
        if period: text += f" ({period})"
        if desc: text += f"\nì„¤ëª…: {desc}"
        
        chunks.append({
            "type": "experience",
            "text": text,
            "metadata": { "source": "resume", "category": "experience", "org": org }
        })

    # ----------------------------------------------------
    # 5. ìˆ˜ìƒ (Awards)
    # ----------------------------------------------------
    awards = parsed_data.get("awards", [])
    for award in awards:
        title = award.get("title", "")
        org = award.get("organization", "")
        date = award.get("date", "")
        
        text = f"[ìˆ˜ìƒ] ìƒí›ˆ: {title}, ê¸°ê´€: {org}"
        if date: text += f" ({date})"
        
        chunks.append({
            "type": "awards",
            "text": text,
            "metadata": { "source": "resume", "category": "awards" }
        })

    # ----------------------------------------------------
    # 6. ìê²©ì¦ (Certifications) - ğŸ”¥ í•µì‹¬ ìˆ˜ì • ì§€ì  ğŸ”¥
    # ----------------------------------------------------
    certifications = parsed_data.get("certifications", [])
    for cert in certifications:
        title = cert.get("title", "")
        org = cert.get("organization", "")
        date = cert.get("date", "")
        
        # [í•´ì„] AIê°€ "ìê²©ì¦ì„ ë³´ìœ í•˜ê³  ìˆë‹¤"ëŠ” ê²ƒì„ í™•ì‹¤íˆ ì¸ì‹í•˜ë„ë¡ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
        text = f"[ìê²©ì¦] ìê²©ëª…: {title}, ë°œí–‰ê¸°ê´€: {org}"
        if date: text += f" (ì·¨ë“ì¼: {date})"
        
        chunks.append({
            "type": "certifications", 
            "text": text,
            "metadata": { "source": "resume", "category": "certification" }
        })

    # ----------------------------------------------------
    # 7. í”„ë¡œì íŠ¸ (Projects)
    # ----------------------------------------------------
    projects = parsed_data.get("projects", [])
    for proj in projects:
        title = proj.get("title", "")
        period = proj.get("period", "")
        desc = proj.get("description", "")
        
        text = f"[í”„ë¡œì íŠ¸] ëª…ì¹­: {title}"
        if period: text += f" ({period})"
        if desc: text += f"\nìƒì„¸: {desc}"
        
        # í”„ë¡œì íŠ¸ ì„¤ëª…ì´ ê¸¸ë©´ í…ìŠ¤íŠ¸ ë¶„í• ê¸°ë¡œ ìª¼ê°­ë‹ˆë‹¤.
        if len(text) > 400:
            split_texts = text_splitter.split_text(text)
            for i, st in enumerate(split_texts):
                chunks.append({
                    "type": "projects",
                    "text": f"(ë¶€ë¶„ {i+1}) {st}",
                    "metadata": { "source": "resume", "category": "project", "title": title }
                })
        else:
            chunks.append({
                "type": "projects",
                "text": text,
                "metadata": { "source": "resume", "category": "project" }
            })

    # ----------------------------------------------------
    # 8. ìê¸°ì†Œê°œì„œ (Self Intro)
    # ----------------------------------------------------
    self_intros = parsed_data.get("self_intro", [])
    for idx, intro in enumerate(self_intros):
        question = intro.get("question", "")
        answer = intro.get("answer", "")
        
        # [í•´ì„] ì§ˆë¬¸ì€ ê·¸ ìì²´ë¡œ ì¤‘ìš”í•˜ë¯€ë¡œ í•˜ë‚˜ì˜ ì¡°ê°ìœ¼ë¡œ ë”°ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        chunks.append({
            "type": "narrative_q",
            "text": f"[ìì†Œì„œ ì§ˆë¬¸{idx+1}] {question}",
            "metadata": { "source": "resume", "category": "narrative", "subtype": "question" }
        })

        # [í•´ì„] ë‹µë³€ì€ ë§¤ìš° ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ê·¸ë˜ì„œ ìœ„ì—ì„œ ì„¤ì •í•œ text_splitterë¡œ ìª¼ê°­ë‹ˆë‹¤.
        if answer:
            # [ë¬¸ë²•] split_text(): ê¸´ ë‹µë³€ì„ 600ì ë‚´ì™¸ì˜ ì¡°ê° ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
            split_texts = text_splitter.split_text(answer)
            for i, split_text in enumerate(split_texts):
                chunks.append({
                    "type": "narrative_a",
                    "text": f"[ìì†Œì„œ ë‹µë³€{idx+1}-{i+1}] {split_text}",
                    "metadata": {
                        "source": "resume",
                        "category": "narrative",
                        "subtype": "answer",
                        "question_ref": question[:20] + "..." # ì–´ë–¤ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì¸ì§€ ë§¥ë½ ì •ë³´ë¥¼ ë‚¨ê¹€
                    }
                })

    print(f"\nâœ… ì´ {len(chunks)}ê°œì˜ ì²­í¬(Chunk) ìƒì„± ì™„ë£Œ")
    return chunks

# -----------------------------------------------------------
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ
# -----------------------------------------------------------
if __name__ == "__main__":
    # [í•´ì„] ì‹¤ì œ ì‹¤í–‰í•˜ë©´ íŒŒì‹± ê²°ê³¼ë¥¼ 'chunked_result.json'ì´ë¼ëŠ” íŒŒì¼ë¡œ ì €ì¥í•´ì¤ë‹ˆë‹¤.
    # ê·¸ë˜ì•¼ ê°œë°œìê°€ ëˆˆìœ¼ë¡œ ì¡°ê°ì´ ì˜ ë‚¬ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë‹ˆê¹Œìš”.
    pass
