import sys
import os
import torch
from sqlalchemy import text

# ğŸš¨ [ìµœì‹  í‘œì¤€] langchain_huggingface ì‚¬ìš©
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

# -----------------------------------------------------------
# [ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, "ai-worker"))

try:
    from db import engine
except ImportError:
    try:
        sys.path.append("/app/backend-core") 
        from db import engine
    except ImportError:
        print("âŒ db.py ë¡œë“œ ì‹¤íŒ¨")
        sys.exit(1)

# -----------------------------------------------------------
# [ëª¨ë¸ ì„¤ì •] Step 6(ì €ì¥) ë•Œ ì“´ ëª¨ë¸ê³¼ 100% ì¼ì¹˜í•´ì•¼ í•¨!
# -----------------------------------------------------------
from .embedding import get_embedder as _get_central_embedder

def get_embedder():
    """ì¤‘ì•™í™”ëœ ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    import torch
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    return _get_central_embedder(device)

from langchain_community.vectorstores import PGVector

# -----------------------------------------------------------
# [í•µì‹¬] ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤ ì‹±ê¸€í†¤ ê´€ë¦¬
# -----------------------------------------------------------
_vector_stores = {}

def get_vector_store(collection_name):
    """ì§€ì •ëœ ì»¬ë ‰ì…˜ì— ëŒ€í•œ PGVector ì¸ìŠ¤í„´ìŠ¤ ì‹±ê¸€í†¤ ë°˜í™˜ (engine ê³µìœ )"""
    global _vector_stores
    if collection_name not in _vector_stores:
        embedder = get_embedder()
        if not embedder:
            return None
        
        # [ìµœì¢… ìˆ˜ì •] ìœ„ì¹˜ ì¸ì(Positional)ë¡œ ì „ë‹¬í•˜ì—¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± ë¬¸ì œ ì™„ë²½ í•´ê²°
        from db import engine
        connection_url = os.getenv("DATABASE_URL", "postgresql+psycopg://admin:1234@db:5432/interview_db")
        _vector_stores[collection_name] = PGVector(
            connection_url,        # 1. connection_string (ìœ„ì¹˜ ì¸ì)
            embedder,              # 2. embedding_function (ìœ„ì¹˜ ì¸ì)
            collection_name=collection_name,
            connection=engine      # 3. ê°ì²´ ê³µìœ 
        )
    return _vector_stores[collection_name]

# -----------------------------------------------------------
# [í•µì‹¬] ê²€ìƒ‰ í•¨ìˆ˜ (LangChain PGVector í™œìš©)
# -----------------------------------------------------------
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

def retrieve_context(query, resume_id=1, top_k=10, filter_type=None):
    """
    LangChain PGVectorë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ë§¥ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” [RAG ê²€ìƒ‰ ì‹œì‘] Query: '{query}' | ResumeID: {resume_id} | Filter: {filter_type}")
    
    # 1. ì„ë² ë”© ëª¨ë¸ ë° ì—°ê²° ì„¤ì •
    embedder = get_embedder()
    if not embedder:
        logger.error("âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return []
    
    try:
        # 2. PGVector ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ìºì‹± í™œìš©)
        vector_store = get_vector_store("resume_all_embeddings")
        if not vector_store:
             return []

        # 3. í•„í„° ì„¤ì • (resume_id + chunk_type)
        search_filter = {"resume_id": resume_id}
        if filter_type:
            search_filter["chunk_type"] = filter_type

        # 4. ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        logger.debug(f"ğŸ“ ì¿¼ë¦¬ ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
        docs_with_scores = vector_store.similarity_search_with_score(
            query, 
            k=top_k,
            filter=search_filter
        )

        # 5. ê²°ê³¼ ê°€ê³µ ë° ë¡œê¹…
        results = []
        if not docs_with_scores:
            logger.warning(f"âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. (Filter: {search_filter})")
            return []

        logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(docs_with_scores)}ê°œì˜ ë¬¸ë§¥ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        for i, (doc, score) in enumerate(docs_with_scores):
            res = {
                'text': doc.page_content,
                'meta': doc.metadata,
                'score': float(score)
            }
            results.append(res)
            
            # ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
            preview = res['text'].replace('\n', ' ')[:100]
            c_type = res['meta'].get('chunk_type', 'N/A')
            logger.info(f"   ğŸ‘‰ [{i+1}] [Dist: {res['score']:.4f} | Type: {c_type}] {preview}...")

        return results

    except Exception as e:
        logger.error(f"âŒ LangChain PGVector ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}", exc_info=True)
        return []

# -----------------------------------------------------------
# [í•µì‹¬] Retriever ìƒì„± í•¨ìˆ˜ (LangChain LCELìš©)
# -----------------------------------------------------------
def get_retriever(resume_id=1, top_k=10, filter_type=None):
    """
    LangChain LCELì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Retriever ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    embedder = get_embedder()
    # 2. ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    vector_store = get_vector_store("resume_all_embeddings")
    if not vector_store:
         return None

    # í•„í„° ì„¤ì •
    search_filter = {"resume_id": resume_id}
    if filter_type:
        search_filter["chunk_type"] = filter_type

    logger.info(f"ğŸ“¡ Retriever ìƒì„± ì™„ë£Œ (ResumeID: {resume_id}, Filter: {filter_type})")
    return vector_store.as_retriever(
        search_kwargs={
            "k": top_k,
            "filter": search_filter
        }
    )

# -----------------------------------------------------------
# [ë³€ê²½ ì™„ë£Œ] ì§ˆë¬¸ ì€í–‰(questions í…Œì´ë¸”) ê²€ìƒ‰ í•¨ìˆ˜ (All LangChain ë°©ì‹)
# -----------------------------------------------------------
def retrieve_similar_questions(query, top_k=5):
    """
    LangChain PGVectorë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ì€í–‰ì—ì„œ ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ” [ì§ˆë¬¸ ì€í–‰ ê²€ìƒ‰ ì‹œì‘] Query: '{query[:50]}...' (Framework: LangChain)")
    
    embedder = get_embedder()
    if not embedder:
        logger.error("âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ì§ˆë¬¸ ì€í–‰ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return []
    
    try:
        # ì§ˆë¬¸ ì€í–‰ì€ ë³„ë„ì˜ ì»¬ë ‰ì…˜/í…Œì´ë¸”(questions)ì„ ì‚¬ìš©í•˜ë¯€ë¡œ collection_nameì„ ë§ì¶°ì¤ë‹ˆë‹¤.
        vector_store = get_vector_store("questions_collection")
        if not vector_store:
             return []
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        docs_with_scores = vector_store.similarity_search_with_score(query, k=top_k)
        
        results = []
        if not docs_with_scores:
            logger.warning("âš ï¸ ì§ˆë¬¸ ì€í–‰ì—ì„œ ê²€ìƒ‰ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: ë­ì²´ì¸ ì—”ì§„ì„ í†µí•´ {len(docs_with_scores)}ê°œì˜ ìœ ì‚¬ ì§ˆë¬¸ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        for i, (doc, score) in enumerate(docs_with_scores):
            res = {
                "text": doc.page_content, 
                "meta": doc.metadata, 
                "score": float(score)
            }
            results.append(res)
            # ìƒì„¸ ë°ì´í„° ë¡œê·¸ ì¶œë ¥
            logger.info(f"   ğŸ‘‰ [{i+1}] [Dist: {res['score']:.4f}] {res['text'][:100]}...")
            
        return results
            
    except Exception as e:
        logger.error(f"âŒ ë­ì²´ì¸ ê¸°ë°˜ ì§ˆë¬¸ ì€í–‰ ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}", exc_info=True)
        return []

# -----------------------------------------------------------
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# -----------------------------------------------------------
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ 1: í•„í„° ì—†ì´ ê²€ìƒ‰ (ê¸°ë³¸)
    print("--- [Test 1: ì „ì²´ ê²€ìƒ‰] ---")
    retrieve_context("ë³´ì•ˆ ê¸°ìˆ  ìŠ¤í‚¬", resume_id=1)
    
    # í…ŒìŠ¤íŠ¸ 2: 'í”„ë¡œì íŠ¸'ë§Œ ì½• ì§‘ì–´ì„œ ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ)
    print("\n--- [Test 2: í”„ë¡œì íŠ¸ í•„í„°ë§ ê²€ìƒ‰] ---")
    found = retrieve_context("ì„±ê³¼ ë‹¬ì„± ê²½í—˜", resume_id=1, filter_category="project")

    if found:
        print("\nâœ… [ê²€ìƒ‰ ê²°ê³¼ í™•ì¸]")
        for item in found:
            # ë©”íƒ€ë°ì´í„°ë„ ê°™ì´ ì¶œë ¥í•´ë´…ë‹ˆë‹¤.
            print(f"[ì¹´í…Œê³ ë¦¬: {item['meta'].get('category')}] {item['text'][:50]}...")