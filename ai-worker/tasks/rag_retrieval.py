import sys
import os
import torch
from sqlalchemy import text

# ğŸš¨ [ìµœì‹  í‘œì¤€] langchain_huggingface ì‚¬ìš©
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings

# -----------------------------------------------------------
# [ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, "ai-worker"))

try:
    from db import engine
except ImportError:
    try:
        sys.path.append("/app/backend-core") 
        from db import engine
    except ImportError:
        print("âŒ db.py ë¡œë“œ ì‹¤íŒ¨")
        sys.exit(1)

# -----------------------------------------------------------
# [ëª¨ë¸ ì„¤ì •] Step 6(ì €ì¥) ë•Œ ì“´ ëª¨ë¸ê³¼ 100% ì¼ì¹˜í•´ì•¼ í•¨!
# -----------------------------------------------------------
EMBEDDING_MODEL = "nlpai-lab/KURE-v1" 

_embedder = None

def get_embedder():
    global _embedder
    if _embedder is None:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        cache_dir = "/app/models/embeddings" if os.path.exists("/app/models") else "./models/embeddings"
        os.makedirs(cache_dir, exist_ok=True)
        
        print(f"[STEP7] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ({EMBEDDING_MODEL}) on {device}...")
        print(f"ğŸ“‚ ìºì‹œ ê²½ë¡œ: {cache_dir}")
        
        try:
            _embedder = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL,
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True},
                cache_folder=cache_dir
            )
            print("âœ… RAG ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì—¬ê¸°ì„œ sys.exit(1)ì„ í•˜ë©´ ì›Œì»¤ ìì²´ê°€ ì£½ìœ¼ë¯€ë¡œ ì£¼ì˜
            return None
    return _embedder

# -----------------------------------------------------------
# [í•µì‹¬] ê²€ìƒ‰ í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìš©)
# -----------------------------------------------------------
from langchain_community.vectorstores import PGVector

# -----------------------------------------------------------
# [í•µì‹¬] ê²€ìƒ‰ í•¨ìˆ˜ (LangChain PGVector í™œìš©)
# -----------------------------------------------------------
def retrieve_context(query, resume_id=1, top_k=3, filter_category=None):
    """
    LangChain PGVectorë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ë§¥ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    print(f"\nğŸ” [RAG ê²€ìƒ‰] í‚¤ì›Œë“œ: '{query}' (ì§€ì›ì ID: {resume_id}, í•„í„°: {filter_category})")
    
    # 1. ì„ë² ë”© ëª¨ë¸ ë° ì—°ê²° ì„¤ì •
    embedder = get_embedder()
    if not embedder:
        print("âŒ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    connection_string = os.getenv("DATABASE_URL", "postgresql+psycopg://postgres:1234@db:5432/interview_db")
    
    try:
        # 2. PGVector ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        vector_store = PGVector(
            connection_string=connection_string,
            embedding_function=embedder,
            collection_name="resume_all_embeddings"
        )

        # 3. í•„í„° ì„¤ì • (resume_id + category)
        search_filter = {"resume_id": resume_id}
        if filter_category:
            search_filter["category"] = filter_category

        # 4. ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        docs_with_scores = vector_store.similarity_search_with_score(
            query, 
            k=top_k,
            filter=search_filter
        )

        # 5. ê²°ê³¼ ê°€ê³µ
        results = []
        for doc, score in docs_with_scores:
            results.append({
                'text': doc.page_content,
                'meta': doc.metadata,
                'score': float(score)  # ê±°ë¦¬ ì ìˆ˜ ì¶”ê°€
            })

        print(f"   ğŸ‘‰ {len(results)}ê°œì˜ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        for i, res in enumerate(results):
            preview = res['text'].replace('\n', ' ')[:80]
            category = res['meta'].get('category', 'N/A')
            print(f"      [{i+1}] (Dist: {res['score']:.4f}, Cat: {category}): {preview}...")

        return results

    except Exception as e:
        print(f"âŒ LangChain PGVector ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

# -----------------------------------------------------------
# [í•µì‹¬] Retriever ìƒì„± í•¨ìˆ˜ (LangChain LCELìš©)
# -----------------------------------------------------------
def get_retriever(resume_id=1, top_k=3, filter_category=None):
    """
    LangChain LCELì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” Retriever ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    embedder = get_embedder()
    connection_string = os.getenv("DATABASE_URL", "postgresql+psycopg://postgres:1234@db:5432/interview_db")
    
    vector_store = PGVector(
        connection_string=connection_string,
        embedding_function=embedder,
        collection_name="resume_all_embeddings"
    )

    # í•„í„° ì„¤ì •
    search_filter = {"resume_id": resume_id}
    if filter_category:
        search_filter["category"] = filter_category

    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•„í„°ë§í•˜ì—¬ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •
    return vector_store.as_retriever(
        search_kwargs={
            "k": top_k,
            "filter": search_filter
        }
    )

# -----------------------------------------------------------
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# -----------------------------------------------------------
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ 1: í•„í„° ì—†ì´ ê²€ìƒ‰ (ê¸°ë³¸)
    print("--- [Test 1: ì „ì²´ ê²€ìƒ‰] ---")
    retrieve_context("ë³´ì•ˆ ê¸°ìˆ  ìŠ¤í‚¬", resume_id=1)
    
    # í…ŒìŠ¤íŠ¸ 2: 'í”„ë¡œì íŠ¸'ë§Œ ì½• ì§‘ì–´ì„œ ê²€ìƒ‰ (í•˜ì´ë¸Œë¦¬ë“œ)
    print("\n--- [Test 2: í”„ë¡œì íŠ¸ í•„í„°ë§ ê²€ìƒ‰] ---")
    found = retrieve_context("ì„±ê³¼ ë‹¬ì„± ê²½í—˜", resume_id=1, filter_category="project")

    if found:
        print("\nâœ… [ê²€ìƒ‰ ê²°ê³¼ í™•ì¸]")
        for item in found:
            # ë©”íƒ€ë°ì´í„°ë„ ê°™ì´ ì¶œë ¥í•´ë´…ë‹ˆë‹¤.
            print(f"[ì¹´í…Œê³ ë¦¬: {item['meta'].get('category')}] {item['text'][:50]}...")
