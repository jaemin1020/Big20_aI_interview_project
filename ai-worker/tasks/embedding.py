import sys
import os
import json
import torch

# ğŸš¨ [ìµœì‹  í‘œì¤€] langchain_huggingface íŒ¨í‚¤ì§€ ì‚¬ìš© (v0.2+)
from langchain_huggingface import HuggingFaceEmbeddings

# -----------------------------------------------------------
# [ëª¨ë¸ ì„¤ì •]
# ì‚¬ìš©ìê°€ ì„ íƒí•œ ìµœì‹  í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸
# -----------------------------------------------------------
EMBEDDING_MODEL = "nlpai-lab/KURE-v1" 

# 2. ì„ë² ë”© ëª¨ë¸ ì‹±ê¸€í†¤ ê´€ë¦¬
_embedder = None

def get_embedder(device):
    global _embedder
    if _embedder is None:
        cache_dir = "/app/models/embeddings" if os.path.exists("/app/models") else "./models/embeddings"
        os.makedirs(cache_dir, exist_ok=True)
        
        print(f"ğŸš€ [STEP5] ì„ë² ë”© ëª¨ë¸ ìƒì£¼ ì‘ì—… ì‹œì‘ (ëª¨ë¸: {EMBEDDING_MODEL})...")
        print(f"ğŸ“‚ ìºì‹œ ê²½ë¡œ: {cache_dir} (ì²« ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œë¡œ ì¸í•´ 3~5ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        _embedder = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': device},
            encode_kwargs={'normalize_embeddings': True},
            cache_folder=cache_dir
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë©”ëª¨ë¦¬ ìƒì£¼ ì™„ë£Œ!")
    return _embedder

def embed_chunks(chunks):
    # 1. ì¥ì¹˜ ì„¤ì • (GPU ìš°ì„ )
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # 2. ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜)
    print(f"ğŸ“¡ [STEP5] ëª¨ë¸ ìƒíƒœ í™•ì¸ ì¤‘...")
    embedder = get_embedder(device)
    print(f"ğŸ‘‰ ì‚¬ìš© ì¥ì¹˜: {device} (Warm Start ì ìš©ì§)")

    # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = [c["text"] for c in chunks]
    
    # 4. ë²¡í„° ë³€í™˜ ìˆ˜í–‰
    try:
        # embed_documents: ì—¬ëŸ¬ ë¬¸ì¥ì„ í•œ ë²ˆì— ë²¡í„°í™”
        vectors = embedder.embed_documents(texts)
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
        return []

    # 5. ê²°ê³¼ í•©ì¹˜ê¸° (ë©”íƒ€ë°ì´í„° + ë²¡í„°)
    embedded_result = []
    for i, c in enumerate(chunks):
        embedded_result.append({
            "text": c["text"],         # ì²­í¬ í…ìŠ¤íŠ¸
            "type": c["type"],         # ë°ì´í„° íƒ€ì… (header, education ë“±)
            "metadata": c["metadata"], # ì›ë³¸ ì¶œì²˜ ì •ë³´
            "vector": vectors[i]       # [í•µì‹¬] 768 or 1024ì°¨ì› ë²¡í„°
        })

    print(f"[STEP5] ì„ë² ë”© ì™„ë£Œ! (ì´ {len(vectors)}ê°œ ì²­í¬)")
    
    # [ì¤‘ìš”] ë²¡í„° ì°¨ì› í™•ì¸ (DB í…Œì´ë¸” ìƒì„± ì‹œ ì´ ìˆ«ìê°€ í•„ìš”í•¨)
    if vectors:
        print(f"ğŸ‘‰ ë²¡í„° ì°¨ì›(Dimension): {len(vectors[0])}")
    
    return embedded_result

# -----------------------------------------------------------
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ
# -----------------------------------------------------------
if __name__ == "__main__":
    # ì´ì „ ë‹¨ê³„ ëª¨ë“ˆ import
    try:
        from parse_resume import parse_resume_final 
        from chunking import chunk_resume
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ Import ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # 1. íŒŒì¼ ê²½ë¡œ í™•ì¸
    target_pdf = "resume.pdf"
    if not os.path.exists(target_pdf):
        target_pdf = "/app/resume.pdf"

    if os.path.exists(target_pdf):
        print(f"ğŸš€ [Pipeline] íŒŒì¼ ë¡œë“œ: {target_pdf}")
        
        # Step 2: íŒŒì‹±
        parsed_data = parse_resume_final(target_pdf)
        
        if parsed_data:
            # Step 4: ì²­í‚¹
            chunks = chunk_resume(parsed_data)
            
            if chunks:
                # Step 5: ì„ë² ë”©
                embedded_data = embed_chunks(chunks)
                
                if embedded_data:
                    # ê²°ê³¼ ì €ì¥
                    output_file = "embedded_result.json"
                    
                    # ë¯¸ë¦¬ë³´ê¸° (ë²¡í„°ëŠ” ë„ˆë¬´ ê¸°ë‹ˆê¹Œ ê¸¸ì´ë§Œ ì¶œë ¥)
                    print("\n--- [ì„ë² ë”© ê²°ê³¼ ì˜ˆì‹œ (ì²« ë²ˆì§¸ ì²­í¬)] ---")
                    sample = embedded_data[0].copy()
                    vec_len = len(sample['vector'])
                    sample["vector"] = f"[Vector of size {vec_len}...]" 
                    print(json.dumps(sample, indent=2, ensure_ascii=False))

                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(embedded_data, f, indent=2, ensure_ascii=False)
                    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {output_file}")
            else:
                print("âŒ ì²­í‚¹ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("âŒ íŒŒì‹± ì‹¤íŒ¨")
    else:
        print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {target_pdf}")
