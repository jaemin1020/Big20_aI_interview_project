import logging
import time
import re
import json
import sys
import os
from celery import shared_task

# DB Helper Functions
from db import (
    engine,
    Session,
    Transcript,
    update_transcript_sentiment,
    update_question_avg_score,
    get_interview_transcripts,
    get_user_answers
)

# ìƒìœ„ ë””ë ‰í† ë¦¬(ai-worker)ë¥¼ ê²½ë¡œì— ì¶”ê°€
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from utils.exaone_llm import get_exaone_llm

logger = logging.getLogger("AI-Worker-Evaluator")

@shared_task(name="tasks.evaluator.analyze_answer")
def analyze_answer(transcript_id: int, question_text: str, answer_text: str, rubric: dict = None, question_id: int = None):
    """ê°œë³„ ë‹µë³€ í‰ê°€ ë° ì‹¤ì‹œê°„ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± íŠ¸ë¦¬ê±°"""
    
    # ğŸ”— ì¦‰ì‹œ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± íŠ¸ë¦¬ê±° (ë¶„ì„ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë°”ë¡œ ìƒì„± ì‹œì‘)
    try:
        from tasks.question_generation import generate_next_question_task
        interview_id = None
        with Session(engine) as session:
            t = session.get(Transcript, transcript_id)
            if t:
                interview_id = t.interview_id
        
        if interview_id:
            generate_next_question_task.delay(interview_id)
            logger.info(f"ğŸš€ [IMMEDIATE] delay() called for Interview {interview_id}")
        else:
            logger.error(f"Could not find interview_id for transcript {transcript_id}")
    except Exception as e:
        logger.error(f"Failed to trigger next question task: {e}")
    logger.info(f"Analyzing Transcript {transcript_id} for Question {question_id}")
    
    if not answer_text or not answer_text.strip():
        logger.warning(f"Empty answer for transcript {transcript_id}. Skipping LLM evaluation.")
        return {
            "technical_score": 0,
            "communication_score": 0,
            "feedback": "ë‹µë³€ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }
    
    start_ts = time.time()
    
    try:
        # GPU ë ˆì´ì–´ í™•ì¸ (CPU ì›Œì»¤ë©´ ë¬´ê±°ìš´ ë¶„ì„ ìƒëµí•˜ì—¬ í ì •ì²´ ë°©ì§€)
        n_gpu_layers = int(os.getenv("N_GPU_LAYERS", "0"))
        
        if n_gpu_layers == 0:
            logger.info("âš¡ [FAST MODE] CPU Worker spotted. Skipping heavy LLM for individual answer evaluation to speed up the process.")
            # ê°œë³„ ë¶„ì„ì€ ê¸°ë³¸ê°’ë§Œ ë¶€ì—¬ (ìµœì¢… ë¦¬í¬íŠ¸ì—ì„œ ì „ì²´ ìš”ì•½ ìˆ˜í–‰)
            result = {
                "technical_score": 3,
                "communication_score": 3,
                "feedback": "ë‹µë³€ì´ ìˆ˜ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ í‰ê°€ëŠ” ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            }
        else:
            llm = get_exaone_llm()
            result = llm.evaluate_answer(
                question_text=question_text,
                answer_text=answer_text,
                rubric=rubric
            )
        
        tech_score = result.get("technical_score", 3)
        comm_score = result.get("communication_score", 3)
        sentiment = ((tech_score + comm_score) / 10.0) - 0.5 
        
        update_transcript_sentiment(
            transcript_id, 
            sentiment_score=sentiment, 
            emotion="neutral"
        )
        
        answer_quality = (tech_score + comm_score) * 10 
        
        if question_id:
            update_question_avg_score(question_id, answer_quality)

        duration = time.time() - start_ts
        logger.info(f"Evaluation Completed ({duration:.2f}s)")
        return result

    except Exception as e:
        logger.error(f"Evaluation Failed: {e}")
        return {"error": str(e)}

@shared_task(name="tasks.evaluator.generate_final_report")
def generate_final_report(interview_id: int):
    """
    ìµœì¢… í‰ê°€ ë³´ê³ ì„œ ìƒì„±
    
    Args:
        interview_id (int): ì¸í„°ë·° ID
    
    Returns:
        None
    
    Raises:
        ValueError: ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°
    
    ìƒì„±ì: ejm
    ìƒì„±ì¼ì: 2026-02-04
    """
    logger.info(f"Generating Final Report for Interview {interview_id}")
    from db import create_or_update_evaluation_report, update_interview_overall_score, get_interview_transcripts
    
    try:
        transcripts = get_interview_transcripts(interview_id)
        if not transcripts:
            logger.warning("No transcripts found for this interview.")
            create_or_update_evaluation_report(
                interview_id,
                technical_score=0, communication_score=0, cultural_fit_score=0,
                summary_text="ê¸°ë¡ëœ ëŒ€í™”ê°€ ì—†ì–´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                details_json={"error": "no_data"}
            )
            return

        conversation = "\n".join([f"{t.speaker}: {t.text}" for t in transcripts])

        try:
            exaone = get_exaone_llm()
            system_msg = "ê·€í•˜ëŠ” ë©´ì ‘ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë©´ì ‘ ì „ì²´ ìš”ì•½ê³¼ ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ì‹­ì‹œì˜¤."
            user_msg = f"""ë‹¤ìŒ ë©´ì ‘ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ JSONìœ¼ë¡œ ë§Œë“œì„¸ìš”.
            
[ëŒ€í™”]
{conversation}

 ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
    "technical_score": 0~100,
    "communication_score": 0~100,
    "cultural_fit_score": 0~100,
    "summary_text": "3ë¬¸ì¥ ì´ë‚´ ìš”ì•½",
    "strengths": ["ê°•ì 1", "ê°•ì 2"],
    "weaknesses": ["ì•½ì 1", "ì•½ì 2"]
}}"""
            
            prompt = exaone._create_prompt(system_msg, user_msg)
            output = exaone.llm(prompt, max_tokens=1024, temperature=0.3)
            raw_result = output['choices'][0]['text'].strip()
            
            json_match = re.search(r'\{.*\}', raw_result, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
            else:
                raise ValueError("No JSON in response")
                
        except Exception as llm_err:
            logger.error(f"LLM Summary failed: {llm_err}")
            result = {
                "technical_score": 75, "communication_score": 75, "cultural_fit_score": 75,
                "summary_text": "ë¶„ì„ ì‹œìŠ¤í…œ ì§€ì—°ìœ¼ë¡œ ìš”ì•½ì´ ì§€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "strengths": ["ì„±ì‹¤í•œ ë‹µë³€"], "weaknesses": ["ìƒì„¸ ë¶„ì„ ë¶ˆê°€"]
            }

        tech = result.get("technical_score", 0)
        comm = result.get("communication_score", 0)
        cult = result.get("cultural_fit_score", 0)
        overall = (tech + comm + cult) / 3

        create_or_update_evaluation_report(
            interview_id,
            technical_score=tech,
            communication_score=comm,
            cultural_fit_score=cult,
            summary_text=result.get("summary_text", ""),
            details_json={
                "strengths": result.get("strengths", []),
                "weaknesses": result.get("weaknesses", [])
            }
        )
        update_interview_overall_score(interview_id, score=overall)
        logger.info(f"âœ… Final Report Generated for Interview {interview_id}")

    except Exception as e:
        logger.error(f"âŒ Error in generate_final_report: {e}")
        create_or_update_evaluation_report(
            interview_id,
            technical_score=0, summary_text=f"ì˜¤ë¥˜: {str(e)}"
        )