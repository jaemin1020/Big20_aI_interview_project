import logging
import time
import re
import json
import sys
import os
from pydantic import BaseModel, Field
from typing import List
from langchain_core.output_parsers import JsonOutputParser
from celery import shared_task

# DB Helper Functions
from db import (
    engine,
    Session,
    Transcript,
    update_transcript_sentiment,
    update_question_avg_score,
    get_interview_transcripts,
    get_user_answers
)

# AI-Worker ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
current_file_path = os.path.abspath(__file__) # tasks/evaluator.py
tasks_dir = os.path.dirname(current_file_path) # tasks/
ai_worker_root = os.path.dirname(tasks_dir)    # ai-worker/

if ai_worker_root not in sys.path:
    sys.path.insert(0, ai_worker_root)

# utils.exaone_llmì€ ì‹¤ì œ ì‚¬ìš© ì‹œì ì— ì„í¬íŠ¸ (ì›Œì»¤ ì‹œì‘ ì‹œ í¬ë˜ì‹œ ë°©ì§€)
try:
    from utils.exaone_llm import get_exaone_llm
except ImportError:
    def get_exaone_llm():
        from ai_worker.utils.exaone_llm import get_exaone_llm
        return get_exaone_llm()

logger = logging.getLogger("AI-Worker-Evaluator")

# -----------------------------------------------------------
# [Schema] í‰ê°€ ë°ì´í„° êµ¬ì¡° ì •ì˜ (Pydantic)
# -----------------------------------------------------------
class AnswerEvalSchema(BaseModel):
    technical_score: int = Field(description="ê¸°ìˆ ì  ì§€ì‹ ë° ìˆ™ë ¨ë„ ì ìˆ˜ (0-5)")
    communication_score: int = Field(description="ì˜ì‚¬ì†Œí†µ ë° ì „ë‹¬ ëŠ¥ë ¥ ì ìˆ˜ (0-5)")
    feedback: str = Field(description="ë‹µë³€ì— ëŒ€í•œ êµ¬ì²´ì ì´ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±")

class FinalReportSchema(BaseModel):
    technical_score: int = Field(description="ì „ì²´ ê¸°ìˆ  ë©´ì ‘ ì ìˆ˜ (0-100)")
    communication_score: int = Field(description="ì „ì²´ ì˜ì‚¬ì†Œí†µ ì ìˆ˜ (0-100)")
    cultural_fit_score: int = Field(description="ì¡°ì§ ì í•©ì„± ì ìˆ˜ (0-100)")
    summary_text: str = Field(description="ë©´ì ‘ ì „ì²´ ìš”ì•½ (3ë¬¸ì¥ ë‚´ì™¸)")
    strengths: List[str] = Field(description="ì§€ì›ìì˜ ì£¼ìš” ê°•ì  3ê°€ì§€")
    weaknesses: List[str] = Field(description="ë³´ì™„ì´ í•„ìš”í•œ ì•½ì  ë° ê°œì„ ì ")

@shared_task(name="tasks.evaluator.analyze_answer")
def analyze_answer(transcript_id: int, question_text: str, answer_text: str, rubric: dict = None, question_id: int = None):
    """ê°œë³„ ë‹µë³€ í‰ê°€ ë° ì‹¤ì‹œê°„ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± íŠ¸ë¦¬ê±°"""
    
    # ğŸ”— ì¦‰ì‹œ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± íŠ¸ë¦¬ê±° (ë¶„ì„ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ë°”ë¡œ ìƒì„± ì‹œì‘)
    try:
        from tasks.question_generator import generate_next_question_task
        interview_id = None
        with Session(engine) as session:
            t = session.get(Transcript, transcript_id)
            if t:
                interview_id = t.interview_id
        
        if interview_id:
            generate_next_question_task.apply_async(args=[interview_id], queue='gpu_queue')
            logger.info(f"ğŸš€ [IMMEDIATE] apply_async(queue='gpu_queue') called for Interview {interview_id}")
        else:
            logger.error(f"Could not find interview_id for transcript {transcript_id}")
    except Exception as e:
        logger.error(f"Failed to trigger next question task: {e}")
    logger.info(f"Analyzing Transcript {transcript_id} for Question {question_id}")
    
    if not answer_text or not answer_text.strip():
        logger.warning(f"Empty answer for transcript {transcript_id}. Skipping LLM evaluation.")
        return {
            "technical_score": 0,
            "communication_score": 0,
            "feedback": "ë‹µë³€ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        }
    
    start_ts = time.time()
    
    # [ìˆ˜ì •: 2026-02-12] ë¹„ì „ ë°ì´í„°(Vision Analysis) ì¡°íšŒ ë° í†µí•©
    vision_data = None
    with Session(engine) as session:
        t = session.get(Transcript, transcript_id)
        if t and t.vision_analysis:
            vision_data = t.vision_analysis

    vision_summary = "ë¹„ì „ ë¶„ì„ ë°ì´í„° ì—†ìŒ"
    vision_score_breakdown = {}
    
    if vision_data:
        # [POC ì ìˆ˜ ë¡œì§ êµ¬í˜„]
        # ì°¸ê³ : ai-worker/poc/cv_poc/CV-V2-TASK.py
        
        # 1. ìƒìˆ˜ ë° ê°€ì¤‘ì¹˜ ì„¤ì •
        WEIGHT_CONFIDENCE = 0.3   # ìì‹ ê° (ë¯¸ì†Œ)
        WEIGHT_FOCUS      = 0.3   # ì‹œì„  ì§‘ì¤‘ (ì •ë©´ ì‘ì‹œ)
        WEIGHT_POSTURE    = 0.2   # ìì„¸ ì•ˆì • (ê³ ê°œ í”ë“¤ë¦¼ ì—†ìŒ)
        WEIGHT_EMOTION    = 0.2   # ì •ì„œ ì•ˆì • (ë¶ˆì•ˆ/ì°Œí‘¸ë¦¼ ì—†ìŒ)
        
        # 2. ì›ë³¸ ë°ì´í„° ì¶”ì¶œ ë° ì •ê·œí™”
        total_frames = vision_data.get('duration_frames', 1)
        if total_frames == 0: total_frames = 1
        
        # ì‹œì„  ë¹„ìœ¨ (ì´ë¯¸ í¼ì„¼íŠ¸ ë‹¨ìœ„)
        gaze_ratio = vision_data.get('gaze_center_pct', 0) 
        
        # ë¯¸ì†Œ ì ìˆ˜ (0.0-1.0 -> 0-100 í™˜ì‚°)
        avg_smile = vision_data.get('avg_smile_score', 0) * 100
        
        # ë¶ˆì•ˆ ì ìˆ˜ (0.0-1.0 -> 0-100 í™˜ì‚°)
        avg_anxiety = vision_data.get('avg_anxiety_score', 0) * 100
        
        # ìì„¸ ì•ˆì • ë¹„ìœ¨ (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìˆ˜ì§‘í•œ 'posture_stable_pct' ì‚¬ìš©)
        posture_ratio = vision_data.get('posture_stable_pct', 80.0) # ì—†ìœ¼ë©´ ê¸°ë³¸ 80
        
        # 3. ê°€ì¤‘ì¹˜ ì ìˆ˜ ê³„ì‚° (PoC ê°€ì¤‘ì¹˜ ì ìš©)
        score_conf = avg_smile * WEIGHT_CONFIDENCE
        score_focus = gaze_ratio * WEIGHT_FOCUS
        score_posture = posture_ratio * WEIGHT_POSTURE
        score_emotion = (100 - avg_anxiety) * WEIGHT_EMOTION
        
        overall_vision_score = score_conf + score_focus + score_posture + score_emotion
        
        vision_score_breakdown = {
            "confidence": round(score_conf, 1),
            "focus": round(score_focus, 1),
            "posture": round(score_posture, 1),
            "emotion": round(score_emotion, 1),
            "total": round(overall_vision_score, 1)
        }
        
        logger.info(f"ğŸ“Š [Vision Score Breakdown] Transcript={transcript_id} | Total={overall_vision_score} | Breakdown={vision_score_breakdown}")
        
        vision_summary = f"""
[ë¹„ì–¸ì–´ì  íƒœë„ ì±„ì  ê²°ê³¼ (ì´ì : {overall_vision_score:.1f}/100)]
1. ìì‹ ê°(ë¯¸ì†Œ): {score_conf:.1f}ì  (ë°°ì  30ì ) - í‰ê·  ë¯¸ì†Œ: {avg_smile:.1f}%
2. ì‹œì„ ì§‘ì¤‘: {score_focus:.1f}ì  (ë°°ì  30ì ) - ì •ë©´ ì‘ì‹œ: {gaze_ratio}%
3. ìì„¸ì•ˆì •: {score_posture:.1f}ì  (ë°°ì  20ì ) - ì•ˆì • ìœ ì§€: {posture_ratio}% (ì¶”ì •ì¹˜)
4. ì •ì„œì•ˆì •: {score_emotion:.1f}ì  (ë°°ì  20ì ) - ë¶ˆì•ˆ ì§€ìˆ˜: {avg_anxiety:.1f}% (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)

* ì´ ì ìˆ˜ëŠ” POC(V4.5) ì•Œê³ ë¦¬ì¦˜ì— ê¸°ë°˜í•˜ì—¬ ì‚°ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.
"""
    # [ìˆ˜ì •: 2026-02-12] ë„ì»¤ ë¡œê·¸ì— ë¹„ì „ ì ìˆ˜ ì¶œë ¥ (User Request - Critical)
    logger.info(f"ğŸ“Š [Vision Score Breakdown] {vision_summary}")

    try:
        # GPU ë ˆì´ì–´ í™•ì¸ (CPU ì›Œì»¤ë©´ ë¬´ê±°ìš´ ë¶„ì„ ìƒëµí•˜ì—¬ í ì •ì²´ ë°©ì§€)
        n_gpu_layers = int(os.getenv("N_GPU_LAYERS", "0"))
        
        if n_gpu_layers == 0:
            logger.info("âš¡ [FAST MODE] CPU Worker spotted. Skipping heavy LLM for individual answer evaluation.")
            result = {
                "technical_score": 3,
                "communication_score": 3,
                "feedback": "ë‹µë³€ì´ ìˆ˜ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ í‰ê°€ëŠ” ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
            }
        else:
            # LangChain Parser ì„¤ì •
            parser = JsonOutputParser(pydantic_object=AnswerEvalSchema)
            
            # ì—”ì§„ ê°€ì ¸ì˜¤ê¸°
            llm_engine = get_exaone_llm()
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_msg = "ê·€í•˜ëŠ” ì „ë¬¸ ë©´ì ‘ê´€ì´ë©°, ì§€ì›ìì˜ ë‹µë³€ì„ ê¸°ìˆ ë ¥ê³¼ ì˜ì‚¬ì†Œí†µ ê´€ì ì—ì„œ í‰ê°€í•©ë‹ˆë‹¤."
            user_msg = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì›ìì˜ ë‹µë³€ì„ ë£¨ë¸Œë¦­ ê¸°ì¤€ì— ë§ì¶° í‰ê°€í•˜ì‹­ì‹œì˜¤.
            
[ì§ˆë¬¸]
{question_text}

[ë‹µë³€]
{answer_text}

[í‰ê°€ ë£¨ë¸Œë¦­]
{json.dumps(rubric, ensure_ascii=False) if rubric else "í‘œì¤€ ë©´ì ‘ í‰ê°€ ê¸°ì¤€"}

{parser.get_format_instructions()}"""
            
            # ìƒì„± ë° íŒŒì‹±
            prompt = llm_engine._create_prompt(system_msg, user_msg)
            raw_output = llm_engine.invoke(prompt, temperature=0.2)
            
            try:
                result = parser.parse(raw_output)
            except Exception as parse_err:
                logger.error(f"Failed to parse LLM output: {parse_err}")
                # í´ë°±: ì •ê·œí‘œí˜„ì‹ ì‹œë„
                json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    result = {"technical_score": 3, "communication_score": 3, "feedback": "í‰ê°€ ë°ì´í„°ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        tech_score = result.get("technical_score", 3)
        comm_score = result.get("communication_score", 3)
        sentiment = ((tech_score + comm_score) / 10.0) - 0.5 
        
        update_transcript_sentiment(
            transcript_id, 
            sentiment_score=sentiment, 
            emotion="neutral"
        )
        
        answer_quality = (tech_score + comm_score) * 10 
        
        if question_id:
            update_question_avg_score(question_id, answer_quality)

        duration = time.time() - start_ts
        logger.info(f"Evaluation Completed ({duration:.2f}s)")
        return result

    except Exception as e:
        logger.error(f"Evaluation Failed: {e}")
        return {"error": str(e)}

@shared_task(name="tasks.evaluator.generate_final_report", queue='gpu_queue')
def generate_final_report(interview_id: int):
    """
    ìµœì¢… í‰ê°€ ë³´ê³ ì„œ ìƒì„±
    
    Args:
        interview_id (int): ì¸í„°ë·° ID
    
    Returns:
        None
    
    Raises:
        ValueError: ë‹µë³€ì´ ì—†ëŠ” ê²½ìš°
    
    """
    logger.info(f"Generating Final Report for Interview {interview_id}")
    from db import create_or_update_evaluation_report, update_interview_overall_score, get_interview_transcripts
    
    try:
        transcripts = get_interview_transcripts(interview_id)
        if not transcripts:
            logger.warning("No transcripts found for this interview.")
            create_or_update_evaluation_report(
                interview_id,
                technical_score=0, communication_score=0, cultural_fit_score=0,
                summary_text="ê¸°ë¡ëœ ëŒ€í™”ê°€ ì—†ì–´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                details_json={"error": "no_data"}
            )
            return

        conversation = "\n".join([f"{t.speaker}: {t.text}" for t in transcripts])

        try:
            # LangChain Parser ì„¤ì •
            parser = JsonOutputParser(pydantic_object=FinalReportSchema)
            
            exaone = get_exaone_llm()
            system_msg = "ê·€í•˜ëŠ” ì¸ì‚¬ ì „ëµ ì „ë¬¸ê°€ì´ì ë©´ì ‘ ë¶„ì„ê´€ì…ë‹ˆë‹¤. ì „ì²´ ëŒ€í™” íë¦„ì„ ë¶„ì„í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤."
            user_msg = f"""ë‹¤ìŒ ë©´ì ‘ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… í‰ê°€ë¥¼ ë‚´ë¦¬ì‹­ì‹œì˜¤.
            
[ë©´ì ‘ ëŒ€í™”]
{conversation}

{parser.get_format_instructions()}"""
            
            # ìƒì„± ë° íŒŒì‹±
            prompt = exaone._create_prompt(system_msg, user_msg)
            raw_output = exaone.invoke(prompt, temperature=0.3)
            
            try:
                result = parser.parse(raw_output)
            except Exception as parse_err:
                logger.error(f"Final report parsing failed: {parse_err}")
                json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    raise parse_err
                
        except Exception as llm_err:
            logger.error(f"LLM Summary failed: {llm_err}")
            result = {
                "technical_score": 75, "communication_score": 75, "cultural_fit_score": 75,
                "summary_text": "ë¶„ì„ ì‹œìŠ¤í…œ ì§€ì—°ìœ¼ë¡œ ìš”ì•½ì´ ì§€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "strengths": ["ì„±ì‹¤í•œ ë‹µë³€"], "weaknesses": ["ìƒì„¸ ë¶„ì„ ë¶ˆê°€"]
            }

        tech = result.get("technical_score", 0)
        comm = result.get("communication_score", 0)
        cult = result.get("cultural_fit_score", 0)
        overall = (tech + comm + cult) / 3

        create_or_update_evaluation_report(
            interview_id,
            technical_score=tech,
            communication_score=comm,
            cultural_fit_score=cult,
            summary_text=result.get("summary_text", ""),
            details_json={
                "strengths": result.get("strengths", []),
                "weaknesses": result.get("weaknesses", [])
            }
        )
        update_interview_overall_score(interview_id, score=overall)
        logger.info(f"âœ… Final Report Generated for Interview {interview_id}")

    except Exception as e:
        logger.error(f"âŒ Error in generate_final_report: {e}")
        create_or_update_evaluation_report(
            interview_id,
            technical_score=0, summary_text=f"ì˜¤ë¥˜: {str(e)}"
        )