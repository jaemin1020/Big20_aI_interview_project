import logging
import time
import re
import json
import sys
import os
from pydantic import BaseModel, Field
from typing import List, Dict, Any
from langchain_core.output_parsers import JsonOutputParser
from celery import shared_task

# DB Helper Functions
from db import (
    engine,
    Session,
    Transcript,
    Interview,
    Company,
    Resume,
    Question,
    update_transcript_sentiment,
    update_transcript_scores,
    update_question_avg_score,
    get_interview_transcripts,
    get_user_answers
)
from sqlmodel import select

# 9~14ë²ˆ ìŠ¤í…Œì´ì§€: ì¸ì¬ìƒ(ideal) ì°¸ê³ ê°€ í•„ìš”í•œ stage ëª©ë¡
# interview_scenario_transition.pyì˜ order 9~14ì— í•´ë‹¹
COMPANY_IDEAL_STAGES = {
    "communication",          # 9. í˜‘ì—…/ì†Œí†µ ì§ˆë¬¸
    "communication_followup", # 10. í˜‘ì—… ì‹¬ì¸µ
    "responsibility",         # 11. ê°€ì¹˜ê´€/ì±…ì„ê° ì§ˆë¬¸
    "responsibility_followup",# 12. ê°€ì¹˜ê´€ ì‹¬ì¸µ
    "growth",                 # 13. ì„±ì¥ê°€ëŠ¥ì„± ì§ˆë¬¸
    "growth_followup",        # 14. ì„±ì¥ê°€ëŠ¥ì„± ì‹¬ì¸µ
}

# AI-Worker ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
current_file_path = os.path.abspath(__file__) # tasks/evaluator.py
tasks_dir = os.path.dirname(current_file_path) # tasks/
ai_worker_root = os.path.dirname(tasks_dir)    # ai-worker/

# backend-core ë° í•˜ìœ„ utils ê²½ë¡œ ì¶”ê°€ (íŒ¨í‚¤ì§€ ì¶©ëŒ ë°©ì§€)
backend_core_path = os.path.abspath(os.path.join(ai_worker_root, "..", "backend-core"))
backend_core_utils = os.path.join(backend_core_path, "utils")
for p in [backend_core_path, backend_core_utils]:
    if p not in sys.path:
        sys.path.insert(0, p)

logger = logging.getLogger("AI-Worker-Evaluator")

# utils.exaone_llmì€ ì‹¤ì œ ì‚¬ìš© ì‹œì ì— ì„í¬íŠ¸ (ì›Œì»¤ ì‹œì‘ ì‹œ í¬ë˜ì‹œ ë°©ì§€)
try:
    from utils.exaone_llm import get_exaone_llm
    # rubric_generatorëŠ” backend-core/utilsì— ìˆìŒ. 
    # ai-worker/utilsì™€ íŒ¨í‚¤ì§€ëª…(utils)ì´ ì¤‘ë³µë˜ë¯€ë¡œ ì§ì ‘ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œë„
    import rubric_generator
    create_evaluation_rubric = rubric_generator.create_evaluation_rubric
    logger.info("âœ… Successfully linked rubric_generator from backend-core")
except ImportError as e:
    logger.warning(f"Could not import from backend-core utils: {e}. Falling back to basics.")

def get_rubric_for_stage(stage_name: str) -> dict:
    """ìŠ¤í…Œì´ì§€ ì´ë¦„ì— ë§ëŠ” ë£¨ë¸Œë¦­ ì˜ì—­ ë°˜í™˜"""
    try:
        full_rubric = create_evaluation_rubric()
        for area in full_rubric["evaluation_areas"]:
            if stage_name in area["target_stages"]:
                logger.info(f"âœ… Found matching Rubric Area: {area['name']} for stage: {stage_name}")
                return area
    except Exception as e:
        logger.error(f"Error mapping rubric for stage {stage_name}: {e}")
    return None

# -----------------------------------------------------------
# [Schema] í‰ê°€ ë°ì´í„° êµ¬ì¡° ì •ì˜ (Pydantic)
# -----------------------------------------------------------
class AnswerEvalSchema(BaseModel):
    total_score: int = Field(description="ë£¨ë¸Œë¦­ ì„¸ë¶€ í•­ëª© ì ìˆ˜ë“¤ì˜ í•©ê³„ (0-100)")
    rubric_scores: Dict[str, int] = Field(description="ë£¨ë¸Œë¦­ ì„¸ë¶€ í•­ëª©ë³„ ì ìˆ˜ (ì˜ˆ: {'ë…¼ë¦¬ì  êµ¬ì¡°': 35, 'í•µì‹¬ ì „ë‹¬ë ¥': 30, ...})")
    feedback: str = Field(description="ë‹µë³€ì— ëŒ€í•œ êµ¬ì²´ì ì´ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°± (ë§ˆí¬ë‹¤ìš´ ì—†ì´ í‰ë¬¸ìœ¼ë¡œ ì‘ì„±)")

class FinalReportSchema(BaseModel):
    overall_score: int = Field(description="ì „ì²´ í‰ê·  ì ìˆ˜ (0-100)")
    technical_score: int = Field(description="ê¸°ìˆ  ì´í•´ë„ (0-100)")
    experience_score: int = Field(description="ì§ë¬´ ê²½í—˜ (0-100)")
    problem_solving_score: int = Field(description="ë¬¸ì œ í•´ê²° (0-100)")
    communication_score: int = Field(description="ì˜ì‚¬ì†Œí†µ (0-100)")
    responsibility_score: int = Field(description="ì±…ì„ê° (0-100)")
    growth_score: int = Field(description="ì„±ì¥ ì˜ì§€ (0-100)")
    
    technical_feedback: str = Field(description="ê¸°ìˆ  ì›ë¦¬ ìˆ˜ì¤€, ì„ íƒ ê·¼ê±°ì˜ íƒ€ë‹¹ì„±, ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„±ì— ëŒ€í•œ êµ¬ì²´ì  ë¶„ì„ (3ë¬¸ì¥ ì´ìƒ)")
    experience_feedback: str = Field(description="í”„ë¡œì íŠ¸ ê²½í—˜ì˜ êµ¬ì²´ì„±, ë³¸ì¸ì˜ ê¸°ì—¬ë„, ì‹¤ë¬´ ì—°ê³„ì„±ì— ëŒ€í•œ ìƒì„¸ í‰ê°€ (3ë¬¸ì¥ ì´ìƒ)")
    problem_solving_feedback: str = Field(description="STAR ê¸°ë²• ê¸°ë°˜ì˜ ë¬¸ì œ ì •ì˜, ì ‘ê·¼ ë°©ì‹, í•´ê²° ê²°ê³¼ ë° êµí›ˆì— ëŒ€í•œ ë¶„ì„ (3ë¬¸ì¥ ì´ìƒ)")
    communication_feedback: str = Field(description="ì „ë¬¸ ìš©ì–´ ì‚¬ìš©ì˜ ì ì ˆì„±, ë©”ì‹œì§€ ì „ë‹¬ë ¥, ê²½ì²­ ë° ë‹µë³€ íƒœë„ í‰ê°€ (3ë¬¸ì¥ ì´ìƒ)")
    responsibility_feedback: str = Field(description="ì§€ì›ìì˜ ì§ì—… ìœ¤ë¦¬, ì±…ì„ê°, ê°€ì¹˜ê´€ì˜ ì¼ê´€ì„± ë° ê¸°ì—… ì¸ì¬ìƒ ë¶€í•© ì—¬ë¶€ ë¶„ì„ (3ë¬¸ì¥ ì´ìƒ)")
    growth_feedback: str = Field(description="ìê¸°ê³„ë°œ ì˜ì§€, ì‹ ê¸°ìˆ  ìŠµë™ ì†ë„, í–¥í›„ ë°œì „ ê°€ëŠ¥ì„± ë° ì‹œë‹ˆì–´ì˜ ì œì–¸ (3ë¬¸ì¥ ì´ìƒ)")

    strengths: List[str] = Field(
        description="ì§€ì›ìì˜ ì£¼ìš” ê°•ì  2-3ê°€ì§€. ê° í•­ëª©ì€ ë©´ì ‘ ë‹µë³€ì—ì„œ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì¸ìš©í•˜ì—¬ 2ë¬¸ì¥ ì´ìƒì˜ ì™„ê²°ëœ ì„œìˆ í˜• ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ì˜ˆ: 'í”„ë¡œì íŠ¸ì—ì„œ RAG ë„ì…ì˜ íƒ€ë‹¹ì„±ì„ ì‹¤í—˜ ë°ì´í„°ë¡œ ì§ì ‘ ê²€ì¦í•œ ì ì€ ê¸°ìˆ ë ¥ê³¼ ë¶„ì„ ëŠ¥ë ¥ì„ ë™ì‹œì— ë³´ì—¬ì¤ë‹ˆë‹¤. íŠ¹íˆ í‚¤ì›Œë“œ ê²€ìƒ‰ ëŒ€ë¹„ ë²¡í„° ê²€ìƒ‰ì˜ hit rateë¥¼ ìˆ˜ì¹˜ë¡œ ë¹„êµí•œ ì ‘ê·¼ ë°©ì‹ì€ ì‹¤ë¬´ ì—­ëŸ‰ì„ ì¦ëª…í•©ë‹ˆë‹¤.'"
    )
    improvements: List[str] = Field(
        description="ë³´ì™„ì´ í•„ìš”í•œ ì•½ì  ë° ê°œì„ ì  2-3ê°€ì§€. ê° í•­ëª©ì€ ë©´ì ‘ ì¤‘ ë“œëŸ¬ë‚œ êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ì¸ìš©í•˜ì—¬ 2ë¬¸ì¥ ì´ìƒì˜ ì™„ê²°ëœ ì„œìˆ í˜• ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ë‹¨ìˆœ í‚¤ì›Œë“œë‚˜ ë‚˜ì—´ì‹ í‘œí˜„ì€ ê¸ˆì§€í•©ë‹ˆë‹¤."
    )
    summary_text: str = Field(description="ì„±ì¥ì„ ìœ„í•œ ì‹œë‹ˆì–´ ìœ„ì›ì¥ì˜ ìµœì¢… í•œë§ˆë”” (3ë¬¸ì¥ ë‚´ì™¸)")

def _analyze_answer_logic(transcript_id: int, question_text: str, answer_text: str, rubric: dict = None, question_id: int = None, question_type: str = None):
    """ê°œë³„ ë‹µë³€ í‰ê°€ í•µì‹¬ ë¡œì§ (DB ì—…ë°ì´íŠ¸ í¬í•¨)"""
    
    start_ts = time.time()
    
    try:
        # ì§ˆë¬¸ ì •ë³´ ì¡°íšŒ (Stage í™•ì¸ìš©)
        stage_name = "unknown"
        if question_id:
            with Session(engine) as session:
                question = session.get(Question, question_id)
                if question:
                    stage_name = question.question_type or "unknown"
        
        # question_typeì´ ì§ì ‘ ë„˜ì–´ì˜¨ ê²½ìš° ìš°ì„  ìˆœìœ„ ë¶€ì—¬
        if question_type and question_type != "unknown":
            stage_name = question_type

        # logger.info(f"ğŸ” Analyzing Answer: Stage={stage_name}, QuestionID={question_id}")

        # [í•µì‹¬] 100ì  ë§Œì  ìƒì„¸ ë£¨ë¸Œë¦­ ìš°ì„  ì ìš©
        real_rubric = get_rubric_for_stage(stage_name)
        if real_rubric:
            rubric = real_rubric
            # logger.info(f"ğŸ“Š Using REAL Detailed Rubric for {stage_name}")
        elif not rubric or "guide" in rubric:
            # logger.warning(f"âš ï¸ No matching detailed rubric found for stage: {stage_name}. Using fallback.")
            if not rubric:
                rubric = {
                    "name": "ì¼ë°˜ í‰ê°€",
                    "detailed_scoring": {"ì „ë°˜ì  ë‹µë³€ í’ˆì§ˆ": 100},
                    "scoring_guide": {"excellent": {"range": [85, 100]}}
                }
        
        # LangChain Parser ì„¤ì •
        parser = JsonOutputParser(pydantic_object=AnswerEvalSchema)
        
        # ì—”ì§„ ê°€ì ¸ì˜¤ê¸°
        llm_engine = get_exaone_llm()
        
        # ì¸ì¬ìƒ(ideal) ì¡°íšŒ
        company_ideal_section = ""
        if question_type in COMPANY_IDEAL_STAGES:
            try:
                with Session(engine) as session:
                    transcript_obj = session.get(Transcript, transcript_id)
                    if transcript_obj:
                        interview_obj = session.get(Interview, transcript_obj.interview_id)
                        if interview_obj:
                            company_obj = None
                            if interview_obj.company_id:
                                company_obj = session.get(Company, interview_obj.company_id)
                            
                            if company_obj and company_obj.ideal:
                                company_ideal_section = f"\n\n[íšŒì‚¬ ì¸ì¬ìƒ ì°¸ê³ ]\nì§€ì› íšŒì‚¬: {company_obj.company_name}\nì¸ì¬ìƒ: {company_obj.ideal}\nâ€» ìœ„ ì¸ì¬ìƒê³¼ì˜ ë¶€í•© ì—¬ë¶€ë¥¼ í‰ê°€ ì‹œ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì‹­ì‹œì˜¤."
                                # logger.info(f"âœ… [{question_type}] ì¸ì¬ìƒ ë¡œë“œ - {company_obj.company_name}")
            except Exception as ideal_err:
                logger.warning(f"âš ï¸ ì¸ì¬ìƒ ì¡°íšŒ ì‹¤íŒ¨: {ideal_err}")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_msg = """[|system|]ê·€í•˜ëŠ” ê¸°ìˆ ë ¥, ì†Œí†µ ëŠ¥ë ¥, ì¡°ì§ ì í•©ì„±ì„ ì •ë°€ ê²€ì¦í•˜ëŠ” 'AI ì±„ìš© í‰ê°€ ìœ„ì›íšŒ'ì˜ ì „ë¬¸ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.
LG AI Researchê°€ ê°œë°œí•œ EXAONEìœ¼ë¡œì„œ, ì œê³µëœ ë£¨ë¸Œë¦­ì„ ì ˆëŒ€ì  ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì•„ ì§€ì›ìì˜ ë‹µë³€ì„ ëƒ‰ì² í•˜ê²Œ ë¶„ì„í•˜ê³  ìˆ˜ì¹˜í™”ëœ ì ìˆ˜ì™€ ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì‚°ì¶œí•˜ì‹­ì‹œì˜¤.

[í‰ê°€ ê°€ì´ë“œë¼ì¸]
1. **ê¸°ìˆ ì  ì—„ë°€ì„±**: ë‹µë³€ì— í¬í•¨ëœ ê¸°ìˆ  ê°œë…ì˜ ì •í™•ì„±ê³¼ ì„ íƒ ê·¼ê±°ì˜ íƒ€ë‹¹ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ ê²€í† í•˜ì‹­ì‹œì˜¤.
2. **ì¦ê±° ì¤‘ì‹¬ í”¼ë“œë°±**: ì§€ì›ìì˜ ë‹µë³€ ì¤‘ ì–´ë–¤ í‘œí˜„ì´ë‚˜ ì‚¬ë¡€ê°€ ë£¨ë¸Œë¦­ ì§€í‘œì— ë¶€í•©í–ˆëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ì‹­ì‹œì˜¤.
3. **ìˆ˜ì¹˜í™”**: ë£¨ë¸Œë¦­ ì ìˆ˜ë¥¼ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ë˜, ë‹µë³€ì´ ëª¨í˜¸í•œ ê²½ìš° ë³´ìˆ˜ì ìœ¼ë¡œ í‰ê°€í•˜ì‹­ì‹œì˜¤.
4. **ì¸ì¬ìƒ ë°˜ì˜**: ì¸ì¬ìƒ ì •ë³´ê°€ ì œê³µëœ ê²½ìš° ë¶„ì„ ê²°ê³¼ì— ë°˜ë“œì‹œ í¬í•¨í•˜ì‹­ì‹œì˜¤.
5. **í…ìŠ¤íŠ¸ ì •ì œ (No Markdown)**: ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì˜¤ì§ ìˆœìˆ˜í•œ í‰ë¬¸(Plain Text)ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.[|endofturn|]"""

        user_msg = f"""[|user|]ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì›ìì˜ ë‹µë³€ì„ ë£¨ë¸Œë¦­ ê¸°ì¤€ì— ë§ì¶° ì •ë°€ í‰ê°€í•˜ì‹­ì‹œì˜¤.
        
[ì§ˆë¬¸]
{question_text}

[ë‹µë³€]
{answer_text}

[í‰ê°€ ë£¨ë¸Œë¦­]
{json.dumps(rubric, ensure_ascii=False) if rubric else "í‘œì¤€ ë©´ì ‘ í‰ê°€ ê¸°ì¤€"}{company_ideal_section}

{parser.get_format_instructions()}[|endofturn|]"""
        
        prompt = f"{system_msg}\n{user_msg}\n[|assistant|]"
        raw_output = llm_engine.invoke(prompt, temperature=0.2)
        
        try:
            result = parser.parse(raw_output)
        except:
            json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
            else:
                result = {"total_score": 70, "rubric_scores": {}, "feedback": "í‰ê°€ ë°ì´í„°ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        tech_score = int(result.get("total_score", 70))
        rubric_scores = result.get("rubric_scores", {})
        
        db_rubric_data = {
            "í‰ê°€ì˜ì—­": rubric.get("name", "ì¼ë°˜ í‰ê°€") if rubric else "ì¼ë°˜ í‰ê°€",
            "ì„¸ë¶€í•­ëª©ì ìˆ˜": rubric_scores,
            "í•­ëª©ë³„ë°°ì ": rubric.get("detailed_scoring", {}) if rubric else {}
        }

        sentiment = (tech_score / 100.0) - 0.5 
        try:
            update_transcript_sentiment(transcript_id, sentiment_score=sentiment, emotion="neutral", total_score=float(tech_score), rubric_score=db_rubric_data)
            update_transcript_scores(transcript_id, total_score=float(tech_score), rubric_score=db_rubric_data)
        except Exception as db_err:
            logger.error(f"âŒ DB ì €ì¥ ì˜¤ë¥˜ (Transcript {transcript_id}): {db_err}")
        
        if question_id:
            update_question_avg_score(question_id, tech_score)

        return result
    except Exception as e:
        logger.error(f"Evaluation Logic Failed: {e}")
        return {"error": str(e)}

@shared_task(name="tasks.evaluator.analyze_answer")
def analyze_answer(transcript_id: int, question_text: str, answer_text: str, rubric: dict = None, question_id: int = None, question_type: str = None):
    """ê°œë³„ ë‹µë³€ í‰ê°€ (ìƒìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
    return _analyze_answer_logic(transcript_id, question_text, answer_text, rubric, question_id, question_type)

@shared_task(name="tasks.evaluator.generate_final_report")
def generate_final_report(interview_id: int):
    """
    ìµœì¢… í‰ê°€ ë³´ê³ ì„œ ìƒì„± (ì‹œë‹ˆì–´ ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ ì ìš©)
    """
    logger.info(f"Generating Final Report for Interview {interview_id}")
    from db import (
        Interview, 
        create_or_update_evaluation_report, 
        update_interview_overall_score, 
        get_interview_transcripts
    )
    
    try:
        # [ì¶”ê°€] 1. ê°œë³„ ë‹µë³€ ì„ ì œì  í‰ê°€ (í‰ê°€ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° í•œêº¼ë²ˆì— ì²˜ë¦¬)
        transcripts = get_interview_transcripts(interview_id)
        user_transcripts = [t for t in transcripts if str(t.speaker).lower() in ('user', 'speaker.user')]
        logger.info(f"ğŸ§ Evaluating {len(user_transcripts)} individual answers before final report...")
        
        for t in user_transcripts:
            # ì´ë¯¸ ì ìˆ˜ê°€ ìˆëŠ” ê²½ìš°ëŠ” ìŠ¤í‚µ (í˜¹ì‹œë‚˜ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ìš©)
            if t.sentiment_score is not None and t.sentiment_score != 0.0:
                continue
            
            try:
                # ì§ˆë¬¸ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ë£¨ë¸Œë¦­ê³¼ í•¨ê»˜ í‰ê°€
                with Session(engine) as session:
                    q = session.get(Question, t.question_id) if t.question_id else None
                    q_content = q.content if q else "ê´€ë ¨ ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    q_rubric = q.rubric_json if q else None
                    q_type = q.question_type if q else "unknown"
                
                logger.info(f"   - Evaluating Transcript {t.id} (Stage: {q_type})")
                _analyze_answer_logic(
                    transcript_id=t.id,
                    question_text=q_content,
                    answer_text=t.text,
                    rubric=q_rubric,
                    question_id=t.question_id,
                    question_type=q_type
                )
            except Exception as e:
                logger.error(f"   - Failed to evaluate transcript {t.id}: {e}")

        # ê°œë³„ í‰ê°€ í›„ ë°ì´í„° ìµœì‹ í™”ë¥¼ ìœ„í•´ transcripts ë‹¤ì‹œ ì¡°íšŒ
        transcripts = get_interview_transcripts(interview_id)
        
        # ğŸ§¹ ë©”ëª¨ë¦¬ ì²­ì†Œ (ë¦¬í¬íŠ¸ ë¶„ì„ ì „ ê³µê°„ í™•ë³´)
        import gc
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # ì¸í„°ë·° ì •ë³´ ë° íšŒì‚¬ ì¸ì¬ìƒ ê°€ì ¸ì˜¤ê¸°
        with Session(engine) as session:
            interview = session.get(Interview, interview_id)
            position = interview.position if interview else "ì§€ì› ì§ë¬´"
            company_name = "í•´ë‹¹ ê¸°ì—…"
            company_ideal = "ê¸°ë³¸ ì¸ì¬ìƒ: ì„±ì‹¤, í˜‘ì—…, ë„ì „"

            # 1. ì¸ì¬ìƒ(ideal) ì •ë³´ ë¡œë“œ
            if interview:
                from db import Company, Resume
                # â‘  Interviewì— ì§ì ‘ ì—°ê²°ëœ íšŒì‚¬ í™•ì¸
                company_obj = None
                if interview.company_id:
                    company_obj = session.get(Company, interview.company_id)
                
                # â‘¡ ì´ë ¥ì„œì˜ target_companyë¥¼ í†µí•œ ê²€ìƒ‰ (fallback)
                if not company_obj and interview.resume_id:
                    resume_obj = session.get(Resume, interview.resume_id)
                    if resume_obj and resume_obj.structured_data:
                        target_co = resume_obj.structured_data.get("header", {}).get("target_company", "")
                        if target_co:
                            from sqlmodel import select as sql_select
                            norm_name = target_co.replace(" ", "").lower()
                            all_cos = session.exec(sql_select(Company)).all()
                            company_obj = next((c for c in all_cos if c.company_name and c.company_name.replace(" ", "").lower() == norm_name), None)

                if company_obj:
                    company_name = company_obj.company_name
                    company_ideal = company_obj.ideal or company_ideal
                    logger.info(f"âœ… ë¦¬í¬íŠ¸ ìƒì„±ìš© ì¸ì¬ìƒ ë¡œë“œ ì™„ë£Œ: {company_name}")

        if not transcripts:
            logger.warning("ì´ ì¸í„°ë·°ì— ëŒ€í•œ ëŒ€í™” ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            create_or_update_evaluation_report(
                interview_id,
                technical_score=0, communication_score=0, cultural_fit_score=0,
                summary_text="ê¸°ë¡ëœ ëŒ€í™”ê°€ ì—†ì–´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                details_json={"error": "no_data"}
            )
            return

        conversation = "\n".join([f"{t.speaker}: {t.text}" for t in transcripts])
        if len(conversation) > 12000: # ëŒ€ëµ 8000 í† í° ë‚´ì™¸ë¡œ ìë¦„ (ì•ˆì „ ê³„ìˆ˜)
            logger.info(f"âš ï¸ Conversation too long ({len(conversation)} chars). Truncating: front 3000 + tail 8000.")
            # ì¤‘í›„ë°˜ë¶€(ê²½í—˜/ë¬¸ì œí•´ê²°/í˜‘ì—…/ê°€ì¹˜ê´€/ì„±ì¥ Q&A)ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ê¸° ìœ„í•´
            # ì• 3000ì(ì†Œê°œ/ë„ì…)ë³´ë‹¤ ë§ˆì§€ë§‰ 8000ì(í•µì‹¬ ì—­ëŸ‰) ìœ„ì£¼ë¡œ ìœ ì§€
            conversation = conversation[:3000] + "\n... (ì¤‘ëµ - ë„ì…ë¶€ ìƒëµ) ...\n" + conversation[-8000:]

        try:
            # LangChain Parser ì„¤ì •
            parser = JsonOutputParser(pydantic_object=FinalReportSchema)
            
            # [í•µì‹¬] ì „ì²´ í‰ê°€ ë£¨ë¸Œë¦­ ê°€ì ¸ì˜¤ê¸°
            full_rubric = {}
            try:
                full_rubric = create_evaluation_rubric()
                logger.info("ğŸ“‹ Full Rubric loaded for Final Report")
            except Exception as re_err:
                logger.error(f"Failed to load full rubric: {re_err}")

            logger.info(f"ğŸ¤– Starting [FINAL REPORT] LLM analysis for Interview {interview_id}...")
            exaone = get_exaone_llm()
            system_msg = f"""[|system|]ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ê¸°ìˆ  ê¸°ì—…ì—ì„œ ìˆ˜ë§Œ ëª…ì˜ ì¸ì¬ë¥¼ ë°œêµ´í•´ì˜¨ '{position}' ë¶„ì•¼ ì „ë¬¸ ì±„ìš© ìœ„ì›ì¥ì…ë‹ˆë‹¤.
LG AI Researchì˜ EXAONEìœ¼ë¡œì„œ, ë©´ì ‘ ì „ì²´ ë°œí™” ë¡œê·¸ì™€ [í‘œì¤€ í‰ê°€ ë£¨ë¸Œë¦­], ê·¸ë¦¬ê³  [ê¸°ì—… ì¸ì¬ìƒ]ì„ ì¢…í•© ë¶„ì„í•˜ì—¬ ì§€ì›ìì˜ ìµœì¢… í•©ê²© ì—¬ë¶€ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ì‹¬ì¸µ ê¸°ìˆ  ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

[í•µì‹¬ í‰ê°€ í”„ë¡œí† ì½œ]
1. **ì—­ëŸ‰ë³„ ë§¤ì¹­ ë¶„ì„**: ë£¨ë¸Œë¦­ì˜ í‰ê°€ ì§€í‘œì™€ ì§€ì›ìì˜ ë‹µë³€ì„ ëŒ€ì¡°í•˜ì—¬, ë‹¨ìˆœíˆ ì˜í–ˆë‹¤ëŠ” í‘œí˜„ì´ ì•„ë‹Œ 'ê·¼ê±° ì¤‘ì‹¬'ì˜ ì„±ì í‘œë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
2. **ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦**: ì¸í„°ë·° ì „ë°˜ì— ê±¸ì³ ì§€ì›ìì˜ ë‹µë³€ì´ ì¼ê´€ëœ ì‹¤ë¬´ ì² í•™ê³¼ ê¸°ìˆ ì  ì›ì¹™ì„ ìœ ì§€í•˜ê³  ìˆëŠ”ì§€ ì²´í¬í•˜ì‹­ì‹œì˜¤.
3. **ì¸ì¬ìƒ ì •ë°€ ê²€ì¦**: ì œê³µëœ [ê¸°ì—… ì¸ì¬ìƒ] í‚¤ì›Œë“œì™€ ì§€ì›ìì˜ ê°€ì¹˜ê´€/ì±…ì„ê° ë‹µë³€(stage 11~14)ì„ ë¹„êµ ë¶„ì„í•˜ì—¬ {company_name}ì— ì í•©í•œ ì¸ì¬ì¸ì§€ 'responsibility_feedback'ê³¼ 'growth_feedback'ì— ìƒì„¸íˆ ì„œìˆ í•˜ì‹­ì‹œì˜¤.
4. **STAR ê¸°ë²• ê¸°ë°˜ ê²€ì¦**: ì§€ì›ìê°€ ì„±ê³¼ë¥¼ ì„¤ëª…í•  ë•Œ ìƒí™©(S)-ê³¼ì—…(T)-í–‰ë™(A)-ê²°ê³¼(R) êµ¬ì¡°ë¥¼ ê°–ì¶”ì–´ ì‹¤ì§ˆì ì¸ ê¸°ì—¬ë„ë¥¼ ì¦ëª…í–ˆëŠ”ì§€ í‰ê°€í•˜ì‹­ì‹œì˜¤.
5. **ì‹œë‹ˆì–´ì˜ ì œì–¸**: ê°•ì ì€ ê·¹ëŒ€í™”í•˜ê³  ì•½ì ì€ ì‹¤ì²œ ê°€ëŠ¥í•œ ì„±ì¥ì˜ ê¸°íšŒë¡œ ì „í™˜í•  ìˆ˜ ìˆë„ë¡ ì‹œë‹ˆì–´ ì „ë¬¸ê°€ì˜ ê¹Šì´ ìˆëŠ” ì¡°ì–¸(Summary)ì„ ì œê³µí•˜ì‹­ì‹œì˜¤.
6. **í…ìŠ¤íŠ¸ ì •ì œ (No Markdown)**: ì „ ì˜ì—­ í”¼ë“œë°± ë° ê°•ì /ë³´ì™„ì  ì‘ì„± ì‹œ ë³¼íŠ¸(**), ì´íƒ¤ë¦­(*), ë¦¬ìŠ¤íŠ¸(-) ë“±ì˜ ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì¼ì ˆ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì˜¤ì§ ìˆœìˆ˜í•œ í‰ë¬¸(Plain Text)ìœ¼ë¡œë§Œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.
7. **ì–¸ì–´ ì¼ê´€ì„± ë° ì¤‘ë³µ ë°©ì§€**: ì „ë¬¸ì ì´ê³  ì •ì œëœ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë˜, ë™ì¼í•œ ë¬¸êµ¬ë‚˜ ìˆ˜ì‹ì–´ê°€ ë°˜ë³µë˜ì–´ ê°€ë…ì„±ì„ í•´ì¹˜ëŠ” í˜„ìƒì„ ë°©ì§€í•˜ì‹­ì‹œì˜¤.[|endofturn|]"""

            user_msg = f"""[|user|]ë‹¤ìŒ ë©´ì ‘ ëŒ€í™” ì „ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.
            
[ë©´ì ‘ ëŒ€í™” ì „ë¬¸]
{conversation}

[í‘œì¤€ í‰ê°€ ë£¨ë¸Œë¦­]
{json.dumps(full_rubric, ensure_ascii=False, indent=2)}

[ê¸°ì—… ì¸ì¬ìƒ]
íšŒì‚¬ëª…: {company_name}
ì¸ì¬ìƒ: {company_ideal}

[ì¶œë ¥ ì œì•½ ì‚¬í•­]
- ëª¨ë“  ë¶„ì„ì€ ë°˜ë“œì‹œ ë£¨ë¸Œë¦­ì˜ ì„¸ë¶€ ëª©í‘œì™€ ì—°ë™ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- íŠ¹íˆ 'responsibility_feedback'ì€ ì§€ì›ìì˜ ì§ì—… ìœ¤ë¦¬ì™€ ì±…ì„ê°ì„, 'growth_feedback'ì€ ë°œì „ ê°€ëŠ¥ì„±ì„ ë¶„ë¦¬í•˜ì—¬ ìƒì„¸íˆ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
- **í•„ìˆ˜ ì‚¬í•­**: ë§Œì•½ ê°€ì¹˜ê´€/ì±…ì„ê°/ì„±ì¥ ê´€ë ¨ íŠ¹ì • ìŠ¤í…Œì´ì§€(11~14) ë°œí™”ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ë©´ì ‘ì´ ì¤‘ë„ ì¢…ë£Œë˜ì—ˆë‹¤ í•˜ë”ë¼ë„, ë©´ì ‘ ì „ì²´ ë°œí™”ì—ì„œ ë“œëŸ¬ë‚œ íƒœë„ì™€ ì¸ì¬ìƒì˜ ì •ë ¬ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ë¡ í•˜ì—¬ ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ë¶„ì„ ë‚´ìš©ì„ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ê²°ì½” ê³µë€ì´ë‚˜ ë””í´íŠ¸ ë¬¸êµ¬ë¡œ ë‚¨ê¸°ì§€ ë§ˆì‹­ì‹œì˜¤.
- strengthsì™€ improvements í•­ëª©ì€ ë©´ì ‘ ì¤‘ íŠ¹ì • ë°œí™”ë¥¼ ê·¼ê±°ë¡œ ì¸ìš©í•˜ì—¬ 2ë¬¸ì¥ ì´ìƒì˜ ì„œìˆ í˜•ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
- ê²°ê³¼ë¬¼ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON í¬ë§·ë§Œ ì¶œë ¥í•˜ë©°, ì‚¬ì¡±ì„ ë¶™ì´ì§€ ë§ˆì‹­ì‹œì˜¤.

{parser.get_format_instructions()}[|endofturn|]"""
            
            # ìƒì„± ë° íŒŒì‹± (EXAONE ì „ìš© í¬ë§· ì‚¬ìš©)
            prompt = f"{system_msg}\n{user_msg}\n[|assistant|]"
            # ë¦¬í¬íŠ¸ëŠ” ë‚´ìš©ì´ ê¸¸ë¯€ë¡œ max_tokensë¥¼ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
            raw_output = exaone.invoke(prompt, temperature=0.3, max_tokens=3000)
            
            if not raw_output:
                raise ValueError("LLM generated empty output (possibly context limit reached)")

            try:
                result = parser.parse(raw_output)
            except Exception as parse_err:
                logger.error(f"ìµœì¢… ë¦¬í¬íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {parse_err}")
                json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    raise parse_err
                
        except Exception as llm_err:
            logger.error(f"LLM Summary failed: {llm_err}")
            # ê°œë³„ ë‹µë³€ë“¤ì˜ ì ìˆ˜ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒë“¤ì˜ í‰ê· ìœ¼ë¡œ í´ë°±
            try:
                user_transcripts = [t for t in transcripts if str(t.speaker).lower() in ('user', 'speaker.user')]
                valid_scores = []
                for t in user_transcripts:
                    try:
                        s = float(t.sentiment_score) if t.sentiment_score is not None else 0.0
                        valid_scores.append(s + 0.5)
                    except:
                        valid_scores.append(0.5) # ê¸°ë³¸ ì ìˆ˜
                
                avg_tech = (sum(valid_scores) / len(valid_scores)) * 100 if valid_scores else 70
            except:
                avg_tech = 70
            
            result = {
                "overall_score": int(avg_tech),
                "technical_score": int(avg_tech), 
                "experience_score": int(avg_tech), "problem_solving_score": int(avg_tech),
                "communication_score": int(avg_tech), "responsibility_score": int(avg_tech), "growth_score": int(avg_tech),
                "summary_text": "ë©´ì ‘ ë°ì´í„° ë¶„ì„ ì¤‘ ì¼ì‹œì ì¸ ì§€ì—°ì´ ë°œìƒí•˜ì—¬ ì¢…í•© ì ìˆ˜ ìœ„ì£¼ë¡œ ì‚°ì¶œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ ë¶„ì„ì€ ë‹µë³€ì˜ í’ˆì§ˆì„ ê¸°ë°˜ìœ¼ë¡œ ìš”ì•½ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "technical_feedback": "ê¸°ìˆ ì  í•µì‹¬ ì›ë¦¬ì— ëŒ€í•œ ì´í•´ë„ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "experience_feedback": "í”„ë¡œì íŠ¸ ê²½í—˜ì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "problem_solving_feedback": "ë…¼ë¦¬ì ì¸ ë¬¸ì œ í•´ê²° ê³¼ì •ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "communication_feedback": "ì „ë°˜ì ì¸ ì˜ì‚¬ì†Œí†µ ëŠ¥ë ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.",
                "responsibility_feedback": "ì§ë¬´ì— ì„í•˜ëŠ” íƒœë„ê°€ ì•ˆì •ì ì…ë‹ˆë‹¤.",
                "growth_feedback": "ì§€ì†ì ì¸ ì„±ì¥ ê°€ëŠ¥ì„±ì´ ì—¿ë³´ì…ë‹ˆë‹¤.",
                "strengths": ["ì„±ì‹¤í•œ ë‹µë³€ ì°¸ì—¬"], "improvements": ["ìƒì„¸ í”¼ë“œë°± ê¸°ìˆ  ì§€ì› í•„ìš”"]
            }

        try:
            def ensure_int(val, default=0):
                try:
                    if val is None: return default
                    return int(float(str(val)))
                except:
                    return default

            # DB ì €ì¥ì„ ìœ„í•´ ì ìˆ˜ ì¶”ì¶œ (ì•ˆì „í•œ ë³€í™˜)
            tech = ensure_int(result.get("technical_score"), 0)
            comm = ensure_int(result.get("communication_score"), 0)
            # cultural_fitì€ responsibilityì™€ growthì˜ í‰ê· ìœ¼ë¡œ ì„ì‹œ ê³„ì‚° (DB ì»¬ëŸ¼ í˜¸í™˜ì„±)
            resp = ensure_int(result.get("responsibility_score"), 0)
            grow = ensure_int(result.get("growth_score"), 0)
            cult = int((resp + grow) / 2)
            overall = ensure_int(result.get("overall_score"), int((tech + comm + cult) / 3))

            # ëª¨ë“  ìƒì„¸ í•„ë“œë¥¼ details_jsonì— ì €ì¥ (í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™)
            details = {
                "experience_score": ensure_int(result.get("experience_score"), 0),
                "problem_solving_score": ensure_int(result.get("problem_solving_score"), 0),
                "responsibility_score": resp,
                "growth_score": grow,
                "technical_feedback": result.get("technical_feedback", ""),
                "experience_feedback": result.get("experience_feedback", ""),
                "problem_solving_feedback": result.get("problem_solving_feedback", ""),
                "communication_feedback": result.get("communication_feedback", ""),
                "responsibility_feedback": result.get("responsibility_feedback", ""),
                "growth_feedback": result.get("growth_feedback", ""),
                "strengths": result.get("strengths", []),
                "improvements": result.get("improvements", [])
            }

            create_or_update_evaluation_report(
                interview_id,
                technical_score=tech,
                communication_score=comm,
                cultural_fit_score=cult,
                summary_text=result.get("summary_text", ""),
                details_json=details
            )
            update_interview_overall_score(interview_id, score=overall)
            logger.info(f"âœ… ì¸í„°ë·° {interview_id}ì— ëŒ€í•œ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")

        except Exception as save_err:
            logger.error(f"Failed to process/save report results: {save_err}")
            raise save_err

    except Exception as e:
        logger.error(f"âŒ Error in generate_final_report: {e}")
        create_or_update_evaluation_report(
            interview_id,
            technical_score=0, summary_text="ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ë°ì´í„° ì²˜ë¦¬ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ëª…ì„¸ì„œë¥¼ ë‹¤ì‹œ ì¡°íšŒí•´ ì£¼ì„¸ìš”."
        )
