import logging
import time
import re
import json
import sys
import os
from pydantic import BaseModel, Field
from typing import List
from langchain_core.output_parsers import JsonOutputParser
from celery import shared_task

# DB Helper Functions
from db import (
    engine,
    Session,
    Transcript,
    Interview,
    update_transcript_sentiment,
    update_question_avg_score,
    get_interview_transcripts,
    get_user_answers
)

# AI-Worker ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
current_file_path = os.path.abspath(__file__) # tasks/evaluator.py
tasks_dir = os.path.dirname(current_file_path) # tasks/
ai_worker_root = os.path.dirname(tasks_dir)    # ai-worker/

# backend-core ê²½ë¡œ ì¶”ê°€ (rubric_generator ì„í¬íŠ¸ë¥¼ ìœ„í•¨)
backend_core_path = os.path.abspath(os.path.join(ai_worker_root, "..", "backend-core"))
if backend_core_path not in sys.path:
    sys.path.insert(0, backend_core_path)

logger = logging.getLogger("AI-Worker-Evaluator")

# utils.exaone_llmì€ ì‹¤ì œ ì‚¬ìš© ì‹œì ì— ì„í¬íŠ¸ (ì›Œì»¤ ì‹œì‘ ì‹œ í¬ë˜ì‹œ ë°©ì§€)
try:
    from utils.exaone_llm import get_exaone_llm
    from utils.rubric_generator import create_evaluation_rubric
except ImportError:
    logger.warning("Could not import from backend-core utils. Falling back to basics.")

def get_rubric_for_stage(stage_name: str) -> dict:
    """ìŠ¤í…Œì´ì§€ ì´ë¦„ì— ë§ëŠ” ë£¨ë¸Œë¦­ ì˜ì—­ ë°˜í™˜"""
    try:
        full_rubric = create_evaluation_rubric()
        for area in full_rubric["evaluation_areas"]:
            if stage_name in area["target_stages"]:
                logger.info(f"âœ… Found matching Rubric Area: {area['name']} for stage: {stage_name}")
                return area
    except Exception as e:
        logger.error(f"Error mapping rubric for stage {stage_name}: {e}")
    return None

# -----------------------------------------------------------
# [Schema] í‰ê°€ ë°ì´í„° êµ¬ì¡° ì •ì˜ (Pydantic)
# -----------------------------------------------------------
class AnswerEvalSchema(BaseModel):
    technical_score: int = Field(description="ê¸°ìˆ ì  ì§€ì‹ ë° ìˆ™ë ¨ë„ ì ìˆ˜ (0-5)")
    communication_score: int = Field(description="ì˜ì‚¬ì†Œí†µ ë° ì „ë‹¬ ëŠ¥ë ¥ ì ìˆ˜ (0-5)")
    feedback: str = Field(description="ë‹µë³€ì— ëŒ€í•œ êµ¬ì²´ì ì´ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±")

class FinalReportSchema(BaseModel):
    overall_score: int = Field(description="ì „ì²´ í‰ê·  ì ìˆ˜ (0-100)")
    technical_score: int = Field(description="ê¸°ìˆ  ì´í•´ë„ (0-100)")
    experience_score: int = Field(description="ì§ë¬´ ê²½í—˜ (0-100)")
    problem_solving_score: int = Field(description="ë¬¸ì œ í•´ê²° (0-100)")
    communication_score: int = Field(description="ì˜ì‚¬ì†Œí†µ (0-100)")
    responsibility_score: int = Field(description="ì±…ì„ê° (0-100)")
    growth_score: int = Field(description="ì„±ì¥ ì˜ì§€ (0-100)")
    
    technical_feedback: str = Field(description="ê¸°ìˆ  ì›ë¦¬ ë° ì„ íƒ ê·¼ê±°ì— ëŒ€í•œ ë¶„ì„")
    experience_feedback: str = Field(description="ì§ë¬´ ê²½í—˜ì˜ êµ¬ì²´ì„±ê³¼ ì‹¤ë¬´ ì—°ê³„ì„±ì— ëŒ€í•œ í‰ê°€")
    problem_solving_feedback: str = Field(description="STAR ê¸°ë²•ì— ê¸°ë°˜í•œ ë…¼ë¦¬ì  ì „ê°œ ëŠ¥ë ¥ ë¶„ì„")
    communication_feedback: str = Field(description="ì „ë¬¸ì–´ ì‚¬ìš©ì˜ ì ì ˆì„± ë° ë©”ì‹œì§€ ì „ë‹¬ë ¥ í‰ê°€")
    responsibility_feedback: str = Field(description="ë‹µë³€ì˜ ì¼ê´€ì„± ë° ì—…ë¬´ì— ì„í•˜ëŠ” ì±…ì„ê° ë¶„ì„")
    growth_feedback: str = Field(description="ìê¸°ê³„ë°œ ì˜ì§€ ë° í–¥í›„ ë°œì „ ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì œì–¸")

    strengths: List[str] = Field(description="ì§€ì›ìì˜ ì£¼ìš” ê°•ì  2-3ê°€ì§€")
    improvements: List[str] = Field(description="ë³´ì™„ì´ í•„ìš”í•œ ì•½ì  ë° ê°œì„ ì  2-3ê°€ì§€")
    summary_text: str = Field(description="ì„±ì¥ì„ ìœ„í•œ ì‹œë‹ˆì–´ ìœ„ì›ì¥ì˜ ìµœì¢… í•œë§ˆë”” (3ë¬¸ì¥ ë‚´ì™¸)")

@shared_task(name="tasks.evaluator.analyze_answer")
def analyze_answer(transcript_id: int, question_text: str, answer_text: str, rubric: dict = None, question_id: int = None):
    """ê°œë³„ ë‹µë³€ í‰ê°€ ë° ì‹¤ì‹œê°„ ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± íŠ¸ë¦¬ê±°"""
    
    start_ts = time.time()
    
    try:
        # ì§ˆë¬¸ ì •ë³´ ì¡°íšŒ (Stage í™•ì¸ìš©)
        stage_name = "unknown"
        if question_id:
            with Session(engine) as session:
                question = session.get(Question, question_id)
                if question:
                    stage_name = question.question_type or "unknown"

        # [í•µì‹¬] ê¸°ì¡´ì˜ ì˜ëª»ëœ 'guide' ë£¨ë¸Œë¦­ ëŒ€ì‹ , rubric_generatorì˜ ì§„ì§œ ë£¨ë¸Œë¦­ ì‚¬ìš©
        if not rubric or "guide" in rubric:
            real_rubric = get_rubric_for_stage(stage_name)
            if real_rubric:
                rubric = real_rubric
                logger.info(f"ğŸ“Š Using REAL Rubric for {stage_name}")

        if not answer_text or not answer_text.strip():
            logger.warning(f"ëŒ€í™” ë‚´ì—­ {transcript_id}ì˜ ë‹µë³€ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. LLM í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {
                "technical_score": 0,
                "communication_score": 0,
                "feedback": "ë‹µë³€ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        # LangChain Parser ì„¤ì •
        parser = JsonOutputParser(pydantic_object=AnswerEvalSchema)
        
        # ì—”ì§„ ê°€ì ¸ì˜¤ê¸°
        llm_engine = get_exaone_llm()
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_msg = "ê·€í•˜ëŠ” ì „ë¬¸ ë©´ì ‘ê´€ì´ë©°, ì§€ì›ìì˜ ë‹µë³€ì„ ê¸°ìˆ ë ¥ê³¼ ì˜ì‚¬ì†Œí†µ ê´€ì ì—ì„œ í‰ê°€í•©ë‹ˆë‹¤."
        user_msg = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì›ìì˜ ë‹µë³€ì„ ë£¨ë¸Œë¦­ ê¸°ì¤€ì— ë§ì¶° í‰ê°€í•˜ì‹­ì‹œì˜¤.
        
[ì§ˆë¬¸]
{question_text}

[ë‹µë³€]
{answer_text}

[í‰ê°€ ë£¨ë¸Œë¦­]
{json.dumps(rubric, ensure_ascii=False) if rubric else "í‘œì¤€ ë©´ì ‘ í‰ê°€ ê¸°ì¤€"}

{parser.get_format_instructions()}"""
        
        # ìƒì„± ë° íŒŒì‹±
        prompt = llm_engine._create_prompt(system_msg, user_msg)
        raw_output = llm_engine.invoke(prompt, temperature=0.2)
        
        try:
            result = parser.parse(raw_output)
        except Exception as parse_err:
            logger.error(f"Failed to parse LLM output: {parse_err}")
            # í´ë°±: ì •ê·œí‘œí˜„ì‹ ì‹œë„ ë˜ëŠ” ê¸°ë³¸ê°’
            json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
            else:
                result = {"technical_score": 3, "communication_score": 3, "feedback": "í‰ê°€ ë°ì´í„°ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        tech_score = result.get("technical_score", 3)
        comm_score = result.get("communication_score", 3)
        sentiment = ((tech_score + comm_score) / 10.0) - 0.5 
        
        update_transcript_sentiment(
            transcript_id, 
            sentiment_score=sentiment, 
            emotion="neutral"
        )
        
        answer_quality = (tech_score + comm_score) * 10 
        
        if question_id:
            update_question_avg_score(question_id, answer_quality)

        duration = time.time() - start_ts
        logger.info(f"ë‹µë³€ í‰ê°€ ì™„ë£Œ ({duration:.2f}ì´ˆ, Stage: {stage_name})")
        return result

    except Exception as e:
        logger.error(f"Evaluation Failed: {e}")
        return {"error": str(e)}

    except Exception as e:
        logger.error(f"Evaluation Failed: {e}")
        return {"error": str(e)}

@shared_task(name="tasks.evaluator.generate_final_report")
def generate_final_report(interview_id: int):
    """
    ìµœì¢… í‰ê°€ ë³´ê³ ì„œ ìƒì„± (ì‹œë‹ˆì–´ ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ ì ìš©)
    """
    logger.info(f"Generating Final Report for Interview {interview_id}")
    from db import (
        Interview, 
        create_or_update_evaluation_report, 
        update_interview_overall_score, 
        get_interview_transcripts
    )
    
    try:
        transcripts = get_interview_transcripts(interview_id)
        logger.info(f"ğŸ“Š Found {len(transcripts)} transcripts for Interview {interview_id}")
        
        # ğŸ§¹ ë©”ëª¨ë¦¬ ì²­ì†Œ (ë¦¬í¬íŠ¸ ë¶„ì„ ì „ ê³µê°„ í™•ë³´)
        import gc
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # ì¸í„°ë·° í¬ì§€ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        with Session(engine) as session:
            interview = session.get(Interview, interview_id)
            position = interview.position if interview else "ì§€ì› ì§ë¬´"

        if not transcripts:
            logger.warning("ì´ ì¸í„°ë·°ì— ëŒ€í•œ ëŒ€í™” ë‚´ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            create_or_update_evaluation_report(
                interview_id,
                technical_score=0, communication_score=0, cultural_fit_score=0,
                summary_text="ê¸°ë¡ëœ ëŒ€í™”ê°€ ì—†ì–´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                details_json={"error": "no_data"}
            )
            return

        conversation = "\n".join([f"{t.speaker}: {t.text}" for t in transcripts])
        if len(conversation) > 12000: # ëŒ€ëµ 8000 í† í° ë‚´ì™¸ë¡œ ìë¦„ (ì•ˆì „ ê³„ìˆ˜)
            logger.info(f"âš ï¸ Conversation too long ({len(conversation)} chars). Truncating to fit LLM context.")
            conversation = conversation[:5000] + "\n... (ì¤‘ëµ) ...\n" + conversation[-6000:]

        try:
            # LangChain Parser ì„¤ì •
            parser = JsonOutputParser(pydantic_object=FinalReportSchema)
            
            # [í•µì‹¬] ì „ì²´ í‰ê°€ ë£¨ë¸Œë¦­ ê°€ì ¸ì˜¤ê¸°
            full_rubric = {}
            try:
                full_rubric = create_evaluation_rubric()
                logger.info("ğŸ“‹ Full Rubric loaded for Final Report")
            except Exception as re_err:
                logger.error(f"Failed to load full rubric: {re_err}")

            logger.info(f"ğŸ¤– Starting [FINAL REPORT] LLM analysis for Interview {interview_id}...")
            exaone = get_exaone_llm()
            system_msg = f"""ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ê¸°ìˆ  ê¸°ì—…ì—ì„œ ìˆ˜ì²œ ëª…ì˜ ì§€ì›ìë¥¼ ê²€ì¦í•´ì˜¨ '{position}' ë¶„ì•¼ ì‹œë‹ˆì–´ ë©´ì ‘ê´€ ìœ„ì›íšŒì˜ ìœ„ì›ì¥ì…ë‹ˆë‹¤. 
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì œê³µëœ ë©´ì ‘ ë¡œê·¸ì™€ [í‘œì¤€ í‰ê°€ ë£¨ë¸Œë¦­]ì„ ë°”íƒ•ìœ¼ë¡œ ì§€ì›ìì˜ ì—­ëŸ‰ì„ 6ê°œ í•µì‹¬ ì§€í‘œë¡œ ì •ë°€ í‰ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[í‘œì¤€ í‰ê°€ ë£¨ë¸Œë¦­]
{json.dumps(full_rubric, ensure_ascii=False, indent=2)}

[í‰ê°€ ì§€ì¹¨]
1. ìœ„ ë£¨ë¸Œë¦­ì˜ 'í‰ê°€ ê¸°ì¤€(Criteria)'ê³¼ 'ë“±ê¸‰ë³„ ì§€í‘œ(Indicators)'ë¥¼ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ì ìˆ˜ë¥¼ ì‚°ì¶œí•˜ì‹­ì‹œì˜¤.
2. STAR ë¶„ì„: ì§€ì›ìê°€ ë‹µë³€ì—ì„œ êµ¬ì²´ì ì¸ ìƒí™©(S), ê³¼ì—…(T), í–‰ë™(A), ê²°ê³¼(R)ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í–ˆëŠ”ì§€ ë¶„ì„í•˜ì‹­ì‹œì˜¤.
3. ê¸°ìˆ ì  ì •í•©ì„±: {position} ì§ë¬´ì— í•„ìš”í•œ í•µì‹¬ ê¸°ìˆ  ì›ë¦¬ì™€ ì„ íƒ ê·¼ê±°ë¥¼ ëª…í™•íˆ ì•Œê³  ìˆëŠ”ì§€ ì²´í¬í•˜ì‹­ì‹œì˜¤.
4. í”¼ë“œë°± ì „ë¬¸ì„±: ë‹¨ìˆœ ì¹­ì°¬ë³´ë‹¤ëŠ” ë£¨ë¸Œë¦­ì˜ ì§€í‘œë¥¼ ê·¼ê±°ë¡œ ë³´ì™„í•  ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ì—¬ ì§€ì›ìì˜ ì„±ì¥ì„ ë•ëŠ” 'ì‹œë‹ˆì–´ì˜ ì¡°ì–¸' í†¤ì„ ìœ ì§€í•˜ì‹­ì‹œì˜¤."""

            user_msg = f"""ë‹¤ìŒ ë©´ì ‘ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë£¨ë¸Œë¦­ ê¸°ì¤€ì— ë”°ë¼ ìµœì¢… í‰ê°€ë¥¼ ë‚´ë¦¬ì‹­ì‹œì˜¤.
            
[ë©´ì ‘ ëŒ€í™”]
{conversation}

[ì œì•½ ì‚¬í•­]
- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì‹œìŠ¤í…œ ì—°ë™ì„ ìœ„í•´ ì§€ì •ëœ JSON í¬ë§·ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
- ê° ì ìˆ˜ëŠ” 0ì ì—ì„œ 100ì  ì‚¬ì´ë¡œ ì‚°ì¶œí•˜ì‹­ì‹œì˜¤.
- strengthsì™€ improvementsëŠ” ë£¨ë¸Œë¦­ì˜ ì§€í‘œë¥¼ ì°¸ê³ í•˜ì—¬ ê°ê° 2-3ê°€ì§€ ë¬¸ìì—´ ë°°ì—´([])ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

{parser.get_format_instructions()}"""
            
            # ìƒì„± ë° íŒŒì‹±
            prompt = exaone._create_prompt(system_msg, user_msg)
            raw_output = exaone.invoke(prompt, temperature=0.3)
            
            if not raw_output:
                raise ValueError("LLM generated empty output (possibly context limit reached)")

            try:
                result = parser.parse(raw_output)
            except Exception as parse_err:
                logger.error(f"ìµœì¢… ë¦¬í¬íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {parse_err}")
                json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                else:
                    raise parse_err
                
        except Exception as llm_err:
            logger.error(f"LLM Summary failed: {llm_err}")
            # ê°œë³„ ë‹µë³€ë“¤ì˜ ì ìˆ˜ê°€ ìˆë‹¤ë©´ ê·¸ê²ƒë“¤ì˜ í‰ê· ìœ¼ë¡œ í´ë°±
            avg_tech = sum([t.sentiment_score + 0.5 for t in transcripts if t.speaker == 'User']) / (len([t for t in transcripts if t.speaker == 'User']) or 1) * 100
            
            result = {
                "overall_score": int(avg_tech) or 70,
                "technical_score": int(avg_tech) or 70, 
                "experience_score": 70, "problem_solving_score": 70,
                "communication_score": 70, "responsibility_score": 70, "growth_score": 70,
                "summary_text": "ëŒ€í™”ëŸ‰ì´ ë„ˆë¬´ ë§ì•„ ìƒì„¸ ë¶„ì„ì´ ì§€ì—°ë˜ì—ˆìŠµë‹ˆë‹¤. ì „ì²´ì ì¸ ë‹µë³€ í’ˆì§ˆì€ ì–‘í˜¸í•©ë‹ˆë‹¤.",
                "technical_feedback": "ê¸°ìˆ ì  ìƒì„¸ ë¶„ì„ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.",
                "experience_feedback": "ê²½í—˜ ìƒì„¸ ë¶„ì„ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.",
                "problem_solving_feedback": "ë¬¸ì œ í•´ê²° ìƒì„¸ ë¶„ì„ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.",
                "communication_feedback": "ì˜ì‚¬ì†Œí†µ ìƒì„¸ ë¶„ì„ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.",
                "responsibility_feedback": "ì±…ì„ê° ìƒì„¸ ë¶„ì„ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.",
                "growth_feedback": "ì„±ì¥ ì˜ì§€ ìƒì„¸ ë¶„ì„ì´ ìƒëµë˜ì—ˆìŠµë‹ˆë‹¤.",
                "strengths": ["ì„±ì‹¤í•œ ë‹µë³€ ì°¸ì—¬"], "improvements": ["ìƒì„¸ í”¼ë“œë°± ê¸°ìˆ  ì§€ì› í•„ìš”"]
            }

        # DB ì €ì¥ì„ ìœ„í•´ ì ìˆ˜ ì¶”ì¶œ
        tech = result.get("technical_score", 0)
        comm = result.get("communication_score", 0)
        # cultural_fitì€ responsibilityì™€ growthì˜ í‰ê· ìœ¼ë¡œ ì„ì‹œ ê³„ì‚° (DB ì»¬ëŸ¼ í˜¸í™˜ì„±)
        cult = (result.get("responsibility_score", 0) + result.get("growth_score", 0)) / 2
        overall = result.get("overall_score", (tech + comm + cult) / 3)

        # ëª¨ë“  ìƒì„¸ í•„ë“œë¥¼ details_jsonì— ì €ì¥ (í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™)
        details = {
            "experience_score": result.get("experience_score", 0),
            "problem_solving_score": result.get("problem_solving_score", 0),
            "responsibility_score": result.get("responsibility_score", 0),
            "growth_score": result.get("growth_score", 0),
            "technical_feedback": result.get("technical_feedback", ""),
            "experience_feedback": result.get("experience_feedback", ""),
            "problem_solving_feedback": result.get("problem_solving_feedback", ""),
            "communication_feedback": result.get("communication_feedback", ""),
            "responsibility_feedback": result.get("responsibility_feedback", ""),
            "growth_feedback": result.get("growth_feedback", ""),
            "strengths": result.get("strengths", []),
            "improvements": result.get("improvements", [])
        }

        create_or_update_evaluation_report(
            interview_id,
            technical_score=tech,
            communication_score=comm,
            cultural_fit_score=cult,
            summary_text=result.get("summary_text", ""),
            details_json=details
        )
        update_interview_overall_score(interview_id, score=overall)
        logger.info(f"âœ… ì¸í„°ë·° {interview_id}ì— ëŒ€í•œ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")

    except Exception as e:
        logger.error(f"âŒ Error in generate_final_report: {e}")
        create_or_update_evaluation_report(
            interview_id,
            technical_score=0, summary_text=f"ì˜¤ë¥˜: {str(e)}"
        )
