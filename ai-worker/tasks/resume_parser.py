
import logging
import os
import json
from celery import shared_task, current_app
from sqlmodel import Session

logger = logging.getLogger(__name__)

try:
    from db_models import Resume
    from db import engine
except ImportError as e:
    logger.error(f"âŒ Critical Import Error in resume_parser: {e}")
    
from .parse_resume import parse_resume_final

logger.info("âœ… Task Module 'tasks.resume_pipeline' is being loaded.")

@shared_task(bind=True, name="tasks.resume_pipeline.process_resume_pipeline", queue='gpu_queue')
def parse_resume_pdf(self, resume_id: int, file_path: str):
    """
    ì´ë ¥ì„œ PDF íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ DBì— ì €ì¥í•˜ê³ , ì„ë² ë”© ìƒì„±ì„ ìš”ì²­í•©ë‹ˆë‹¤.
    """
    # 1. íŒŒì¼ ê²½ë¡œ ì •ê·œí™” (ì»¨í…Œì´ë„ˆ í™˜ê²½ì— ë§ê²Œ ì¡°ì •)
    # ë°±ì—”ë“œì—ì„œ ì˜¨ ë¡œì»¬ ê²½ë¡œë‚˜ ìƒëŒ€ ê²½ë¡œë¥¼ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì˜ /app/uploads ê²½ë¡œë¡œ ê°•ì œ ë³€í™˜
    filename = os.path.basename(file_path)
    # /app/uploadsëŠ” docker-composeì—ì„œ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ
    normalized_path = os.path.join("/app/uploads", filename)
    
    logger.info(f"ğŸš€ [START] Resume parsing ID: {resume_id}")
    logger.info(f"Original path: {file_path}")
    logger.info(f"Normalized path: {normalized_path}")
    
    try:
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(normalized_path):
            logger.error(f"âŒ File not found at normalized path: {normalized_path}")
            # í´ë°±: ì›ë˜ ê²½ë¡œë¡œ í•œ ë²ˆ ë” ì‹œë„
            if os.path.exists(file_path):
                normalized_path = file_path
            else:
                _update_status(resume_id, "failed")
                return
            
        # 2. íŒŒì‹± ì‹¤í–‰
        logger.info(f"ğŸ” Parsing PDF...")
        parsed_data = parse_resume_final(normalized_path)
        logger.info(f"âœ… Parsing Success: {parsed_data.get('header', {}).get('name')} detected")
        
        # 3. DB ì—…ë°ì´íŠ¸
        with Session(engine) as session:
            resume = session.get(Resume, resume_id)
            if not resume:
                logger.error(f"âŒ Resume {resume_id} not found in DB")
                return

            resume.structured_data = parsed_data
            target_pos = parsed_data.get("header", {}).get("target_role")
            if target_pos:
                resume.target_position = target_pos
                
            resume.extracted_text = json.dumps(parsed_data, ensure_ascii=False)
            resume.processing_status = "processing" 
            session.add(resume)
            session.commit()
            logger.info(f"ğŸ’¾ DB Updated for Resume {resume_id}")
            
        # 4. ì„ë² ë”© íƒœìŠ¤í¬ í˜¸ì¶œ (ì´ë¦„ ëª…í™•í™”)
        current_app.send_task(
            "tasks.resume_embedding.generate_resume_embeddings",
            args=[resume_id],
            queue='gpu_queue'
        )
        logger.info(f"â¡ï¸ [NEXT] Sent embedding task for Resume {resume_id}")

    except Exception as e:
        logger.error(f"Error parsing resume {resume_id}: {e}", exc_info=True)
        _update_status(resume_id, "failed")

def _update_status(resume_id: int, status: str):
    with Session(engine) as session:
        resume = session.get(Resume, resume_id)
        if resume:
            resume.processing_status = status
            session.add(resume)
            session.commit()
