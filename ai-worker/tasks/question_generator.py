import sys
import os
import re
import json
import gc 
import logging
import torch
from datetime import datetime
from celery import shared_task
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ==========================================
# 1. ì´ˆê¸° ì„¤ì • ë° ëª¨ë¸ ê²½ë¡œ ìµœì í™”
# ==========================================

if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
model_path = docker_path if os.path.exists(docker_path) else local_path

# ==========================================
# 2. í˜ë¥´ì†Œë‚˜ ì„¤ì • (Prompt Engineering)
# ==========================================

PROMPT_TEMPLATE = """[|system|]ë‹¹ì‹ ì€ ì§€ì›ìì˜ ì—­ëŸ‰ì„ ì •ë°€í•˜ê²Œ ê²€ì¦í•˜ëŠ” ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
ì œê³µëœ [ì´ë ¥ì„œ ë¬¸ë§¥]ê³¼ [ë©´ì ‘ ì§„í–‰ ìƒí™©]ì„ ë°”íƒ•ìœ¼ë¡œ, ì§€ì›ìì—ê²Œ ë˜ì§ˆ 'ë‹¤ìŒ ì§ˆë¬¸' 1ê°œë§Œ ìƒì„±í•˜ì‹­ì‹œì˜¤.

[ì ˆëŒ€ ê·œì¹™]
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
2. ì§ˆë¬¸ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, 150ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
3. íŠ¹ìˆ˜ë¬¸ì(JSON ê¸°í˜¸, ì—­ë”°ì˜´í‘œ ë“±)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì˜¤ì§ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
4. "ì§ˆë¬¸:" ì´ë¼ëŠ” ìˆ˜ì‹ì–´ ì—†ì´ ë°”ë¡œ ì§ˆë¬¸ ë³¸ë¬¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
5. ì´ì „ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ í•˜ì‹­ì‹œì˜¤.

[ì´ë ¥ì„œ ë° ë‹µë³€ ë¬¸ë§¥]
{context}

[í˜„ì¬ ë©´ì ‘ ë‹¨ê³„ ì •ë³´]
- ë‹¨ê³„ëª…: {stage_name}
- ê°€ì´ë“œ: {guide}

[|user|]ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.[|endofturn|]
[|assistant|]"""

# ==========================================
# 3. ë©”ì¸ ì‘ì—…: ì§ˆë¬¸ ìƒì„± íƒœìŠ¤í¬
# ==========================================

@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    """
    ì¸í„°ë·° ì§„í–‰ ìƒí™©ì„ íŒŒì•…í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ì˜ AI ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    from db import engine, Session, select, Interview, Transcript, Speaker, Question, save_generated_question
    from utils.exaone_llm import get_exaone_llm
    from tasks.tts import synthesize_task  # [ìˆ˜ì •] ì •í™•í•œ íƒœìŠ¤í¬ í•¨ìˆ˜ëª… ì„í¬íŠ¸
    
    with Session(engine) as session:
        interview = session.get(Interview, interview_id)
        if not interview: 
            logger.error(f"Interview {interview_id} not found.")
            return {"status": "error", "message": "Interview not found"}

            # 2. ë§ˆì§€ë§‰ AI ë°œí™” í™•ì¸ (Stage íŒë³„ + ì¤‘ë³µ ë°©ì§€)
            # [ìˆ˜ì •] User transcriptëŠ” question_idê°€ ì—†ì–´ stage íŒë³„ ë¶ˆê°€ â†’ ë§ˆì§€ë§‰ AI ë°œí™” ê¸°ì¤€ìœ¼ë¡œ íŒë³„
            stmt_all = select(Transcript).where(Transcript.interview_id == interview_id).order_by(Transcript.order.desc())
            last_transcript = session.exec(stmt_all).first()

            stmt_ai = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.AI
            ).order_by(Transcript.order.desc(), Transcript.id.desc())  # idë¥¼ tiebreakerë¡œ ì‚¬ìš© (order ê°™ì„ ë•Œ ìµœì‹  AI ë°œí™” ë³´ì¥)
            last_ai_transcript = session.exec(stmt_ai).first()

            # ë§ˆì§€ë§‰ AI ë°œí™”ê°€ 10ì´ˆ ì´ë‚´ë¼ë©´ ìŠ¤í‚µ (Race Condition ë°©ì§€)
            if last_ai_transcript:
                diff = (datetime.utcnow() - last_ai_transcript.timestamp).total_seconds()
                if diff < 10:
                    logger.info(f"Skipping duplicate request for interview {interview_id}")
                    return {"status": "skipped"}

            # [ìˆ˜ì •] 3. ì „ê³µ/ì§ë¬´ ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ê²°ì •
            major = ""
            if interview.resume and interview.resume.structured_data:
                sd = interview.resume.structured_data
                if isinstance(sd, str):
                    sd = json.loads(sd)
                edu = sd.get("education", [])
                major = next((e.get("major", "") for e in edu if e.get("major", "").strip()), "")

            is_transition = check_if_transition(major, interview.position)
            get_next_stage_func = get_next_stage_transition if is_transition else get_next_stage_normal

            # ë§ˆì§€ë§‰ AI ë°œí™”ì˜ question_typeìœ¼ë¡œ í˜„ì¬ stage íŒë³„
            if last_ai_transcript and last_ai_transcript.question_id:
                last_question = session.get(Question, last_ai_transcript.question_id)
                last_stage_name = last_question.question_type if last_question else "intro"
            else:
                last_stage_name = "intro"

            logger.info(f"Current stage determined: {last_stage_name} (is_transition={is_transition})")
            next_stage = get_next_stage_func(last_stage_name)

            if not next_stage:
                logger.info(f"Interview {interview_id} finished. Transitioning to COMPLETED.")
                interview.status = "COMPLETED"
                session.add(interview)
                session.commit()
                return {"status": "completed"}

            # [ì¤‘ë³µ ë°©ì§€ ê°œì„ ] next_stageê°€ ì´ë¯¸ ìƒì„±ëëŠ”ì§€ í™•ì¸ (timestamp ê¸°ë°˜ X â†’ stage ê¸°ë°˜ O)
            # ê¸°ì¡´ 10ì´ˆ ì²´í¬ëŠ” ì‚¬ìš©ìê°€ ë¹ ë¥´ê²Œ ë‹µë³€í•˜ë©´ Q3 ìƒì„±ì´ ì˜ì›íˆ ìŠ¤í‚µë˜ëŠ” ë²„ê·¸ ë°œìƒ
            if last_ai_transcript:
                last_q_for_check = session.get(Question, last_ai_transcript.question_id) if last_ai_transcript.question_id else None
                if last_q_for_check and last_q_for_check.question_type == next_stage['stage']:
                    diff = (datetime.utcnow() - last_ai_transcript.timestamp).total_seconds()
                    if diff < 30:
                        logger.info(f"Next stage '{next_stage['stage']}' already generated {diff:.1f}s ago, skipping duplicate")
                        return {"status": "skipped"}

            # 4. [ìµœì í™”] template stageëŠ” RAG/LLM ì—†ì´ ì¦‰ì‹œ í¬ë§·
            if next_stage.get("type") == "template":
                candidate_name = "ì§€ì›ì"
                target_role = interview.position or "í•´ë‹¹ ì§ë¬´"
                if interview.resume and interview.resume.structured_data:
                    sd = interview.resume.structured_data
                    if isinstance(sd, str):
                        sd = json.loads(sd)
                    candidate_name = sd.get("header", {}).get("name", "ì§€ì›ì")
                    target_role = sd.get("header", {}).get("target_role", target_role)

                template_vars = {"candidate_name": candidate_name, "target_role": target_role, "major": major}
                tpl = next_stage.get("template", "{candidate_name} ì§€ì›ìë‹˜, ê³„ì†í•´ì£¼ì„¸ìš”.")
                try:
                    formatted = tpl.format(**template_vars)
                except KeyError:
                    formatted = tpl

                intro_msg = next_stage.get("intro_sentence", "")
                display_name = next_stage.get("display_name", "ë©´ì ‘ì§ˆë¬¸")
                final_content = f"[{display_name}] {intro_msg} {formatted}".strip() if intro_msg else f"[{display_name}] {formatted}"
                logger.info(f"Template stage '{next_stage['stage']}' â†’ ì¦‰ì‹œ í¬ë§· ì™„ë£Œ (RAG/LLM ìƒëµ)")

            else:
                # 4-b. AI stage: RAGë¡œ ë¬¸ë§¥ í™•ë³´ í›„ LLM ìƒì„±
                # query_templateì˜ {target_role}, {major} ë³€ìˆ˜ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜
                query_template = next_stage.get("query_template", interview.position)
                try:
                    query = query_template.format(
                        target_role=interview.position or "í•´ë‹¹ ì§ë¬´",
                        major=major or ""
                    )
                except (KeyError, ValueError):
                    query = query_template  # ì¹˜í™˜ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ì‚¬ìš©
                rag_results = retrieve_context(query, resume_id=interview.resume_id, top_k=3)
                context_text = "\n".join([r['text'] for r in rag_results]) if rag_results else "íŠ¹ë³„í•œ ì •ë³´ ì—†ìŒ"

                if last_transcript and last_transcript.speaker == "User":
                    context_text += f"\n[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]: {last_transcript.text}"

                llm = get_exaone_llm()
                prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
                chain = prompt | llm | StrOutputParser()

                final_content = chain.invoke({
                    "context": context_text,
                    "stage_name": next_stage['display_name'],
                    "guide": next_stage.get('guide', '')
                })

            # 6. DB ì €ì¥ (Question ë° Transcript)
            # categoryê°€ Noneì¸ ê²½ìš°ì—ë„ fallback ì ìš© (get defaultëŠ” Noneì¼ ë•Œ ì‘ë™ ì•ˆí•¨)
            question_id = save_generated_question(
                interview_id=interview_id,
                content=final_content,
                category=next_stage.get('category') or 'behavioral',  # None â†’ 'behavioral'
                stage=next_stage['stage'],
                guide=next_stage.get('guide', ''),
                session=session
            )

            # 7. ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # 5. ê²°ê³¼ ì €ì¥
            category_raw = next_stage_data.get("category", "technical")
            category_map = {"certification": "technical", "project": "technical", "narrative": "behavioral", "problem_solving": "situational"}
            db_category = category_map.get(category_raw, "technical")
            
            # [ì¶”ê°€] ë©´ì ‘ ë‹¨ê³„ë³„ í•œêµ­ì–´ ëª…ì¹­ ë° ì•ˆë‚´ ë¬¸êµ¬ ê°€ì ¸ì˜¤ê¸°
            try:
                if is_transition:
                    from config.interview_scenario_transition import INTERVIEW_STAGES as TRANS_STAGES
                    target_stages = TRANS_STAGES
                else:
                    from config.interview_scenario import INTERVIEW_STAGES as STD_STAGES
                    target_stages = STD_STAGES
            except ImportError:
                from config.interview_scenario import INTERVIEW_STAGES as STD_STAGES
                target_stages = STD_STAGES

            stage_display = "ì‹¬ì¸µ ë©´ì ‘"
            intro_msg = ""
            for s in target_stages:
                if s["stage"] == stage_name:
                    stage_display = s.get("display_name", stage_display)
                    intro_msg = s.get("intro_sentence", "")
                    break
            
            # ê¼¬ë¦¬ì§ˆë¬¸ì˜ ê²½ìš° ê³ ì •ëœ ì¸íŠ¸ë¡œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ LLMì—ê²ŒëŠ” ì‹œí‚¤ì§€ ì•ŠìŒ)
            if stage_type == "followup":
                intro_msg = "ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìŠµë‹ˆë‹¤."
            elif intro_msg == "ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìŠµë‹ˆë‹¤.":
                # ë©”ì¸ ì§ˆë¬¸ì¸ë° ì‹œë‚˜ë¦¬ì˜¤ì— ì˜ëª» ë“¤ì–´ê°€ ìˆëŠ” ê²½ìš° ì œê±°
                intro_msg = ""

            # ì§ˆë¬¸ ì•ì— [ë‹¨ê³„] ë° ì•ˆë‚´ ë¬¸êµ¬ ì¶”ê°€
            final_content = f"[{stage_display}] {intro_msg} {content}" if intro_msg else f"[{stage_display}] {content}"
            
            logger.info(f"ğŸ’¾ Saving generated question to DB for Interview {interview_id} (Stage: {stage_name})")
            q_id = save_generated_question(interview_id, final_content, db_category, stage_name, next_stage_data.get("guide", ""), session=session)
            
            # [í•µì‹¬ ì¶”ê°€] ì§ˆë¬¸ ì €ì¥ í›„ ì „ìš© TTS ìƒì„± íƒœìŠ¤í¬ ì¦‰ì‹œ íŠ¸ë¦¬ê±°
            if q_id:
                logger.info(f"ğŸ”Š Triggering TTS synthesis for Question ID: {q_id}")
                synthesize_task.delay(final_content, language="auto", question_id=q_id)

            return {"status": "success", "stage": stage_name, "question": final_content}
        except Exception as e:
            logger.error(f"âŒ ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ (Retry ì‹œë„): {e}")
            raise self.retry(exc=e, countdown=3)
        finally:
            gc.collect()