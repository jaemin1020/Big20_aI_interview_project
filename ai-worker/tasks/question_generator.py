import sys
import os
import re
import json
import gc 
import logging
import torch
from datetime import datetime, timezone
from celery import shared_task
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ==========================================
# 1. ì´ˆê¸° ì„¤ì • ë° ëª¨ë¸ ê²½ë¡œ ìµœì í™”
# ==========================================

if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
model_path = docker_path if os.path.exists(docker_path) else local_path

# ==========================================
# 2. í˜ë¥´ì†Œë‚˜ ì„¤ì • (Prompt Engineering)
# ==========================================

PROMPT_TEMPLATE = """[|system|]ë‹¹ì‹ ì€ ì§€ì›ìì˜ ì—­ëŸ‰ì„ ì •ë°€í•˜ê²Œ ê²€ì¦í•˜ëŠ” ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
ì œê³µëœ [ì´ë ¥ì„œ ë¬¸ë§¥]ê³¼ [ë©´ì ‘ ì§„í–‰ ìƒí™©]ì„ ë°”íƒ•ìœ¼ë¡œ, ì§€ì›ìì—ê²Œ ë˜ì§ˆ 'ë‹¤ìŒ ì§ˆë¬¸' 1ê°œë§Œ ìƒì„±í•˜ì‹­ì‹œì˜¤.

[ì ˆëŒ€ ê·œì¹™]
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
2. ì§ˆë¬¸ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, 150ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
3. íŠ¹ìˆ˜ë¬¸ì(JSON ê¸°í˜¸, ì—­ë”°ì˜´í‘œ ë“±)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì˜¤ì§ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
4. "ì§ˆë¬¸:" ì´ë¼ëŠ” ìˆ˜ì‹ì–´ ì—†ì´ ë°”ë¡œ ì§ˆë¬¸ ë³¸ë¬¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
5. ì´ì „ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ í•˜ì‹­ì‹œì˜¤.
7. **ê¼¬ë¦¬ì§ˆë¬¸(Follow-up) ê·œì¹™**: ë°˜ë“œì‹œ "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìŠµë‹ˆë‹¤."ë¡œ ì‹œì‘í•˜ì‹­ì‹œì˜¤. ì´ì–´ì„œ ì§€ì›ìì˜ ë‹µë³€ ì¤‘ ê°€ì¥ í•µì‹¬ì ì¸ ê¸°ìˆ  í‚¤ì›Œë“œë‚˜ í”„ë¡œì íŠ¸ ì„±ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **êµ¬ì ˆ(ì¼ë¶€)**ì„ ê³¨ë¼ ë°˜ë“œì‹œ ì‘ì€ë”°ì˜´í‘œ(' ') ì•ˆì— ë„£ì–´ "...ë¼ê³  í•˜ì…¨ëŠ”ë°,"ë¡œ ì—°ê²°í•˜ì‹­ì‹œì˜¤. ë¬¸ì¥ ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ê¸°ë³´ë‹¤ í•µì‹¬ ì˜ë¯¸ê°€ ë‹´ê¸´ 'êµ¬ì ˆ' ìœ„ì£¼ë¡œ ì¸ìš©í•˜ì‹­ì‹œì˜¤.
8. **ì‹¬ì¸µ ì§ˆë¬¸ ì „ê°œ**: ì‘ì€ë”°ì˜´í‘œë¡œ ì¸ìš©í•œ êµ¬ì ˆ ì† í‚¤ì›Œë“œì˜ ì •ì˜ë¥¼ ë¬»ê³ , ì§€ì›í•˜ì‹  ì§ë¬´({target_role})ì—ì„œ í•´ë‹¹ ê¸°ìˆ ì´ ì‹¤ë¬´ì ìœ¼ë¡œ ì–´ë–»ê²Œ í™œìš©ë  ìˆ˜ ìˆì„ì§€ ì§ˆë¬¸í•˜ì‹­ì‹œì˜¤. ì¸ìš©êµ¬(' ') ì™¸ì— ë³¼ë“œì²´(**) ë“± ì–´ë– í•œ íŠ¹ìˆ˜ ê¸°í˜¸ë„ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

[ì´ë ¥ì„œ ë° ë‹µë³€ ë¬¸ë§¥]
{context}

[í˜„ì¬ ë©´ì ‘ ë‹¨ê³„ ì •ë³´]
- ë‹¨ê³„ëª…: {stage_name}
- ê°€ì´ë“œ: {guide}

[|user|]ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.[|endofturn|]
[|assistant|]"""

# ==========================================
# 3. ë©”ì¸ ì‘ì—…: ì§ˆë¬¸ ìƒì„± íƒœìŠ¤í¬
# ==========================================

@shared_task(bind=True, name="tasks.question_generation.generate_next_question")
def generate_next_question_task(self, interview_id: int):
    """
    ì¸í„°ë·° ì§„í–‰ ìƒí™©ì„ íŒŒì•…í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ì˜ AI ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    from db import engine, Session, select, Interview, Transcript, Speaker, Question, save_generated_question
    from utils.exaone_llm import get_exaone_llm
    from tasks.tts import synthesize_task
    from utils.interview_helpers import check_if_transition
    from config.interview_scenario import get_next_stage as get_next_stage_normal
    from config.interview_scenario_transition import get_next_stage as get_next_stage_transition
    from tasks.rag_retrieval import retrieve_context, retrieve_similar_questions
    try:
        with Session(engine) as session:
            interview = session.get(Interview, interview_id)
            if not interview: 
                logger.error(f"Interview {interview_id} not found.")
                return {"status": "error", "message": "Interview not found"}

            # 2. ë§ˆì§€ë§‰ AI ë°œí™” í™•ì¸ (Stage íŒë³„ + ì¤‘ë³µ ë°©ì§€)
            # [ìˆ˜ì •] User transcriptëŠ” question_idê°€ ì—†ì–´ stage íŒë³„ ë¶ˆê°€ â†’ ë§ˆì§€ë§‰ AI ë°œí™” ê¸°ì¤€ìœ¼ë¡œ íŒë³„
            stmt_all = select(Transcript).where(Transcript.interview_id == interview_id).order_by(Transcript.order.desc())
            last_transcript = session.exec(stmt_all).first()

            stmt_ai = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.AI
            ).order_by(Transcript.order.desc(), Transcript.id.desc())  # idë¥¼ tiebreakerë¡œ ì‚¬ìš© (order ê°™ì„ ë•Œ ìµœì‹  AI ë°œí™” ë³´ì¥)
            last_ai_transcript = session.exec(stmt_ai).first()

            # [ìˆ˜ì •] RAG ì¿¼ë¦¬ë¡œ ì‚¬ìš©í•  ì§€ì›ìì˜ 'ì§„ì§œ' ë§ˆì§€ë§‰ ë‹µë³€ ë³„ë„ ì¶”ì¶œ
            stmt_user = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.USER
            ).order_by(Transcript.order.desc(), Transcript.id.desc())
            last_user_transcript = session.exec(stmt_user).first()

            # ë§ˆì§€ë§‰ AI ë°œí™”ê°€ 10ì´ˆ ì´ë‚´ë¼ë©´ ìŠ¤í‚µ (Race Condition ë°©ì§€)
            if last_ai_transcript:
                ts = last_ai_transcript.timestamp
                now = datetime.now()
                # timezone-aware vs naive í˜¼ìš© ë°©ì§€
                if ts.tzinfo is not None:
                    now = datetime.now(timezone.utc)
                diff = (now - ts).total_seconds()
                if abs(diff) < 3:
                    logger.info(f"Skipping near-instant duplicate for interview {interview_id} (diff={diff:.1f}s)")
                    return {"status": "skipped"}

            # [ìˆ˜ì •] 3. ì „ê³µ/ì§ë¬´ ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ê²°ì •
            major = ""
            if interview.resume and interview.resume.structured_data:
                sd = interview.resume.structured_data
                if isinstance(sd, str):
                    sd = json.loads(sd)
                edu = sd.get("education", [])
                major = next((e.get("major", "") for e in edu if e.get("major", "").strip()), "")

            is_transition = check_if_transition(major, interview.position)
            get_next_stage_func = get_next_stage_transition if is_transition else get_next_stage_normal

            # ë§ˆì§€ë§‰ AI ë°œí™”ì˜ question_typeìœ¼ë¡œ í˜„ì¬ stage íŒë³„
            if last_ai_transcript and last_ai_transcript.question_id:
                last_question = session.get(Question, last_ai_transcript.question_id)
                last_stage_name = last_question.question_type if last_question else "intro"
            else:
                last_stage_name = "intro"

            logger.info(f"Current stage determined: {last_stage_name} (is_transition={is_transition})")
            next_stage = get_next_stage_func(last_stage_name)

            if not next_stage:
                logger.info(f"Interview {interview_id} finished. Transitioning to COMPLETED.")
                interview.status = "COMPLETED"
                session.add(interview)
                session.commit()
                return {"status": "completed"}

            # [ìˆ˜ì •] ê¼¬ë¦¬ì§ˆë¬¸(followup) ìƒì„± ì œí•œ ë¡œì§
            # ë‹¤ìŒ ë‹¨ê³„ê°€ followupì¸ë°, ë§ˆì§€ë§‰ ë°œí™”ìê°€ ì—¬ì „íˆ AIë¼ë©´ ì§€ì›ìê°€ ì•„ì§ ë‹µë³€ì„ ì•ˆ í•œ ê²ƒì„.
            if next_stage.get("type") == "followup":
                if last_transcript and last_transcript.speaker == "AI":
                    logger.info(f"Next stage is followup, but WAITING for user answer. Skipping generation.")
                    return {"status": "waiting_for_user"}

            # [ì¤‘ë³µ ë°©ì§€ ê°œì„ ] next_stageê°€ ì´ë¯¸ ìƒì„±ëëŠ”ì§€ í™•ì¸ (timestamp ê¸°ë°˜ X â†’ stage ê¸°ë°˜ O)
            if last_ai_transcript:
                last_q_for_check = session.get(Question, last_ai_transcript.question_id) if last_ai_transcript.question_id else None
                if last_q_for_check and last_q_for_check.question_type == next_stage['stage']:
                    ts2 = last_ai_transcript.timestamp
                    now2 = datetime.now()
                    if ts2.tzinfo is not None:
                        now2 = datetime.now(timezone.utc)
                    diff2 = (now2 - ts2).total_seconds()
                    if 0 < diff2 < 120:  # ì–‘ìˆ˜ & 2ë¶„ ì´ë‚´ ë™ì¼ stage ì¬ìƒì„± ë°©ì§€
                        logger.info(f"Next stage '{next_stage['stage']}' already generated {diff2:.1f}s ago, skipping duplicate")
                        return {"status": "skipped"}

            # 4. [ìµœì í™”] template stageëŠ” RAG/LLM ì—†ì´ ì¦‰ì‹œ í¬ë§·
            if next_stage.get("type") == "template":
                candidate_name = "ì§€ì›ì"
                target_role = interview.position or "í•´ë‹¹ ì§ë¬´"
                
                if interview.resume and interview.resume.structured_data:
                    sd = interview.resume.structured_data
                    if isinstance(sd, str): sd = json.loads(sd)
                    candidate_name = sd.get("header", {}).get("name") or sd.get("header", {}).get("candidate_name") or "ì§€ì›ì"
                    target_role = sd.get("header", {}).get("target_role") or target_role
                    
                    # 1. ìê²©ì¦ ë¦¬ìŠ¤íŠ¸ì—… (ëª¨ë‘ ì¶”ì¶œ)
                    certs = sd.get("certifications", [])
                    if certs:
                        cert_names = [c.get("title") or c.get("name") for c in certs if (c.get("title") or c.get("name"))]
                        cert_list = ", ".join(cert_names)
                
                if not cert_list: cert_list = "ê´€ë ¨ ìê²©"

                # 4. ê²½ë ¥ ì‚¬í•­ ë° í”„ë¡œì íŠ¸ ë¶„ë¦¬ ì¶”ì¶œ
                act_org, act_role = "ê´€ë ¨ ê¸°ê´€", "ë‹´ë‹¹ ì—…ë¬´"
                proj_org, proj_name = "í•´ë‹¹ ê¸°ê´€", "ê´€ë ¨ í”„ë¡œì íŠ¸"

                if interview.resume and interview.resume.structured_data:
                    sd = interview.resume.structured_data
                    if isinstance(sd, str): sd = json.loads(sd)
                    
                    # 4-1. ê²½ë ¥ (activities)
                    acts = sd.get("activities", [])
                    if acts:
                        act_org = acts[0].get("organization") or acts[0].get("name") or act_org
                        act_role = acts[0].get("role") or acts[0].get("position") or act_role
                    
                    # 4-2. í”„ë¡œì íŠ¸ (projects) - ì‹ ê·œ í¬ë§· ë°˜ì˜ (0:ê¸°ê°„, 1:ì œëª©, 2:ê¸°ê´€)
                    projs = sd.get("projects", [])
                    if projs:
                        proj_name = projs[0].get("title") or proj_name
                        proj_org = projs[0].get("organization") or proj_org

                template_vars = {
                    "candidate_name": candidate_name, 
                    "target_role": target_role, 
                    "major": major or "í•´ë‹¹ ì „ê³µ",
                    "cert_list": cert_list,
                    "act_org": act_org,
                    "act_role": act_role,
                    "proj_org": proj_org,
                    "proj_name": proj_name
                }
                
                tpl = next_stage.get("template", "{candidate_name} ì§€ì›ìë‹˜, ê³„ì†í•´ì£¼ì„¸ìš”.")
                try:
                    formatted = tpl.format(**template_vars)
                except KeyError:
                    # í•„ìš”í•œ í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì•ˆì „ ì¥ì¹˜
                    formatted = tpl.replace("{candidate_name}", candidate_name).replace("{course_name}", course_name).replace("{cert_name}", cert_name)

                intro_msg = next_stage.get("intro_sentence", "")
                display_name = next_stage.get("display_name", "ë©´ì ‘ì§ˆë¬¸")
                final_content = f"[{display_name}] {intro_msg} {formatted}".strip() if intro_msg else f"[{display_name}] {formatted}"
                logger.info(f"Template stage '{next_stage['stage']}' (v2) â†’ ì¦‰ì‹œ í¬ë§· ì™„ë£Œ (Direct Extraction)")

            else:
                # [ë¡œì§ ë‹¨ìˆœí™˜] ê¼¬ë¦¬ì§ˆë¬¸ê³¼ ì¼ë°˜ ì§ˆë¬¸ì˜ ì»¨í…ìŠ¤íŠ¸ ë¶„ë¦¬
                if next_stage.get("type") == "followup":
                    # ê¼¬ë¦¬ì§ˆë¬¸: RAG/ì§ˆë¬¸ì€í–‰ ëª¨ë‘ ìŠ¤í‚µí•˜ê³  ì˜¤ì§ 'ì§ˆë¬¸-ë‹µë³€' ë§¥ë½ë§Œ ì‚¬ìš© (í™˜ê° 0%)
                    logger.info("ğŸ¯ Follow-up mode: RAG & Question Bank disabled. Focusing purely on conversation context.")
                    context_text = f"ì´ì „ ì§ˆë¬¸: {last_ai_transcript.text if last_ai_transcript else 'ì—†ìŒ'}\n"
                    if last_user_transcript:
                        context_text += f"[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]: {last_user_transcript.text}"
                    rag_results = []
                else:
                    # ì¼ë°˜ AI ì§ˆë¬¸ (ê²½í—˜/ë¬¸ì œí•´ê²° ë“±): ì´ë ¥ì„œ RAG ê²€ìƒ‰ ìˆ˜í–‰
                    query_template = next_stage.get("query_template", interview.position)
                    try:
                        query = query_template.format(
                            target_role=interview.position or "í•´ë‹¹ ì§ë¬´",
                            major=major or ""
                        )
                    except (KeyError, ValueError):
                        query = query_template

                    category_raw = next_stage.get("category")
                    rag_results = []
                    context_text = ""

                    if category_raw == "certification" and interview.resume and interview.resume.structured_data:
                        sd = interview.resume.structured_data
                        if isinstance(sd, str): sd = json.loads(sd)
                        certs = sd.get("certifications", [])
                        important_certs = [c for c in certs if any(kw in c.get('title', '') for kw in ["ë°ì´í„°", "ë¶„ì„", "RAG", "AI", "í´ë¼ìš°ë“œ", "SQL", "ADSP", "ì •ë³´ì²˜ë¦¬"])]
                        final_certs = important_certs if important_certs else certs
                        if final_certs:
                            logger.info(f"âœ… RAG ê±´ë„ˆëœ€ (êµ¬ì¡°í™” ë°ì´í„° í™œìš©)")
                            context_text = "ì§€ì›ìê°€ ë³´ìœ í•œ ìê²©ì¦ ëª©ë¡:\n" + "\n".join([f"- {c.get('title')}" for c in final_certs])
                            rag_results = [{'text': f"ìê²©ëª…: {final_certs[0].get('title')}"}]
                        else:
                            rag_results = retrieve_context(query, resume_id=interview.resume_id, top_k=3)
                            context_text = "\n".join([r['text'] for r in rag_results]) if rag_results else "íŠ¹ë³„í•œ ì •ë³´ ì—†ìŒ"
                    else:
                        filter_type = None
                        if category_raw == "certification": filter_type = "certifications"
                        rag_results = retrieve_context(query, resume_id=interview.resume_id, top_k=3, filter_type=filter_type)
                        context_text = "\n".join([r['text'] for r in rag_results]) if rag_results else "íŠ¹ë³„í•œ ì •ë³´ ì—†ìŒ"
                        
                    if last_user_transcript:
                        context_text += f"\n[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]: {last_user_transcript.text}"

                llm = get_exaone_llm()
                prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
                chain = prompt | llm | StrOutputParser()

                final_content = chain.invoke({
                    "context": context_text,
                    "stage_name": next_stage['display_name'],
                    "guide": next_stage.get('guide', ''),
                    "target_role": interview.position or "ì§€ì› ì§ë¬´"
                })

                # ì¸íŠ¸ë¡œ ë©”ì‹œì§€ ì¡°í•© (3ë²ˆ ì§ˆë¬¸ ì „ìš© ë¡œì§ í¬í•¨)
                candidate_name = "ì§€ì›ì"
                if interview.resume and interview.resume.structured_data:
                    sd = interview.resume.structured_data
                    if isinstance(sd, str): sd = json.loads(sd)
                    candidate_name = sd.get("header", {}).get("name", "ì§€ì›ì")

                intro_tpl = next_stage.get("intro_sentence", "")
                if next_stage['stage'] == 'skill' and 'cert_name' in intro_tpl:
                    # RAG ê²°ê³¼ì—ì„œ ì²« ë²ˆì§¸ ìê²©ì¦ ì´ë¦„ ì¶”ì¶œ ì‹œë„
                    cert_name = "ìë£Œì— ëª…ì‹œëœ"
                    if rag_results:
                        # "[ìê²©ì¦] ìê²©ëª…: XXX" í˜•íƒœì—ì„œ ì´ë¦„ ì¶”ì¶œ
                        match = re.search(r'ìê²©ëª…:\s*([^,\(]+)', rag_results[0]['text'])
                        if match: cert_name = match.group(1).strip()
                    intro_msg = intro_tpl.format(candidate_name=candidate_name, cert_name=cert_name)
                elif intro_tpl:
                    try:
                        intro_msg = intro_tpl.format(candidate_name=candidate_name)
                    except:
                        intro_msg = intro_tpl
                else:
                    intro_msg = ""

                if next_stage.get("type") == "followup":
                    intro_msg = "" # í”„ë¡¬í”„íŠ¸ì—ì„œ ì´ë¯¸ ìƒì„±í•˜ë¯€ë¡œ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ë¹„ì›€
                
                display_name = next_stage.get("display_name", "ì‹¬ì¸µ ë©´ì ‘")
                final_content = f"[{display_name}] {intro_msg} {final_content}".strip() if intro_msg else f"[{display_name}] {final_content}".strip()

            # 6. DB ì €ì¥ (Question ë° Transcript)

            # 6. DB ì €ì¥ (Question ë° Transcript)
            category_raw = next_stage.get("category", "technical")
            category_map = {"certification": "technical", "project": "technical", "narrative": "behavioral", "problem_solving": "situational"}
            db_category = category_map.get(category_raw, "technical")

            logger.info(f"ğŸ’¾ Saving generated question to DB for Interview {interview_id} (Stage: {next_stage['stage']})")
            q_id = save_generated_question(
                interview_id=interview_id,
                content=final_content,
                category=db_category,
                stage=next_stage['stage'],
                guide=next_stage.get('guide', ''),
                session=session
            )

            # 7. ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # 8. TTS ìƒì„± íƒœìŠ¤í¬ ì¦‰ì‹œ íŠ¸ë¦¬ê±°
            if q_id:
                logger.info(f"ğŸ”Š Triggering TTS synthesis for Question ID: {q_id}")
                synthesize_task.delay(final_content, language="auto", question_id=q_id)

            return {"status": "success", "stage": next_stage['stage'], "question": final_content}
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ (Retry ì‹œë„): {e}")
        raise self.retry(exc=e, countdown=3)
    finally:
        gc.collect()