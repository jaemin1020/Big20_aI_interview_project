import sys
import os
import re
import gc
import logging
import torch
import json
from datetime import datetime
from celery import shared_task  # Celery ë¹„ë™ê¸° ì‘ì—… ë°ì½”ë ˆì´í„°

# ==========================================
# 1. Initial setup & path optimization
# ==========================================

# ğŸ”¹ sys.path: Pythonì´ ëª¨ë“ˆì„ ì°¾ëŠ” ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
# "/app" ê²½ë¡œê°€ ì—†ìœ¼ë©´ ë§¨ ì•ì— ì¶”ê°€
# â†’ ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ ë¡œì»¬ ëª¨ë“ˆ import ë³´ì¥
if "/app" not in sys.path:
    sys.path.insert(0, "/app")

# ğŸ”¹ logger ìƒì„± (worker ì „ìš© ì´ë¦„)
logger = logging.getLogger("AI-Worker-QuestionGen")

# ==========================================
# LangChain imports
# ==========================================

from langchain_core.prompts import PromptTemplate
# PromptTemplate: LLMì— ë„£ì„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬

from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
# OutputParser: LLM ì¶œë ¥ í›„ì²˜ë¦¬

from langchain_core.runnables import RunnablePassthrough
# LCELì—ì„œ ë°ì´í„° íë¦„ ì—°ê²°ìš© (ì—¬ê¸°ì„œëŠ” ë¯¸ì‚¬ìš©)

# ==========================================
# 2. Persona Prompt (Prompt Engineering)
# ==========================================

PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ë² í…Œë‘ ë©´ì ‘ê´€ì´ë©° ì „ë¬¸ ì±„ìš© ìœ„ì›ì¥ì…ë‹ˆë‹¤. 
ì§€ì›ìì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë‚ ì¹´ë¡œìš°ë©´ì„œë„ ì„±ì·¨ ì§€í–¥ì ì¸ ë‹¤ìŒ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ì‹­ì‹œì˜¤.

[ë©´ì ‘ ìƒí™© ê´€ì œ ì •ë³´]
- ì§€ì› ì§ë¬´: {position}
- ì§€ì›ì ì„±ëª…: {name}
- í˜„ì¬ ë‹¨ê³„: {stage_display}
- í•µì‹¬ ê°€ì´ë“œ: {guide}

[ì°¸ê³ : ì§€ì›ì ì´ë ¥ì„œ ë¬¸ë§¥]
{context}

[ì´ì „ ëŒ€í™” ìš”ì•½]
{chat_history}

[ì§ˆë¬¸ ìƒì„± ì§€ì¹¨]
1. ë°˜ë“œì‹œ ë‹¤ìŒ ì§ˆë¬¸ 1ê°œë§Œ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
2. ì§€ì›ìì˜ ë‹µë³€ ë‚´ìš©ì—ì„œ ê¸°ìˆ ì  í‚¤ì›Œë“œë‚˜ ê²½í—˜ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•˜ì‹­ì‹œì˜¤.
3. ì§ˆë¬¸ ëì—ëŠ” "..." ê°™ì€ íŠ¹ìˆ˜ë¬¸ìë¥¼ ë‚¨ë°œí•˜ì§€ ë§ê³  ì •ì¤‘í•œ ë§ˆì¹¨í‘œë‚˜ ë¬¼ìŒí‘œë¡œ ëë‚´ì‹­ì‹œì˜¤.
4. ê¸¸ì´ëŠ” 150ì ì´ë‚´ë¡œ í•µì‹¬ë§Œ ì°Œë¥´ì‹­ì‹œì˜¤.
5. {stage_display} ì„±ê²©ì— ë§ëŠ” ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

ì§ˆë¬¸:"""

# ==========================================
# 3. Main Task: Generate Question
# ==========================================

# ğŸ”¹ Celery task ë“±ë¡
# â†’ ë¹„ë™ê¸° ì›Œì»¤ì—ì„œ ì‹¤í–‰ë¨
@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    """
    [Role]
    Determine interview progress and generate the next AI question.
    """

    # ğŸ”¹ ë‚´ë¶€ import (circular import ë°©ì§€ + worker startup ì†ë„ ê°œì„ )
    from db import (
        engine,
        Session,
        select,
        Interview,
        Transcript,
        Speaker,
        Question,
        save_generated_question
    )
    from utils.exaone_llm import get_exaone_llm
    from tasks.rag_retrieval import retrieve_context

    logger.info(f"ğŸš€ [START] Generating next question for Interview {interview_id}")

    # ğŸ”¹ DB ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    # with ë¸”ë¡ ì¢…ë£Œ ì‹œ ìë™ close
    with Session(engine) as session:

        # ======================================
        # 1. Load interview
        # ======================================

        # session.get(Model, pk)
        interview = session.get(Interview, interview_id)

        if not interview:
            logger.error(f"Interview {interview_id} not found")
            return {"status": "error", "message": "Interview not found"}

        # ======================================
        # 2. Load transcripts (latest first)
        # ======================================

        # ğŸ”¹ SQLModel select
        stmt = (
            select(Transcript)
            .where(Transcript.interview_id == interview_id)
            .order_by(Transcript.order.desc())  # ìµœì‹ ìˆœ
        )

        transcripts = session.exec(stmt).all()

        if not transcripts:
            logger.warning(
                f"No transcripts found for interview {interview_id}. "
                "Logic might need initial setup."
            )
            return {"status": "error", "message": "No transcripts found"}

        last_transcript = transcripts[0]

        # ======================================
        # 3. Duplicate generation guard
        # ======================================

        # ğŸ”¹ ë§ˆì§€ë§‰ í™”ìê°€ AIë©´
        if last_transcript.speaker == "AI":
            diff = (datetime.utcnow() - last_transcript.timestamp).total_seconds()

            # ğŸ”¹ 10ì´ˆ ì´ë‚´ ì¬ìš”ì²­ ì°¨ë‹¨
            if diff < 10:
                logger.info(f"Skipping: Last AI message was just {diff:.1f}s ago.")
                return {"status": "skipped", "reason": "too_soon"}

        # ======================================
        # 4. Decide next stage (Scenario logic)
        # ======================================

        from utils.interview_helpers import check_if_transition, get_candidate_info

        # ğŸ”¹ ì§€ì›ì ì •ë³´ ì¡°íšŒ
        cand_info = get_candidate_info(session, interview.resume_id)

        # ğŸ”¹ ì§ë¬´ ì „í™˜ì ì—¬ë¶€ íŒë‹¨
        is_transition = check_if_transition(
            cand_info.get("major", ""),
            interview.position
        )

        # ğŸ”¹ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ê¸°
        if is_transition:
            import config.interview_scenario_transition as scenario
        else:
            import config.interview_scenario as scenario

        # ======================================
        # Find last AI stage
        # ======================================

        # ğŸ”¹ generator expression + next()
        # ì¡°ê±´ ë§ëŠ” ì²« ìš”ì†Œ ë°˜í™˜
        last_ai_transcript = next(
            (t for t in transcripts if t.speaker == "AI"),
            None
        )

        # ê¸°ë³¸ ë‹¨ê³„
        last_stage_name = "motivation"

        # ğŸ”¹ ë§ˆì§€ë§‰ AI ì§ˆë¬¸ì˜ stage ì¶”ì 
        if last_ai_transcript and last_ai_transcript.question_id:
            q = session.get(Question, last_ai_transcript.question_id)
            if q:
                last_stage_name = q.question_type or "motivation"

        # ğŸ”¹ ë‹¤ìŒ ë‹¨ê³„ ê³„ì‚°
        next_stage_data = scenario.get_next_stage(last_stage_name)

        # ======================================
        # Interview finished
        # ======================================

        if not next_stage_data:
            logger.info(
                f"Interview {interview_id} finished (No more stages). "
                "Status -> COMPLETED"
            )
            interview.status = "COMPLETED"
            session.add(interview)
            session.commit()
            return {"status": "completed"}

        stage_name = next_stage_data["stage"]
        stage_display = next_stage_data.get("display_name", "ì‹¬ì¸µ ë©´ì ‘")
        stage_guide = next_stage_data.get("guide", "ì§€ì›ìì˜ ì—­ëŸ‰ì„ ê²€ì¦í•˜ì‹­ì‹œì˜¤.")

        logger.info(f"Target Stage: {stage_name} ({stage_display})")

        # ======================================
        # 5. RAG context retrieval
        # ======================================

        rag_context = ""

        # ğŸ”¹ íŠ¹ì • íƒ€ì…ì¼ ë•Œë§Œ RAG ìˆ˜í–‰
        if next_stage_data.get("type") in ("ai", "followup"):

            # query_template.format(...)
            query = next_stage_data.get(
                "query_template",
                interview.position
            ).format(target_role=interview.position)

            # ğŸ”¹ ë²¡í„° ê²€ìƒ‰
            search_results = retrieve_context(
                query,
                resume_id=interview.resume_id,
                top_k=5
            )

            # ğŸ”¹ ë¦¬ìŠ¤íŠ¸ â†’ ë¬¸ìì—´ ê²°í•©
            rag_context = "\n".join([r['text'] for r in search_results])

        # ======================================
        # 6. Recent chat summary
        # ======================================

        chat_limit = 5

        # transcripts[:chat_limit] â†’ ìµœì‹  5ê°œ
        # [::-1] â†’ ì‹œê°„ ìˆœ ì •ë ¬
        recent_chats = transcripts[:chat_limit][::-1]

        chat_history_str = "\n".join(
            [f"{t.speaker}: {t.text}" for t in recent_chats]
        )

        # ======================================
        # 7. LLM call (LCEL)
        # ======================================

        try:
            llm = get_exaone_llm()

            prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)

            output_parser = StrOutputParser()

            # ğŸ”¹ LCEL chain êµ¬ì„±
            # prompt â†’ llm â†’ parser
            chain = prompt | llm | output_parser

            # ğŸ”¹ ì²´ì¸ ì‹¤í–‰
            final_content = chain.invoke({
                "position": interview.position,
                "name": cand_info.get("candidate_name", "ì§€ì›ì"),
                "stage_display": stage_display,
                "guide": stage_guide,
                "context": (
                    rag_context
                    if rag_context
                    else "ì´ë ¥ì„œ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§ë¬´ ì§€ì‹ì„ ë¬¼ì–´ë³´ì‹­ì‹œì˜¤."
                ),
                "chat_history": chat_history_str
            })

            # ======================================
            # Post-processing
            # ======================================

            # ğŸ”¹ ì´ë¯¸ [Stage] ë¶™ì–´ ìˆìœ¼ë©´ ì¤‘ë³µ ë°©ì§€
            if not final_content.startswith('['):
                intro_msg = next_stage_data.get("intro_sentence", "")

                final_content = (
                    f"[{stage_display}] {intro_msg} {final_content}"
                    if intro_msg
                    else f"[{stage_display}] {final_content}"
                )

            # ======================================
            # 8. Save result
            # ======================================

            save_generated_question(
                interview_id=interview_id,
                content=final_content,
                category=next_stage_data.get("category", "general"),
                stage=stage_name,
                guide=stage_guide
            )

            logger.info(
                f"âœ… Successfully generated and saved next question "
                f"for Interview {interview_id}"
            )

            # ======================================
            # Memory cleanup
            # ======================================

            gc.collect()

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            return {"status": "success", "stage": stage_name}

        except Exception as llm_err:
            logger.error(f"âŒ LLM generation failed: {llm_err}")
            return {"status": "error", "message": "LLM failed"}
