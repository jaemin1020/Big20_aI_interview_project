import os
import logging
from celery import shared_task
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from typing import Optional, List
import torch
import re

# DB í—¬í¼ í•¨ìˆ˜ import
from db import engine
from sqlmodel import Session, select

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ì„¤ì •
MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"

class QuestionGenerator:
    """
    RAG ê¸°ë°˜ ì‹¬ì¸µ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸° (EXAONE-3.5-7.8B-Instruct ì‚¬ìš©)
    ë©´ì ‘ ë‹¨ê³„ë³„ ì‹œë‚˜ë¦¬ì˜¤(Plan)ì— ë”°ë¼ ì´ë ¥ì„œ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì§ˆë¬¸ ìƒì„±
    
    Attributes:
        _instance (QuestionGenerator): ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
        _initialized (bool): ì´ˆê¸°í™” ì—¬ë¶€
        llm (ExaoneLLM): EXAONE LLM ì¸ìŠ¤í„´ìŠ¤
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        logger.info("Initializing RAG Question Generator")
        self.llm = get_exaone_llm()
        self._initialized = True
        logger.info("âœ… Question Generator Initialized")

    def generate_questions(self, position: str, interview_id: Optional[int] = None, count: int = 5) -> List[str]:
        """
        ë©´ì ‘ ë‹¨ê³„ë³„ RAG ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
        
        Args:
            position: ì§€ì› ì§ë¬´
            interview_id: ë©´ì ‘ ID
            count: ìƒì„±í•  ì´ ì§ˆë¬¸ ìˆ˜
        """
        questions = []
        
        # 1. ì´ë ¥ì„œ ID ë° ì§€ì›ì ì •ë³´ ì¡°íšŒ
        resume_id = None
        candidate_name = "ì§€ì›ì"
        
        if interview_id:
            try:
                with Session(engine) as session:
                    # Circular import ë°©ì§€ë¥¼ ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ import
                    from db import Interview, Resume
                    interview = session.get(Interview, interview_id)
                    if interview and interview.resume_id:
                        resume_id = interview.resume_id
                        resume = session.get(Resume, resume_id)
                        # ì´ë ¥ì„œì—ì„œ ì§€ì›ì ì´ë¦„ ì¶”ì¶œ ì‹œë„ (í—¤ë” ì •ë³´ ë“±)
                        if resume and resume.structured_data:
                            candidate_name = resume.structured_data.get("target_company", {}).get("name", "ì§€ì›ì")
                            # structured_data êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                            if isinstance(resume.structured_data.get("header"), dict):
                                candidate_name = resume.structured_data["header"].get("name", "ì§€ì›ì")
            except Exception as e:
                logger.warning(f"ì´ë ¥ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # 2. RAG ë¶ˆê°€ëŠ¥ ì‹œ ê¸°ë³¸ ìƒì„± ë¡œì§ìœ¼ë¡œ Fallback
        if not resume_id:
            logger.info("Resume ID not found. Using generic generation.")
            return self.llm.generate_questions(position=position, count=count)
        
        # 3. ë©´ì ‘ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜ (Step 8 ê¸°ë°˜)
        interview_plan = [
            {
                "stage": "1. ì§ë¬´ ì§€ì‹ í‰ê°€",
                "search_query": f"{position} í•µì‹¬ ê¸°ìˆ  ìŠ¤í‚¬ ë„êµ¬ ì›ë¦¬",
                "filter_category": "metric", # ìê²©ì¦/ìŠ¤í‚¬
                "guide": "ì§€ì›ìê°€ ì‚¬ìš©í•œ ê¸°ìˆ (Tool, Language)ì˜ êµ¬ì²´ì ì¸ ì„¤ì •ë²•ì´ë‚˜, ê¸°ìˆ ì  ì›ë¦¬(Deep Dive)ë¥¼ ë¬¼ì–´ë³¼ ê²ƒ."
            },
            {
                "stage": "2. ì§ë¬´ ê²½í—˜ í‰ê°€",
                "search_query": "í”„ë¡œì íŠ¸ ì„±ê³¼ ë‹¬ì„± ë¬¸ì œí•´ê²°",
                "filter_category": "project",
                "guide": "í”„ë¡œì íŠ¸ì—ì„œ ë‹¬ì„±í•œ ìˆ˜ì¹˜ì  ì„±ê³¼(%)ì˜ ê²°ì •ì  ìš”ì¸ì´ ë¬´ì—‡ì¸ì§€, êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë°ì´í„°ë¥¼ ë‹¤ë¤˜ëŠ”ì§€ ë¬¼ì–´ë³¼ ê²ƒ."
            },
            {
                "stage": "3. ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ í‰ê°€",
                "search_query": "ê¸°ìˆ ì  ë‚œê´€ ê·¹ë³µ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…",
                "filter_category": "project",
                "guide": "ì§ë©´í•œ í•œê³„ì ì´ë‚˜ ë¬¸ì œ ìƒí™©ì„ ì–´ë–»ê²Œ ì •ì˜í–ˆê³ , ì–´ë–¤ ë…¼ë¦¬ì  ì‚¬ê³  ê³¼ì •ì„ í†µí•´ í•´ê²°ì±…ì„ ë„ì¶œí–ˆëŠ”ì§€ ë¬¼ì–´ë³¼ ê²ƒ."
            },
            {
                "stage": "4. ì˜ì‚¬ì†Œí†µ ë° í˜‘ì—… í‰ê°€",
                "search_query": "í˜‘ì—… ê°ˆë“± í•´ê²° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
                "filter_category": "narrative",
                "guide": "íŒ€ì›ê³¼ì˜ ì˜ê²¬ ëŒ€ë¦½ ìƒí™©ì—ì„œ ë³¸ì¸ì˜ ì£¼ì¥ì„ ê´€ì² ì‹œí‚¤ê¸° ìœ„í•´ ì–´ë–¤ ê°ê´€ì  ê·¼ê±°ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ ëŒ€í™” ê³¼ì •ì„ ë¬¼ì–´ë³¼ ê²ƒ."
            },
            {
                "stage": "5. ì§ë¬´ ì í•©ì„± ë° ì„±ì¥ ê°€ëŠ¥ì„±",
                "search_query": f"{position} íŠ¸ë Œë“œ ì„±ì¥ ê³„íš",
                "filter_category": "narrative",
                "guide": "ì§ë¬´ì™€ ê´€ë ¨ëœ ìµœì‹  íŠ¸ë Œë“œë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•˜ê³  ìˆìœ¼ë©°, ì´ë¥¼ ì‹¤ë¬´ì— ì–´ë–»ê²Œ ì ìš©í•  ê²ƒì¸ì§€ ë¬¼ì–´ë³¼ ê²ƒ."
            }
        ]
        
        # 4. ì‹œë‚˜ë¦¬ì˜¤ ë°˜ë³µí•˜ë©° ì§ˆë¬¸ ìƒì„±
        # countê°€ planë³´ë‹¤ í¬ë©´ planì„ ë°˜ë³µ, ì‘ìœ¼ë©´ ì•ì—ì„œë¶€í„° ìë¦„
        generated_count = 0
        plan_idx = 0
        
        while generated_count < count:
            step = interview_plan[plan_idx % len(interview_plan)]
            plan_idx += 1
            
            # RAG ê²€ìƒ‰
            contexts = self._retrieve_context(
                resume_id=resume_id,
                query=step['search_query'],
                filter_category=step['filter_category'],
                top_k=2
            )
            
            # ì§ˆë¬¸ ìƒì„±
            if contexts:
                q = self.llm.generate_human_like_question(
                    name=candidate_name,
                    stage=step['stage'],
                    guide=step['guide'] + f" (ì§€ì› ì§ë¬´: {position})",
                    context_list=contexts
                )
                if q not in questions: # ì¤‘ë³µ ë°©ì§€
                    questions.append(q)
                    generated_count += 1
            else:
                # ì»¨í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ Fallback ì§ˆë¬¸ í•˜ë‚˜ ì¶”ê°€
                logger.info(f"ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ: {step['stage']}")
                # ê·¸ëƒ¥ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ê±°ë‚˜ ê¸°ë³¸ ì§ˆë¬¸ ì¶”ê°€
                # ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰ (while loop)
                # ë¬´í•œ ë£¨í”„ ë°©ì§€: ì‹œë„ë¥¼ ë„ˆë¬´ ë§ì´ í•˜ë©´ ì¤‘ë‹¨
                if plan_idx > count * 3:
                     break

        # ë¶€ì¡±ë¶„ ì±„ìš°ê¸°
        if len(questions) < count:
             fallback = self.llm._get_fallback_questions(position, count - len(questions))
             questions.extend(fallback)
             
        return questions[:count]

    def _retrieve_context(self, resume_id: int, query: str, filter_category: str, top_k: int = 2) -> List[Dict]:
        """ë‚´ë¶€ RAG ê²€ìƒ‰ ë¡œì§"""
        try:
            from db import ResumeSectionEmbedding, ResumeSectionType
            from utils.vector_utils import get_embedding_generator
            
            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_map = {
                "metric": [ResumeSectionType.CERTIFICATION, ResumeSectionType.SKILL, ResumeSectionType.LANGUAGE, ResumeSectionType.EDUCATION],
                "project": [ResumeSectionType.PROJECT, ResumeSectionType.EXPERIENCE],
                "narrative": [ResumeSectionType.SELF_INTRODUCTION]
            }
            target_types = category_map.get(filter_category)

            # ì„ë² ë”©
            generator = get_embedding_generator()
            query_vector = generator.encode_query(query)

            # ê²€ìƒ‰
            with Session(engine) as session:
                dist_expr = ResumeSectionEmbedding.embedding.cosine_distance(query_vector)
                stmt = select(ResumeSectionEmbedding, dist_expr.label("distance")).where(
                    ResumeSectionEmbedding.resume_id == resume_id,
                    ResumeSectionEmbedding.embedding.isnot(None)
                )
                if target_types:
                    stmt = stmt.where(ResumeSectionEmbedding.section_type.in_(target_types))
                
                stmt = stmt.order_by(dist_expr).limit(top_k)
                rows = session.exec(stmt).all()
                
                return [{"text": row[0].content, "similarity": 1 - (row[1]/2)} for row in rows]
                
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _generate_new_questions(self, position: str, count: int, examples: list, context: str = ""):
        """LLMìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ ìƒì„± (Few-Shot + Context)"""
        
        
        # Few-Shot ì˜ˆì‹œ êµ¬ì„± (ì˜ˆì‹œê°€ ì—†ìœ¼ë©´ ê°•ë ¥í•œ í•œêµ­ì–´ ê¸°ë³¸ ì˜ˆì‹œ ì œê³µ)
        if examples:
            few_shot_examples = "\n".join([f"- {q}" for q in examples[:3]])
        else:
            few_shot_examples = """
- Reactì˜ Virtual DOMì´ ë¬´ì—‡ì´ë©°, ì´ê²ƒì´ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì—ì„œ Promiseì™€ async/awaitì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
- ì‚¬ìš©í•´ë³¸ ìƒíƒœ ê´€ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë¬´ì—‡ì´ë©°, ê·¸ ì„ íƒ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
"""
        
        # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        context_section = f"\n\nì¶”ê°€ ì»¨í…ìŠ¤íŠ¸:\n{context}" if context else ""
        
        # ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°
        prompt = [{'role':'system','content':
        (f"""
        ë‹¹ì‹ ì€ í•œêµ­ ê¸°ì—…ì˜ ë©´ì ‘ê´€ì´ì ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {position} ì§ë¬´ì— ì í•©í•œ 'í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸'ì„ {count}ê°œ ìƒì„±í•˜ì„¸ìš”.
        {context_section}
        
        ê¸°ì¡´ ì§ˆë¬¸ ì˜ˆì‹œ:
        {few_shot_examples}
        
        [ì¤‘ìš” ìš”êµ¬ì‚¬í•­]
        1. ëª¨ë“  ì§ˆë¬¸ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ì–´, íƒœêµ­ì–´ ë“± íƒ€ ì–¸ì–´ í˜¼ìš© ê¸ˆì§€)
        2. ê¸°ìˆ ì  ê¹Šì´ì™€ ì‹¤ë¬´ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³´ì„¸ìš”.
        3. ì§€ì›ìì˜ ì´ë ¥ì„œ ë‚´ìš©ê³¼ ì—°ê´€ëœ ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”. (ì´ë ¥ì„œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        4. íšŒì‚¬ì˜ ì¸ì¬ìƒê³¼ ì—°ê²°ëœ ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”. (íšŒì‚¬ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        5. ê° ì§ˆë¬¸ì€ ë²ˆí˜¸ ì—†ì´ í•œ ì¤„ì”©ë§Œ ì‘ì„±í•˜ì„¸ìš”.
        6. ì§ˆë¬¸ì˜ ì–´ì¡°ëŠ” ì •ì¤‘í•˜ê³  ì „ë¬¸ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        7. ê°•ì¡° í‘œì‹œ(**text**) ê¸ˆì§€
        """)}]
        
        try:
            # Llama 3.2 ëª¨ë¸ì„ ìœ„í•œ ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            prompt_str = self.tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)
            
            # ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•´ ë” ê¸´ í† í° í—ˆìš© (return_full_text=False ì„¤ì • ë•ë¶„ì— prompt_strì€ ì œì™¸ë¨)
            response = self.llm.invoke(prompt_str)
            
            # ì‘ë‹µ íŒŒì‹±
            
            # 1. íŠ¹ìˆ˜ í† í° ë° ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œê±° íŒ¨í„´
            system_patterns = [
                r"<\|.*?\|>",  # íŠ¹ìˆ˜ í† í°
                r"Cutting Knowledge Date",
                r"Today Date",
                r"^system$", # í—¤ë” ì”ì—¬ë¬¼
                r"^user$",
                r"^assistant$",
                r"ë‹¹ì‹ ì€ ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤", # í”„ë¡¬í”„íŠ¸ ì—ì½” ë°©ì§€
                r"ìš”êµ¬ì‚¬í•­:",
                r"ê¸°ì¡´ ì§ˆë¬¸ ì˜ˆì‹œ:",
                r"ì§ˆë¬¸ \d+ê°œ:"
            ]
            
            clean_lines = []
            for line in response.split('\n'):
                line = line.strip()
                if not line:
                    continue
                    
                # ì‹œìŠ¤í…œ ë©”ì‹œì§€ íŒ¨í„´ì´ í¬í•¨ëœ ë¼ì¸ ê±´ë„ˆë›°ê¸°
                if any(re.search(pat, line) for pat in system_patterns):
                    continue
                
                # í”„ë¡¬í”„íŠ¸ì˜ ì§€ì‹œì‚¬í•­ ë¬¸ì¥ê³¼ ìœ ì‚¬í•˜ë©´ ê±´ë„ˆë›°ê¸° (Echo ë°©ì§€ 2ì°¨ í•„í„°)
                if "í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸" in line or "ì´ë ¥ì„œ ë‚´ìš©ê³¼ ì—°ê´€" in line or "í•œ ì¤„ë¡œ ì‘ì„±" in line:
                    continue

                # #ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì„ ë¼ì¸ ê±´ë„ˆë›°ê¸°
                if line.startswith('#'):
                    continue
                    
                clean_lines.append(line)

            # 2. ì§ˆë¬¸ ì¶”ì¶œ ë° ì •ì œ
            questions = []
            for line in clean_lines:
                # ë²ˆí˜¸ ì œê±° (ì˜ˆ: "1. ì§ˆë¬¸" -> "ì§ˆë¬¸", "- ì§ˆë¬¸" -> "ì§ˆë¬¸")
                clean_q = re.sub(r'^[\d\-\.\s]+', '', line)
                
                # Markdown ê°•ì¡° ì œê±° (**text** -> text)
                clean_q = re.sub(r'\*\*(.*?)\*\*', r'\1', clean_q)
                
                # ì•ë’¤ ë”°ì˜´í‘œ ë° ê³µë°± ì œê±°
                clean_q = clean_q.strip('"\' ')
                
                # [í•„í„°ë§ ê°œì„ ] Whitelist ë°©ì‹ì€ ë„ˆë¬´ ì—„ê²©í•˜ì—¬ Blacklist ë°©ì‹ìœ¼ë¡œ ë³€ê²½
                # ì¼ë³¸ì–´(íˆë¼ê°€ë‚˜/ê°€íƒ€ì¹´ë‚˜), í•œì, íƒœêµ­ì–´ ë“±ì´ í¬í•¨ëœ ê²½ìš°ë§Œ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” í—ˆìš©
                # ê¸°ìˆ  ë©´ì ‘ ì§ˆë¬¸ì—ëŠ” ë‹¤ì–‘í•œ íŠ¹ìˆ˜ë¬¸ì(@, #, &, [] ë“±)ê°€ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
                forbidden_pattern = r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\u0E00-\u0E7F]'
                if re.search(forbidden_pattern, clean_q):
                    logger.warning(f"ì œì™¸ëœ ì§ˆë¬¸(ë‹¤êµ­ì–´ í¬í•¨): {clean_q}")
                    continue
                
                # ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì€ ê²ƒì€ ì§ˆë¬¸ì´ ì•„ë‹ í™•ë¥  ë†’ìŒ (10ì ì´ìƒ)
                if len(clean_q) > 10:
                    questions.append(clean_q)
            
            # ë§Œì•½ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ Fallback ì§ˆë¬¸ìœ¼ë¡œ ì±„ì›€
            if len(questions) < count:
                logger.warning(f"ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜ ë¶€ì¡± ({len(questions)}/{count}). Fallbackìœ¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.")
                fallback_needed = count - len(questions)
                fallbacks = self._get_fallback_questions(position, fallback_needed)
                questions.extend(fallbacks)
                
            logger.info(f"ìµœì¢… ë°˜í™˜ ì§ˆë¬¸: {questions[:count]}")
            return questions[:count]
        except Exception as e:
            logger.error(f"LLM ì§ˆë¬¸ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë³´ë‹¨ Fallback ë¦¬í„´
            return self._get_fallback_questions(position, count)
    
    def _get_fallback_questions(self, position: str, count: int) -> List[str]:
        """í´ë°± ì§ˆë¬¸ ìƒì„±"""
        fallback_questions = [
            f"{position} ì§ë¬´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ì—­ëŸ‰ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ìµœê·¼ ê²ªì—ˆë˜ ê°€ì¥ ì–´ë ¤ìš´ ê¸°ìˆ ì  ì±Œë¦°ì§€ëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?",
            f"{position} ì§ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° í•„ìš”í•œ í•µì‹¬ ê¸°ìˆ ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ë‚˜ìš”?",
            "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ì˜ê²¬ ì¶©ëŒì´ ìˆì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?",
            "ë³¸ì¸ì˜ ê°•ì ì„ ì‹¤ë¬´ì—ì„œ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?",
            "ìš°ë¦¬ íšŒì‚¬ì— ì§€ì›í•œ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "5ë…„ í›„ ë³¸ì¸ì˜ ëª¨ìŠµì„ ì–´ë–»ê²Œ ê·¸ë¦¬ê³  ê³„ì‹ ê°€ìš”?",
            "ì‹¤íŒ¨í•œ í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ ê·¸ë¡œë¶€í„° ë°°ìš´ ì ì„ ê³µìœ í•´ì£¼ì„¸ìš”."
        ]
        return fallback_questions[:count]

@shared_task(name="tasks.question_generator.generate_questions")
def generate_questions_task(position: str, interview_id: int = None, count: int = 5):
    """
    ì§ˆë¬¸ ìƒì„± Task (TTS í¬í•¨)
    
    Args:
        position (str): ì§€ì› ì§ë¬´
        interview_id (int, optional): ë©´ì ‘ ID. Defaults to None.
        count (int, optional): ìƒì„±í•  ì§ˆë¬¸ ìˆ˜. Defaults to 5.
    
    Returns:
        List[Dict[str, str]]: ìƒì„±ëœ ì§ˆë¬¸ ë° ì˜¤ë””ì˜¤ URL ë¦¬ìŠ¤íŠ¸ 
        Example: [{"text": "ì§ˆë¬¸ë‚´ìš©", "audio_url": "/static/audio/..."}]
    
    Raises:
        Exception: ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨
    
    ìƒì„±ì: ejm
    ìƒì„±ì¼ì: 2026-02-04
    """
    import uuid
    from .tts import tts_engine, load_tts_engine

    try:
        generator = QuestionGenerator()
        questions = generator.generate_questions(position, interview_id, count)
        
        # TTS Integration
        results = []
        
        # Ensure TTS engine is ready
        if tts_engine is None or tts_engine.tts is None:
            load_tts_engine()

        # Save directory inside container
        save_dir = "/app/uploads/audio"
        os.makedirs(save_dir, exist_ok=True)
            
        for q in questions:
            audio_url = None
            if tts_engine and tts_engine.tts:
                try:
                    filename = f"q_{uuid.uuid4().hex}.wav"
                    filepath = os.path.join(save_dir, filename)
                    
                    if tts_engine.synthesize(q, filepath):
                        # URL path for backend (mounted as /static)
                        audio_url = f"/static/audio/{filename}"
                except Exception as ex:
                    logger.error(f"TTS failing for question '{q[:20]}...': {ex}")
            
            results.append({"text": q, "audio_url": audio_url})
            
        return results

    except Exception as e:
        logger.error(f"Task Error: {e}")
        return []

# Eager Initialization: Worker ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
try:
    logger.info("ğŸ”¥ Pre-loading Question Generator with EXAONE...")
    _warmup_generator = QuestionGenerator()
    logger.info("âœ… Question Generator ready for requests")
except Exception as e:
    logger.warning(f"âš ï¸ Failed to pre-load model (will load on first request): {e}")

