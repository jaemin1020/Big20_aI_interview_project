import sys
import os
import re
import json
import gc 
import logging
import torch
from datetime import datetime
from celery import shared_task
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ==========================================
# 1. ì´ˆê¸° ì„¤ì • ë° ëª¨ë¸ ê²½ë¡œ ìµœì í™”
# ==========================================

if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
model_path = docker_path if os.path.exists(docker_path) else local_path

# ==========================================
# 2. í˜ë¥´ì†Œë‚˜ ì„¤ì • (Prompt Engineering)
# ==========================================

PROMPT_TEMPLATE = """[|system|]ë‹¹ì‹ ì€ ì§€ì›ìì˜ ì—­ëŸ‰ì„ ì •ë°€í•˜ê²Œ ê²€ì¦í•˜ëŠ” ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
ì œê³µëœ [ì´ë ¥ì„œ ë¬¸ë§¥]ê³¼ [ë©´ì ‘ ì§„í–‰ ìƒí™©]ì„ ë°”íƒ•ìœ¼ë¡œ, ì§€ì›ìì—ê²Œ ë˜ì§ˆ 'ë‹¤ìŒ ì§ˆë¬¸' 1ê°œë§Œ ìƒì„±í•˜ì‹­ì‹œì˜¤.

[ì ˆëŒ€ ê·œì¹™]
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
2. ì§ˆë¬¸ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, 150ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
3. íŠ¹ìˆ˜ë¬¸ì(JSON ê¸°í˜¸, ì—­ë”°ì˜´í‘œ ë“±)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì˜¤ì§ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
4. "ì§ˆë¬¸:" ì´ë¼ëŠ” ìˆ˜ì‹ì–´ ì—†ì´ ë°”ë¡œ ì§ˆë¬¸ ë³¸ë¬¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
5. ì´ì „ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ í•˜ì‹­ì‹œì˜¤.
6. **í™˜ê° ì£¼ì˜**: ì´ë ¥ì„œì— "ìµíˆê³  ì‹¶ë‹¤", "ê³µë¶€í•  ê³„íšì´ë‹¤", "ì„±ì¥í•˜ê² ë‹¤" ë“± ë¯¸ë˜ í¬ë¶€ë¡œ ì íŒ ë‚´ìš©ì€ ì‹¤ì œ ì‹¤ë¬´ ê²½í—˜ì´ë‚˜ í”„ë¡œì íŠ¸ ì„±ê³¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ì§ˆë¬¸í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ë¯¸ë˜ í¬ë¶€ëŠ” í•™ìŠµ ì˜ì§€ë‚˜ ê´€ì‹¬ë„ë¥¼ ë¬»ëŠ” ìš©ë„ë¡œë§Œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.

[ì´ë ¥ì„œ ë° ë‹µë³€ ë¬¸ë§¥]
{context}

[í˜„ì¬ ë©´ì ‘ ë‹¨ê³„ ì •ë³´]
- ë‹¨ê³„ëª…: {stage_name}
- ê°€ì´ë“œ: {guide}

[|user|]ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.[|endofturn|]
[|assistant|]"""

# ==========================================
# 3. ë©”ì¸ ì‘ì—…: ì§ˆë¬¸ ìƒì„± íƒœìŠ¤í¬
# ==========================================

@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    """
    ì¸í„°ë·° ì§„í–‰ ìƒí™©ì„ íŒŒì•…í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ì˜ AI ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    from db import engine, Session, select, Interview, Transcript, Speaker, Question, save_generated_question
    from utils.exaone_llm import get_exaone_llm
    from tasks.tts import synthesize_task
    from utils.interview_helpers import check_if_transition
    from config.interview_scenario import get_next_stage as get_next_stage_normal
    from config.interview_scenario_transition import get_next_stage as get_next_stage_transition
    from tasks.rag_retrieval import retrieve_context
    try:
        with Session(engine) as session:
            interview = session.get(Interview, interview_id)
            if not interview: 
                logger.error(f"Interview {interview_id} not found.")
                return {"status": "error", "message": "Interview not found"}

            # 2. ë§ˆì§€ë§‰ AI ë°œí™” í™•ì¸ (Stage íŒë³„ + ì¤‘ë³µ ë°©ì§€)
            # [ìˆ˜ì •] User transcriptëŠ” question_idê°€ ì—†ì–´ stage íŒë³„ ë¶ˆê°€ â†’ ë§ˆì§€ë§‰ AI ë°œí™” ê¸°ì¤€ìœ¼ë¡œ íŒë³„
            stmt_all = select(Transcript).where(Transcript.interview_id == interview_id).order_by(Transcript.order.desc())
            last_transcript = session.exec(stmt_all).first()

            stmt_ai = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.AI
            ).order_by(Transcript.order.desc(), Transcript.id.desc())  # idë¥¼ tiebreakerë¡œ ì‚¬ìš© (order ê°™ì„ ë•Œ ìµœì‹  AI ë°œí™” ë³´ì¥)
            last_ai_transcript = session.exec(stmt_ai).first()

            # ë§ˆì§€ë§‰ AI ë°œí™”ê°€ 10ì´ˆ ì´ë‚´ë¼ë©´ ìŠ¤í‚µ (Race Condition ë°©ì§€)
            if last_ai_transcript:
                diff = (datetime.utcnow() - last_ai_transcript.timestamp).total_seconds()
                if diff < 10:
                    logger.info(f"Skipping duplicate request for interview {interview_id}")
                    return {"status": "skipped"}

            # [ìˆ˜ì •] 3. ì „ê³µ/ì§ë¬´ ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ê²°ì •
            major = ""
            if interview.resume and interview.resume.structured_data:
                sd = interview.resume.structured_data
                if isinstance(sd, str):
                    sd = json.loads(sd)
                edu = sd.get("education", [])
                major = next((e.get("major", "") for e in edu if e.get("major", "").strip()), "")

            is_transition = check_if_transition(major, interview.position)
            get_next_stage_func = get_next_stage_transition if is_transition else get_next_stage_normal

            # ë§ˆì§€ë§‰ AI ë°œí™”ì˜ question_typeìœ¼ë¡œ í˜„ì¬ stage íŒë³„
            if last_ai_transcript and last_ai_transcript.question_id:
                last_question = session.get(Question, last_ai_transcript.question_id)
                last_stage_name = last_question.question_type if last_question else "intro"
            else:
                last_stage_name = "intro"

            logger.info(f"Current stage determined: {last_stage_name} (is_transition={is_transition})")
            next_stage = get_next_stage_func(last_stage_name)

            if not next_stage:
                logger.info(f"Interview {interview_id} finished. Transitioning to COMPLETED.")
                interview.status = "COMPLETED"
                session.add(interview)
                session.commit()
                return {"status": "completed"}

            # [ìˆ˜ì •] ê¼¬ë¦¬ì§ˆë¬¸(followup) ìƒì„± ì œí•œ ë¡œì§
            # ë‹¤ìŒ ë‹¨ê³„ê°€ followupì¸ë°, ë§ˆì§€ë§‰ ë°œí™”ìê°€ ì—¬ì „íˆ AIë¼ë©´ ì§€ì›ìê°€ ì•„ì§ ë‹µë³€ì„ ì•ˆ í•œ ê²ƒì„.
            if next_stage.get("type") == "followup":
                if last_transcript and last_transcript.speaker == "AI":
                    logger.info(f"Next stage is followup, but WAITING for user answer. Skipping generation.")
                    return {"status": "waiting_for_user"}

            # [ì¤‘ë³µ ë°©ì§€ ê°œì„ ] next_stageê°€ ì´ë¯¸ ìƒì„±ëëŠ”ì§€ í™•ì¸ (timestamp ê¸°ë°˜ X â†’ stage ê¸°ë°˜ O)
            if last_ai_transcript:
                last_q_for_check = session.get(Question, last_ai_transcript.question_id) if last_ai_transcript.question_id else None
                if last_q_for_check and last_q_for_check.question_type == next_stage['stage']:
                    diff = (datetime.utcnow() - last_ai_transcript.timestamp).total_seconds()
                    if diff < 30:
                        logger.info(f"Next stage '{next_stage['stage']}' already generated {diff:.1f}s ago, skipping duplicate")
                        return {"status": "skipped"}

            # 4. [ìµœì í™”] template stageëŠ” RAG/LLM ì—†ì´ ì¦‰ì‹œ í¬ë§·
            if next_stage.get("type") == "template":
                candidate_name = "ì§€ì›ì"
                target_role = interview.position or "í•´ë‹¹ ì§ë¬´"
                if interview.resume and interview.resume.structured_data:
                    sd = interview.resume.structured_data
                    if isinstance(sd, str):
                        sd = json.loads(sd)
                    candidate_name = sd.get("header", {}).get("name", "ì§€ì›ì")
                    target_role = sd.get("header", {}).get("target_role", target_role)

                template_vars = {"candidate_name": candidate_name, "target_role": target_role, "major": major}
                tpl = next_stage.get("template", "{candidate_name} ì§€ì›ìë‹˜, ê³„ì†í•´ì£¼ì„¸ìš”.")
                try:
                    formatted = tpl.format(**template_vars)
                except KeyError:
                    formatted = tpl

                intro_msg = next_stage.get("intro_sentence", "")
                display_name = next_stage.get("display_name", "ë©´ì ‘ì§ˆë¬¸")
                final_content = f"[{display_name}] {intro_msg} {formatted}".strip() if intro_msg else f"[{display_name}] {formatted}"
                logger.info(f"Template stage '{next_stage['stage']}' â†’ ì¦‰ì‹œ í¬ë§· ì™„ë£Œ (RAG/LLM ìƒëµ)")

            else:
                # 4-b. AI stage: RAGë¡œ ë¬¸ë§¥ í™•ë³´ í›„ LLM ìƒì„±
                query_template = next_stage.get("query_template", interview.position)
                try:
                    query = query_template.format(
                        target_role=interview.position or "í•´ë‹¹ ì§ë¬´",
                        major=major or ""
                    )
                except (KeyError, ValueError):
                    query = query_template 
                
                # [ê°œì„ ] ì¹´í…Œê³ ë¦¬ê°€ 'certification'ì¸ ê²½ìš° ìê²©ì¦(certifications) ì„¹ì…˜ë§Œ ì—„ê²©í•˜ê²Œ í•„í„°ë§
                filter_type = None
                category_raw = next_stage.get("category")
                if category_raw == "certification":
                    filter_type = "certifications"
                    logger.info("ğŸ¯ Category 'certification' detected: Strict filtering for certifications context.")

                rag_results = retrieve_context(query, resume_id=interview.resume_id, top_k=3, filter_type=filter_type)
                
                # ìê²©ì¦ ì¹´í…Œê³ ë¦¬ì¸ë° ê²°ê³¼ê°€ ë§Œì•½ ì—†ë‹¤ë©´ ì „ì²´ ì„¹ì…˜ì—ì„œ ì¬ê²€ìƒ‰
                if not rag_results and filter_type == "certifications":
                    logger.info("âš ï¸ No exact 'certifications' section found, searching globally for certificates.")
                    rag_results = retrieve_context(f"ë³´ìœ  ìê²©ì¦ ìê²© ì·¨ë“ {query}", resume_id=interview.resume_id, top_k=3)

                context_text = "\n".join([r['text'] for r in rag_results]) if rag_results else "íŠ¹ë³„í•œ ì •ë³´ ì—†ìŒ"

                if last_transcript and last_transcript.speaker == "User":
                    context_text += f"\n[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]: {last_transcript.text}"

                llm = get_exaone_llm()
                prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
                chain = prompt | llm | StrOutputParser()

                final_content = chain.invoke({
                    "context": context_text,
                    "stage_name": next_stage['display_name'],
                    "guide": next_stage.get('guide', '')
                })

                # ì¸íŠ¸ë¡œ ë©”ì‹œì§€ ì¡°í•© (3ë²ˆ ì§ˆë¬¸ ì „ìš© ë¡œì§ í¬í•¨)
                candidate_name = "ì§€ì›ì"
                if interview.resume and interview.resume.structured_data:
                    sd = interview.resume.structured_data
                    if isinstance(sd, str): sd = json.loads(sd)
                    candidate_name = sd.get("header", {}).get("name", "ì§€ì›ì")

                intro_tpl = next_stage.get("intro_sentence", "")
                if next_stage['stage'] == 'skill' and 'cert_name' in intro_tpl:
                    # RAG ê²°ê³¼ì—ì„œ ì²« ë²ˆì§¸ ìê²©ì¦ ì´ë¦„ ì¶”ì¶œ ì‹œë„
                    cert_name = "ìë£Œì— ëª…ì‹œëœ"
                    if rag_results:
                        # "[ìê²©ì¦] ìê²©ëª…: XXX" í˜•íƒœì—ì„œ ì´ë¦„ ì¶”ì¶œ
                        match = re.search(r'ìê²©ëª…:\s*([^,\(]+)', rag_results[0]['text'])
                        if match: cert_name = match.group(1).strip()
                    intro_msg = intro_tpl.format(candidate_name=candidate_name, cert_name=cert_name)
                elif intro_tpl:
                    try:
                        intro_msg = intro_tpl.format(candidate_name=candidate_name)
                    except:
                        intro_msg = intro_tpl
                else:
                    intro_msg = ""

                if next_stage.get("type") == "followup":
                    intro_msg = "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìŠµë‹ˆë‹¤."
                
                display_name = next_stage.get("display_name", "ì‹¬ì¸µ ë©´ì ‘")
                final_content = f"[{display_name}] {intro_msg} {final_content}".strip() if intro_msg else f"[{display_name}] {final_content}".strip()

            # 6. DB ì €ì¥ (Question ë° Transcript)

            # 6. DB ì €ì¥ (Question ë° Transcript)
            category_raw = next_stage.get("category", "technical")
            category_map = {"certification": "technical", "project": "technical", "narrative": "behavioral", "problem_solving": "situational"}
            db_category = category_map.get(category_raw, "technical")

            logger.info(f"ğŸ’¾ Saving generated question to DB for Interview {interview_id} (Stage: {next_stage['stage']})")
            q_id = save_generated_question(
                interview_id=interview_id,
                content=final_content,
                category=db_category,
                stage=next_stage['stage'],
                guide=next_stage.get('guide', ''),
                session=session
            )

            # 7. ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            # 8. TTS ìƒì„± íƒœìŠ¤í¬ ì¦‰ì‹œ íŠ¸ë¦¬ê±°
            if q_id:
                logger.info(f"ğŸ”Š Triggering TTS synthesis for Question ID: {q_id}")
                synthesize_task.delay(final_content, language="auto", question_id=q_id)

            return {"status": "success", "stage": next_stage['stage'], "question": final_content}
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ (Retry ì‹œë„): {e}")
        raise self.retry(exc=e, countdown=3)
    finally:
        gc.collect()