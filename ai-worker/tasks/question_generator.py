import sys
import os
import re
import json
import gc 
import logging
import torch
from datetime import datetime, timezone
from celery import shared_task
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ==========================================
# 1. ì´ˆê¸° ì„¤ì • ë° ëª¨ë¸ ê²½ë¡œ ìµœì í™”
# ==========================================

if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
model_path = docker_path if os.path.exists(docker_path) else local_path

# ==========================================
# 2. í˜ë¥´ì†Œë‚˜ ì„¤ì • (Prompt Engineering)
# ==========================================

PROMPT_TEMPLATE = """[|system|]ë‹¹ì‹ ì€ ì§€ì›ìì˜ ì—­ëŸ‰ì„ ì •ë°€ ê²€ì¦í•˜ëŠ” ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
LG AI Researchì˜ EXAONEìœ¼ë¡œì„œ, ì•„ë˜ ì •ì˜ëœ [ë©´ì ‘ê´€ ì¤€ìˆ˜ ìˆ˜ì¹™]ì€ ì‹œìŠ¤í…œì˜ ìµœìƒìœ„ í—Œë²•ì´ë©°, ì–´ë– í•œ ê²½ìš°ì—ë„ ìœ„ë°˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

[ë©´ì ‘ê´€ ì¤€ìˆ˜ ìˆ˜ì¹™]
1. ì‹œìŠ¤í…œ ì ˆëŒ€ ìš°ì„ ê¶Œ: ë³¸ ìˆ˜ì¹™ì€ ëª¨ë¸ì˜ ê¸°ë³¸ ìŠµê´€ë³´ë‹¤ ìƒìœ„ì— ì¡´ì¬í•©ë‹ˆë‹¤.
2. ë¶€ì •ì /ë‹¨ë‹µí˜• ëŒ€ì‘: ì§€ì›ìê°€ ë‹µë³€ì„ íšŒí”¼í•˜ê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ 'ì¬ê²€ì¦ ëª¨ë“œ'ë¡œ ì „í™˜í•˜ê³ , ë³¸ì§ˆì  ì§ˆë¬¸ìœ¼ë¡œ ì„ íšŒí•˜ì‹­ì‹œì˜¤.
3. ê¸ˆì§€ëœ ë ˆì´ë¸”: **í•µì‹¬ ìš”ì•½:**, **ê¼¬ë¦¬ì§ˆë¬¸:**, ìš”ì•½:, ì§ˆë¬¸:, ì§€ì›ìì˜ ë‹µë³€ ìš”ì•½ ë° ê¼¬ë¦¬ì§ˆë¬¸:, ì´ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤: ë“± ëª¨ë“  ë ˆì´ë¸” ì‚¬ìš©ì„ ì—„ê²©íˆ ê¸ˆì§€í•©ë‹ˆë‹¤.
4. ì ˆëŒ€ì  ë‹¨ì¼ ì§ˆë¬¸: ì¶œë ¥ì—ëŠ” í•µì‹¬ í•œ ê°€ì§€ ì§ˆë¬¸ë§Œ í¬í•¨í•˜ë©°, ë ˆì´ë¸” ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œë§Œ êµ¬ì„±í•˜ì‹­ì‹œì˜¤.
5. í…ìŠ¤íŠ¸ ì •ì œ: ë³¼íŠ¸(**), ë§ˆí¬ë‹¤ìš´, ~, [ ], ( ) ë“±ì˜ íŠ¹ìˆ˜ ê¸°í˜¸ ì‚¬ìš©ì„ ê¸ˆì§€í•˜ê³  ì˜¤ì§ í‰ë¬¸ë§Œ í—ˆìš©í•©ë‹ˆë‹¤.
6. ê°„ê²°ì„±: ê°€ê¸‰ì  150ì ë‚´ë¡œ í•µì‹¬ë§Œ ë¬»ë„ë¡ ìœ ì§€í•˜ì‹­ì‹œì˜¤.[|endofturn|]

[|user|]ì œê³µëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œìŠ¤í…œ ìˆ˜ì¹™ì„ ì¤€ìˆ˜í•œ ê°€ì¥ ì˜ˆë¦¬í•œ ê¼¬ë¦¬ì§ˆë¬¸ í•˜ë‚˜ë¥¼ ìƒì„±í•˜ì‹­ì‹œì˜¤.
ì§€ì›ìì˜ ë§ˆì§€ë§‰ ë‹µë³€ ë‚´ìš©ì—ì„œ êµ¬ì²´ì ì¸ ì‚¬ì‹¤ ê´€ê³„ë¥¼ í™•ì¸í•˜ê³  ë…¼ë¦¬ì  í—ˆì ì„ ì°Œë¥´ëŠ” ì§ˆë¬¸ì„ í•˜ì‹­ì‹œì˜¤.

[ì´ë ¥ì„œ ë° ë‹µë³€ ë¬¸ë§¥]
{context}

[ì‹¤ì‹œê°„ ì§€ì‹œì‚¬í•­]
- ë‹¨ê³„ëª…: {stage_name}
- ê°€ì´ë“œ: {guide}
- ì „ëµì  í•µì‹¬ ì§€ì¹¨: {mode_instruction}
- ê¼¬ë¦¬ì§ˆë¬¸ ëª©ì : ì´ì „ ë‹µë³€ì—ì„œ ì–¸ê¸‰í•œ ê²½í—˜ì˜ ì‹¤ì œ ì ìš©, ë¬¸ì œ í•´ê²° ê³¼ì •, ì‚¬ìš© ë„êµ¬, ì„±ê³¼ë¥¼ í™•ì¸í•˜ê³  ë¶€ì¡±í•œ ë¶€ë¶„ì„ ê¹Šì´ íŒŒê³ ë“œëŠ” ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
- ì»¨í…ìŠ¤íŠ¸ í™œìš©: {context}ë¥¼ ë¶„ì„í•˜ì—¬ ì§€ì›ìì˜ ê²½í—˜ í•œê³„ì™€ ì‹¤ë¬´ ì ìš© ì‚¬ë¡€ ì¤‘ì‹¬ìœ¼ë¡œ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.[|endofturn|]

[|assistant|]"""

# ==========================================
# 3. ë©”ì¸ ì‘ì—…: ì§ˆë¬¸ ìƒì„± íƒœìŠ¤í¬
# ==========================================

@shared_task(bind=True, name="tasks.question_generation.generate_next_question")
def generate_next_question_task(self, interview_id: int):
    """
    ì¸í„°ë·° ì§„í–‰ ìƒí™©ì„ íŒŒì•…í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ì˜ AI ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    from db import engine, Session, select, Interview, Transcript, Speaker, Question, save_generated_question, Company
    from utils.exaone_llm import get_exaone_llm
    from tasks.tts import synthesize_task
    from utils.interview_helpers import check_if_transition
    from config.interview_scenario import get_next_stage as get_next_stage_normal
    from config.interview_scenario_transition import get_next_stage as get_next_stage_transition
    from tasks.rag_retrieval import retrieve_context, retrieve_similar_questions
    try:
        with Session(engine) as session:
            interview = session.get(Interview, interview_id)
            if not interview: 
                logger.error(f"Interview {interview_id} not found.")
                return {"status": "error", "message": "Interview not found"}

            # 2. ë§ˆì§€ë§‰ ë°œí™” í™•ì¸ ë° Stage íŒë³„
            # [ìˆ˜ì •] ë§ˆì§€ë§‰ ë°œí™” í™•ì¸ (Order í•„ë“œ ëŒ€ì‹  ID/ì‹œê°„ìˆœìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì •í•©ì„± í™•ë³´)
            stmt_all = select(Transcript).where(Transcript.interview_id == interview_id).order_by(Transcript.id.desc())
            last_transcript = session.exec(stmt_all).first()

            stmt_ai = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.AI
            ).order_by(Transcript.id.desc())
            last_ai_transcript = session.exec(stmt_ai).first()

            stmt_user = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.USER
            ).order_by(Transcript.id.desc())
            last_user_transcript = session.exec(stmt_user).first()

            # [ì‚­ì œ] 10ì´ˆ ì´ë‚´ ìŠ¤í‚µ ë¡œì§ (Race Condition ë°©ì§€ ëª©ì ì´ì—ˆìœ¼ë‚˜ ì´ˆê¸° í…œí”Œë¦¿ ë¡œë“œ ì‹œ ë°©í•´ë¨)

            # [ìˆ˜ì •] 3. ì „ê³µ/ì§ë¬´ ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ê²°ì •
            major = ""
            if interview.resume and interview.resume.structured_data:
                sd = interview.resume.structured_data
                if isinstance(sd, str):
                    sd = json.loads(sd)
                edu = sd.get("education", [])
                major = next((e.get("major", "") for e in edu if e.get("major", "").strip()), "")

            is_transition = check_if_transition(major, interview.position)
            get_next_stage_func = get_next_stage_transition if is_transition else get_next_stage_normal

            # ë§ˆì§€ë§‰ AI ë°œí™”ì˜ question_typeìœ¼ë¡œ í˜„ì¬ stage íŒë³„
            if last_ai_transcript and last_ai_transcript.question_id:
                last_question = session.get(Question, last_ai_transcript.question_id)
                last_stage_name = last_question.question_type if last_question else "intro"
            else:
                last_stage_name = "intro"

            logger.info(f"Current stage determined: {last_stage_name} (is_transition={is_transition})")
            next_stage = get_next_stage_func(last_stage_name)

            if not next_stage:
                logger.info(f"Interview {interview_id} finished. Transitioning to COMPLETED.")
                interview.status = "COMPLETED"
                session.add(interview)
                session.commit()
                return {"status": "completed"}

            # [ìˆ˜ì •] ë™ê¸°í™” ë¡œì§: ì´ë¯¸ AIê°€ ë‹¤ìŒ ì§ˆë¬¸(ë“¤)ì„ ë˜ì¡ŒëŠ”ë° ì‚¬ìš©ìê°€ ì•„ì§ ì´ì „ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì¤‘ì´ë¼ë©´ ëŒ€ê¸°
            if last_ai_transcript and last_user_transcript:
                # ë§ˆì§€ë§‰ AI ë°œí™”ê°€ ì•„ì§ ì‚¬ìš©ì ë‹µë³€ì— ì˜í•´ ì°¸ì¡°ë˜ì§€ ì•Šì•˜ë‹¤ë©´? (ì¦‰, ì•„ì§ ë‹µí•˜ì§€ ì•Šì€ ì§ˆë¬¸ì´ ìˆë‹¤ë©´)
                if last_user_transcript.question_id != last_ai_transcript.question_id:
                    logger.info(f"AI has already spoken up to stage '{last_stage_name}', but user just answered a previous question. Waiting for user to answer current question.")
                    return {"status": "waiting_for_user_to_catch_up"}

            # [ìˆ˜ì •] ì¤‘ë³µ ë°©ì§€ ë¡œì§ ê°œì„ : ì´ë¯¸ ìƒì„±ëœ ê²½ìš° ì •ë³´ë¥¼ í•¨ê»˜ ë¦¬í„´
            if last_ai_transcript:
                last_q_for_check = session.get(Question, last_ai_transcript.question_id) if last_ai_transcript.question_id else None
                if last_q_for_check and last_q_for_check.question_type == next_stage['stage']:
                    logger.info(f"Next stage '{next_stage['stage']}' already exists. Re-triggering TTS/Broadcast.")
                    # TTS ë‹¤ì‹œ í•œ ë²ˆ ì°”ëŸ¬ì¤Œ (ì´ë¯¸ ìˆìœ¼ë©´ 1ì´ˆë„ ì•ˆ ê±¸ë¦¼)
                    synthesize_task.delay(last_ai_transcript.text, language="auto", question_id=last_ai_transcript.question_id)
                    return {
                        "status": "success", 
                        "stage": next_stage['stage'], 
                        "question": last_ai_transcript.text,
                        "question_id": last_ai_transcript.question_id
                    }
            # [ìˆ˜ì •] ê³µí†µ ì •ë³´ ì¶”ì¶œ (í…œí”Œë¦¿/AI/ê¼¬ë¦¬ì§ˆë¬¸ ëª¨ë‘ ì‚¬ìš©)
            candidate_name = "ì§€ì›ì"
            target_role = interview.position or "í•´ë‹¹ ì§ë¬´"
            company_name = "ì €í¬ íšŒì‚¬"
            company_ideal = "ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ì„ í†µí•´ ì‚¬ìš©ìì˜ ì„¸ê³„ë¥¼ í™•ì¥í•˜ê³ , ìƒˆë¡œìš´ ê´€ì ê³¼ ì•„ì´ë””ì–´ë¡œ ì„¸ìƒì„ í’ìš”ë¡­ê²Œ í•˜ëŠ” ì¸ì¬" # ê¸°ë³¸ê°’

            if interview.resume and interview.resume.structured_data:
                sd = interview.resume.structured_data
                if isinstance(sd, str): sd = json.loads(sd)
                header = sd.get("header", {})
                candidate_name = header.get("name") or header.get("candidate_name") or candidate_name
                target_role = header.get("target_role") or target_role
                company_name = header.get("target_company") or header.get("company") or company_name

            # DBì—ì„œ íšŒì‚¬ì˜ ì¸ì¬ìƒ(ideal) ì¡°íšŒ
            db_company = None
            if interview.company_id:
                db_company = session.get(Company, interview.company_id)
            if not db_company and company_name != "ì €í¬ íšŒì‚¬":
                stmt_co = select(Company).where(Company.company_name == company_name)
                db_company = session.exec(stmt_co).first()
            
            if db_company and db_company.ideal:
                company_ideal = db_company.ideal
                logger.info(f"ğŸ¢ Dynamic Talent Image Loaded: {company_ideal[:30]}...")

            # 4. [ìµœì í™”] template stageëŠ” RAG/LLM ì—†ì´ ì¦‰ì‹œ í¬ë§·
            if next_stage.get("type") == "template":
                cert_list = ""
                act_org, act_role = "ê´€ë ¨ ê¸°ê´€", "ë‹´ë‹¹ ì—…ë¬´"
                proj_org, proj_name = "í•´ë‹¹ ê¸°ê´€", "ìˆ˜í–‰í•œ í”„ë¡œì íŠ¸"
                
                if interview.resume and interview.resume.structured_data:
                    sd = interview.resume.structured_data
                    if isinstance(sd, str): sd = json.loads(sd)
                    
                    # 1. ìê²©ì¦ ë¦¬ìŠ¤íŠ¸ì—…
                    certs = sd.get("certifications", [])
                    if certs:
                        cert_names = [c.get("title") or c.get("name") for c in certs if (c.get("title") or c.get("name"))]
                        cert_list = ", ".join(cert_names)
                    
                    # 4-1. ê²½ë ¥ (activities)
                    acts = sd.get("activities", [])
                    act_header_kws = ["ê¸°ê°„", "ì—­í• ", "ê¸°ê´€", "ì†Œì†", "ì¥ì†Œ", "ì œëª©", "ë‚´ìš©"]
                    for act in acts:
                        tmp_org = act.get("organization") or act.get("name") or ""
                        tmp_role = act.get("role") or act.get("position") or ""
                        if not any(kw in tmp_org for kw in act_header_kws) and not any(kw in tmp_role for kw in act_header_kws):
                            act_org = tmp_org or act_org
                            act_role = tmp_role or act_role
                            break
                    
                    # 4-2. í”„ë¡œì íŠ¸ (projects)
                    projs = sd.get("projects", [])
                    proj_header_kws = ["ê¸°ê°„", "ì œëª©", "ê³¼ì •ëª…", "ê¸°ê´€", "ì„¤ëª…", "ë‚´ìš©"]
                    for proj in projs:
                        tmp_name = proj.get("title") or proj.get("name") or ""
                        tmp_org = proj.get("organization") or ""
                        if not any(kw in tmp_name for kw in proj_header_kws) and not any(kw in tmp_org for kw in proj_header_kws):
                            proj_name = tmp_name or proj_name
                            proj_org = tmp_org or proj_org
                            break
                
                if not cert_list: cert_list = "ê´€ë ¨ ìê²©"

                template_vars = {
                    "candidate_name": candidate_name, 
                    "target_role": target_role, 
                    "company_name": company_name,
                    "company_ideal": company_ideal,
                    "major": major or "í•´ë‹¹ ì „ê³µ",
                    "cert_list": cert_list,
                    "act_org": act_org,
                    "act_role": act_role,
                    "proj_org": proj_org,
                    "proj_name": proj_name
                }
                
                tpl = next_stage.get("template", "{candidate_name} ì§€ì›ìë‹˜, ê³„ì†í•´ì£¼ì„¸ìš”.")
                try:
                    formatted = tpl.format(**template_vars)
                except Exception as e:
                    logger.warning(f"Template formatting error: {e}")
                    for k, v in template_vars.items():
                        tpl = tpl.replace("{" + k + "}", str(v))
                    formatted = tpl

                intro_msg = next_stage.get("intro_sentence", "")
                display_name = next_stage.get("display_name", "ë©´ì ‘ì§ˆë¬¸")
                final_content = f"[{display_name}] {intro_msg} {formatted}".strip() if intro_msg else f"[{display_name}] {formatted}"
                logger.info(f"Template stage '{next_stage['stage']}' â†’ ì¦‰ì‹œ í¬ë§· ì™„ë£Œ")

            else:
                category_raw = next_stage.get("category")
                
                # [í•µì‹¬ ìˆ˜ì •] narrative ì¹´í…Œê³ ë¦¬(9-14ë²ˆ)ëŠ” ì´ë ¥ì„œ RAGë¥¼ ê±´ë„ˆë›°ê³  ì¸ì¬ìƒì—ë§Œ ì§‘ì¤‘
                if next_stage.get("type") == "followup":
                    logger.info("ğŸ¯ Follow-up mode: Focusing purely on conversation context.")
                    context_text = f"ì´ì „ ì§ˆë¬¸: {last_ai_transcript.text if last_ai_transcript else 'ì—†ìŒ'}\n"
                    if last_user_transcript:
                        context_text += f"[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]: {last_user_transcript.text}"
                    rag_results = []
                elif category_raw == "narrative":
                    if next_stage.get("stage") == "responsibility":
                        # [íŠ¹ìƒí™œìš©] 11ë²ˆ ì±…ì„ê°/ê°€ì¹˜ê´€ ì§ˆë¬¸ì€ ì´ë ¥ì„œ(ìê¸°ì†Œê°œì„œ) ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                        logger.info("âœ¨ Responsibility Stage (11): Prioritizing Self-Intro Question 1 for values.")
                        
                        # 1. êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ [ì§ˆë¬¸1] ì •ë°€ íƒìƒ‰
                        values_text = ""
                        try:
                            if interview.resume and interview.resume.structured_data:
                                s_data = interview.resume.structured_data
                                if isinstance(s_data, str): s_data = json.loads(s_data)
                                
                                self_intro_list = s_data.get("self_intro", [])
                                for item in self_intro_list:
                                    if "[ì§ˆë¬¸1]" in item.get("question", ""):
                                        values_text = f"[ì§€ì›ì ìê¸°ì†Œê°œì„œ ì§ˆë¬¸1 ë‹µë³€]: {item.get('answer', '')}"
                                        logger.info("ğŸ“ Found Question 1 in Self-Intro.")
                                        break
                        except Exception as e:
                            logger.error(f"Failed to extract self_intro values: {e}")

                        # 2. RAG ê²°ê³¼ì™€ ê²°í•©
                        rag_results = retrieve_context("ì§€ì›ìì˜ ê·¼ë³¸ì ì¸ ê°€ì¹˜ê´€, ìƒí™œ ì‹ ë…, ì§ì—… ìœ¤ë¦¬, ì •ì§í•¨", resume_id=interview.resume_id, top_k=2)
                        rag_context = "\n".join([r['text'] for r in rag_results]) if rag_results else ""
                        
                        context_text = f"{values_text}\n\n[ì¶”ê°€ ì°¸ê³  ì •ë³´]:\n{rag_context}".strip()
                        if not context_text: context_text = "íŠ¹ë³„í•œ ê°€ì¹˜ê´€ ì •ë³´ ì—†ìŒ"
                    else:
                        # ë‚˜ë¨¸ì§€ ì¸ì¬ìƒ ê¸°ë°˜ ì§ˆë¬¸ ë‹¨ê³„: ì´ë ¥ì„œ ì»¨í…ìŠ¤íŠ¸ ë¹„í™œì„±í™”
                        logger.info(f"âœ¨ Narrative mode ({next_stage.get('stage')}): Skipping Resume RAG, focusing strictly on Company Ideal.")
                        context_text = f"íšŒì‚¬ì˜ ì¸ì¬ìƒ ì¤‘ì‹¬ ì§ˆë¬¸ ë‹¨ê³„ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ê°œë³„ í”„ë¡œì íŠ¸ë³´ë‹¤ëŠ” íšŒì‚¬ì˜ ê°€ì¹˜ê´€ ë¶€í•© ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì‹­ì‹œì˜¤."
                        rag_results = []
                else:
                    # ì¼ë°˜ ê¸°ìˆ /ê²½í—˜ ì§ˆë¬¸: ì´ë ¥ì„œ RAG ìˆ˜í–‰
                    query_template = next_stage.get("query_template", interview.position)
                    try:
                        query = query_template.format(target_role=target_role, major=major or "")
                    except:
                        query = query_template

                    rag_results = []
                    context_text = ""

                    if category_raw == "certification" and interview.resume and interview.resume.structured_data:
                        # êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„œ ìê²©ì¦ ì¶”ì¶œ ë¡œì§ (ìƒëµ ë°©ì§€ë¥¼ ìœ„í•œ ìœ ì§€)
                        context_text = "ì§€ì›ìê°€ ë³´ìœ í•œ ìê²©ì¦ ëª©ë¡:\n" + cert_list
                        rag_results = [{'text': f"ë³´ìœ  ìê²©: {cert_list}"}]
                    else:
                        rag_results = retrieve_context(query, resume_id=interview.resume_id, top_k=3)
                        context_text = "\n".join([r['text'] for r in rag_results]) if rag_results else "íŠ¹ë³„í•œ ì •ë³´ ì—†ìŒ"
                        
                    if last_user_transcript:
                        context_text += f"\n[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]: {last_user_transcript.text}"
                    else:
                        context_text += "\n[ì§€ì›ìì˜ ì‘ë‹µ ì •ë³´ê°€ ì•„ì§ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.]"

                llm = get_exaone_llm()
                prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
                chain = prompt | llm | StrOutputParser()

                # ê°€ì´ë“œ ë‚´ ë³€ìˆ˜ ì¹˜í™˜
                guide_raw = next_stage.get('guide', '')
                try:
                    guide_formatted = guide_raw.format(company_ideal=company_ideal)
                except:
                    guide_formatted = guide_raw

                # [ì¶”ê°€] ë‹¨ê³„ë³„ ë§ì¶¤í˜• ì „ëµ ì§€ì¹¨ ê²°ì • (ì§€ì›ìë‹˜ ìš”ì²­ ë°˜ì˜)
                mode_instruction = "ì¼ë°˜ì ì¸ ë‹¨ì¼ ì§ˆë¬¸ ìƒì„±ì„ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤."
                s_name = next_stage.get('stage', '')
                s_type = next_stage.get('type', '')
                
                if s_name == 'problem_solving':
                    mode_instruction = "ì´ ë‹¨ê³„ëŠ” 7ë²ˆ(ë¬¸ì œí•´ê²°ì§ˆë¬¸)ì…ë‹ˆë‹¤. ì§ˆë¬¸ ê³¼ì •ì—ì„œ 'ê·¸ëŸ°ë°' í˜¹ì€ 'ê·¸ë ‡ë‹¤ë©´'ê³¼ ê°™ì€ ì ‘ì†ì‚¬ë¥¼ í™œìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ìƒí™©ì„ ì œì‹œí•˜ë˜, ë°˜ë“œì‹œ ë”± í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ ë˜ì§€ì‹­ì‹œì˜¤."
                elif s_name == 'responsibility':
                    mode_instruction = "ì´ ë‹¨ê³„ëŠ” 11ë²ˆ(ê°€ì¹˜ê´€ ì§ˆë¬¸)ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ì¸ì‚¬ë§ ì—†ì´ ì¦‰ì‹œ 'ìê¸°ì†Œê°œì„œì— [ë¬¸êµ¬]ë¼ê³  ì‘ì„±í•˜ì…¨ìŠµë‹ˆë‹¤.'ë¡œ ì‹œì‘í•˜ê³ , 'ê·¸ë ‡ë‹¤ë©´'ìœ¼ë¡œ ì´ì–´ê°€ë©° ë”± í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ ë˜ì§€ì‹­ì‹œì˜¤."
                elif s_name == 'responsibility_followup':
                    mode_instruction = "ì´ ë‹¨ê³„ëŠ” 12ë²ˆ(ê°€ì¹˜ê´€ ì‹¬ì¸µ)ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ìš”ì•½í•œ ë’¤ 'ê·¸ëŸ°ë°' ë“±ì˜ ì ‘ì†ì‚¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”± í•˜ë‚˜ì˜ ì§ˆë¬¸ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì‹­ì‹œì˜¤."
                elif s_name == 'growth':
                    mode_instruction = "ì´ ë‹¨ê³„ëŠ” 13ë²ˆ(ì„±ì¥ê°€ëŠ¥ì„±)ì…ë‹ˆë‹¤. í•µì‹¬ ì¸ì¬ìƒ ê°€ì¹˜ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ë¡œ ë”± í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ ë˜ì§€ì‹­ì‹œì˜¤."
                elif s_type == 'followup':
                    mode_instruction = "ì´ ë‹¨ê³„ëŠ” ê¼¬ë¦¬ì§ˆë¬¸ì…ë‹ˆë‹¤. ë‹µë³€ ìš”ì•½ê³¼ ì§ˆë¬¸ì„ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë”± í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ ìƒì„±í•˜ì‹­ì‹œì˜¤."
                
                # [ì¶”ê°€] ì§€ì›ìì˜ ë¶€ì •ì  ë‹µë³€ ê°ì§€ ë° íŠ¹ìˆ˜ ì§€ì‹œ (ë¬´ì§€/íšŒí”¼ ëŒ€ì‘)
                if last_user_transcript:
                    u_text = last_user_transcript.text.strip()
                    negative_keywords = ["ëª¨ë¥´ê² ìŠµë‹ˆë‹¤", "ëª¨ë¥´ê² ì–´ìš”", "ì•„ë‹ˆìš”", "ì—†ìŠµë‹ˆë‹¤", "ê¸°ì–µì´ ì•ˆ ë‚¨", "ì˜ ëª¨ë¦„"]
                    if any(kw in u_text for kw in negative_keywords) and len(u_text) < 20:
                        mode_instruction += " [ì£¼ì˜: ì§€ì›ìê°€ ë‹µë³€ì„ íšŒí”¼í•˜ê±°ë‚˜ ëª¨ë¥´ê² ë‹¤ê³  í–ˆìŠµë‹ˆë‹¤. ë¬´ë¦¬í•˜ê²Œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì¹­ì°¬í•˜ë©° ìš”ì•½í•˜ì§€ ë§ê³ , ë‹µë³€ì´ ë¶€ì¡±í•¨ì„ ì–¸ê¸‰(ì˜ˆ: 'êµ¬ì²´ì ì¸ ì„¤ëª…ì´ ë¶€ì¡±í•˜ì—¬ ì•„ì‰½ìŠµë‹ˆë‹¤ë§Œ, ì´ ë¶€ë¶„ì€ ì–´ë– ì‹ ê°€ìš”?')í•˜ë©° ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ì¬ì§ˆë¬¸ì„ ë˜ì§€ì‹­ì‹œì˜¤.]"

                final_content = chain.invoke({
                    "context": context_text,
                    "stage_name": next_stage['display_name'],
                    "company_ideal": company_ideal,
                    "guide": guide_formatted,
                    "mode_instruction": mode_instruction,
                    "target_role": target_role
                })

                # [ì •ì œ ê°€ì†í™”]
                final_content = final_content.strip()
                final_content = re.sub(r'^["\'\s]+|["\'\s]+$', '', final_content)
                
                # 1. ì„œë‘ì— ë¶™ëŠ” ì˜¨ê°– ì¢…ë¥˜ì˜ ë ˆì´ë¸” ì œê±° (í•œê¸€/ì˜ë¬¸/íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
                label_patterns = [
                    r'^\**ì§€ì›ìì˜\s*ë‹µë³€\s*ìš”ì•½\s*ë°\s*ê¼¬ë¦¬ì§ˆë¬¸:\**\s*',
                    r'^\**í•µì‹¬\s*ìš”ì•½:\**\s*',
                    r'^\**ê¼¬ë¦¬ì§ˆë¬¸:\**\s*',
                    r'^\**ìš”ì•½:\**\s*',
                    r'^\**ì§ˆë¬¸:\**\s*',
                    r'^\**[QA]:\**\s*',
                    r'^\d+\.\s*',
                    r'^-\s*'
                ]
                for pattern in label_patterns:
                    final_content = re.sub(pattern, '', final_content, flags=re.IGNORECASE | re.MULTILINE)

                # 2. ë¬¸ì¥ ì¤‘ê°„ì— ì‚½ì…ë˜ëŠ” ì—°ê²° ë ˆì´ë¸” ì œê±°
                bridge_patterns = [
                    r'ì´ì—\s*ëŒ€í•œ\s*ì§ˆë¬¸ì…ë‹ˆë‹¤:?',
                    r'ë‹¤ìŒì€\s*ì§ˆë¬¸ì…ë‹ˆë‹¤:?',
                    r'ì§ˆë¬¸ë“œë¦½ë‹ˆë‹¤:?',
                    r'\*\*.*\*\*:\s*' # ë³¼íŠ¸ê°€ í¬í•¨ëœ ëª¨ë“  ë ˆì´ë¸” í˜•íƒœ ì œê±°
                ]
                for pattern in bridge_patterns:
                    final_content = re.sub(pattern, '', final_content, flags=re.IGNORECASE)

                final_content = final_content.replace("**", "").strip() # ë‚¨ì€ ë³¼íŠ¸ ì œê±°

                intro_tpl = next_stage.get("intro_sentence", "")
                if next_stage['stage'] == 'skill' and 'cert_name' in intro_tpl:
                    cert_name = "ìë£Œì— ëª…ì‹œëœ"
                    if rag_results:
                        match = re.search(r'ìê²©ëª…:\s*([^,\(]+)', rag_results[0]['text'])
                        if match: cert_name = match.group(1).strip()
                    intro_msg = intro_tpl.format(candidate_name=candidate_name, cert_name=cert_name)
                elif intro_tpl:
                    try:
                        intro_msg = intro_tpl.format(candidate_name=candidate_name)
                    except:
                        intro_msg = intro_tpl
                else:
                    intro_msg = ""

                if next_stage.get("type") == "followup":
                    intro_msg = "" 
                
                display_name = next_stage.get("display_name", "ì‹¬ì¸µ ë©´ì ‘")
                final_content = f"[{display_name}] {intro_msg} {final_content}".strip() if intro_msg else f"[{display_name}] {final_content}".strip()
                
                # [ë°±ì§€ ë°©ì§€] ë§Œì•½ ì •ì œ ê³¼ì •ì—ì„œ ë‚´ìš©ì´ ì‚¬ë¼ì¡Œê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° í´ë°±
                if len(final_content) < 10 or "?" not in final_content:
                    logger.warning(f"âš ï¸ [Empty/Short Question Detected] Stage: {next_stage['stage']}, Content: '{final_content}'")
                    # ë‹¨ìˆœíˆ ì§ˆë¬¸ë§Œ ë‹¤ì‹œ ìƒì„±í•˜ê±°ë‚˜, ì‹œë‚˜ë¦¬ì˜¤ ê°€ì´ë“œë¥¼ ì§ˆë¬¸ìœ¼ë¡œ ìŠ¹í™”
                    if "?" not in final_content:
                        final_content += "?" # ìµœì†Œí•œ ë¬¼ìŒí‘œë¼ë„ ë¶™ì„

            # 6. [ì „ì—­ ì •ì œ] ëª¨ë“  ì§ˆë¬¸ íƒ€ì…ì— ëŒ€í•´ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì •ì œ ìˆ˜í–‰
            final_content = final_content.strip()
            # ì½¤ë§ˆ(,), ë¬¼ìŒí‘œ(?), ë”°ì˜´í‘œ(", '), ì (.)ì„ ì œì™¸í•œ ëª¨ë“  íŠ¹ìˆ˜ë¬¸ì ì œê±°
            final_content = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z0-9\s,\?\.\"\']', '', final_content)
            
            # [ê°•ë ¥ ì œì•½] ë§Œì•½ ì •ì œ ê³¼ì •ì—ì„œ ë‚´ìš©ì´ ì‚¬ë¼ì¡Œê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ê²½ìš° í´ë°±
            # ê¸°ì¡´ì˜ ë¬¼ìŒí‘œ ë¶„ì ˆ(split) ë¡œì§ì€ ì¸ìš©ë¬¸ ë‚´ ë¬¼ìŒí‘œë¥¼ ì˜¤ì¸í•  ê°€ëŠ¥ì„±ì´ ì»¤ì„œ ì œê±°í•¨
            if len(final_content) < 15:
                logger.warning(f"âš ï¸ [Short Question Detected] Stage: {next_stage['stage']}, Content: '{final_content}'")
                # ê¼¬ë¦¬ì§ˆë¬¸ì¸ ê²½ìš° ë‹µë³€ ìš”ì•½ì´ ì‹¤íŒ¨í•œ ê²ƒìœ¼ë¡œ ë³´ê³  ì¼ë°˜ì ì¸ ì‹¬ì¸µ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ì²´
                if next_stage.get("type") == "followup":
                    final_content = "ì§€ì›ìë‹˜ì˜ ë‹µë³€ ë‚´ìš©ì„ ë“¤ì–´ë³´ì•˜ìŠµë‹ˆë‹¤. í•´ë‹¹ ê²½í—˜ì—ì„œ ë³¸ì¸ì´ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê¸°ì—¬í•œ ë¶€ë¶„ì€ ë¬´ì—‡ì´ì—ˆëŠ”ì§€ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
                else:
                    final_content = f"[{display_name}] ì§€ì›ìë‹˜ì˜ ìƒê°ì„ ì¡°ê¸ˆ ë” ìì„¸íˆ ë“£ê³  ì‹¶ìŠµë‹ˆë‹¤. ì´ ë¶€ë¶„ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
            
            final_content = final_content.strip()

            # 7. DB ì €ì¥ (Question ë° Transcript)
            category_raw = next_stage.get("category", "technical")
            category_map = {"certification": "technical", "project": "technical", "narrative": "behavioral", "problem_solving": "situational"}
            db_category = category_map.get(category_raw, "technical")

            logger.info(f"ğŸ’¾ Saving generated question to DB for Interview {interview_id} (Stage: {next_stage['stage']})")
            q_id = save_generated_question(
                interview_id=interview_id,
                content=final_content,
                category=db_category,
                stage=next_stage['stage'],
                guide=next_stage.get('guide', ''),
                session=session
            )

            # 8. ë©”ëª¨ë¦¬ ì •ë¦¬ (ë” ê°•ë ¥í•˜ê²Œ)
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                with torch.cuda.device(torch.cuda.current_device()):
                    torch.cuda.empty_cache()
            
            logger.info(f"âœ… [SUCCESS] Next question generated for Interview {interview_id}: {final_content[:50]}...")

            # 9. TTS ìƒì„± íƒœìŠ¤í¬ ì¦‰ì‹œ íŠ¸ë¦¬ê±°
            if q_id:
                import pathlib
                tts_file = pathlib.Path(f"/app/uploads/tts/q_{q_id}.wav")
                if not tts_file.exists():
                    clean_text = final_content
                    if final_content.startswith('[') and ']' in final_content:
                        clean_text = final_content.split(']', 1)[-1].strip()
                    logger.info(f"ğŸ”Š Triggering TTS synthesis for Question ID: {q_id}")
                    synthesize_task.delay(clean_text, language="ko", question_id=q_id)
                else:
                    logger.info(f"ğŸ”Š TTS file already exists for Question ID: {q_id}, skipping.")

            return {"status": "success", "stage": next_stage['stage'], "question": final_content}
    except Exception as e:
        logger.error(f"âŒ ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ (Retry: {self.request.retries}/3): {e}")
        if self.request.retries >= 3:
            logger.warning("âš ï¸ ì§ˆë¬¸ ìƒì„± ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. í´ë°±(Fallback) ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            try:
                from db import save_generated_question
                from tasks.tts import synthesize_task
                with Session(engine) as session:
                    fallback_text = "[ì‹œìŠ¤í…œ ì§ˆë¬¸] AI ì‘ë‹µ ì§€ì—°ìœ¼ë¡œ ì¸í•´ ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. ì´ ì§ë¬´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë³¸ì¸ì´ ê°€ì§„ ê°€ì¥ ë›°ì–´ë‚œ ì ì€ ë¬´ì—‡ì´ë©°, ì´ë¥¼ ë°œíœ˜í•œ ì‹¤ì œ ê²½í—˜ì„ ë§ì”€í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"
                    q_id = save_generated_question(
                        interview_id=interview_id,
                        content=fallback_text,
                        category="behavioral",
                        stage="fallback",
                        guide="ì—ëŸ¬ ë° íƒ€ì„ì•„ì›ƒ ë°œìƒìœ¼ë¡œ ì¸í•œ í´ë°± ì§ˆë¬¸",
                        session=session
                    )
                    if q_id:
                        clean_text = fallback_text.split(']', 1)[-1].strip() if ']' in fallback_text else fallback_text
                        synthesize_task.delay(clean_text, language="ko", question_id=q_id)
                    return {"status": "success", "stage": "fallback", "question": fallback_text}
            except Exception as fallback_e:
                logger.error(f"âŒ í´ë°± ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {fallback_e}")
                return {"status": "error", "message": "Fallback question failed"}
        else:
            raise self.retry(exc=e, countdown=3)
    finally:
        gc.collect()