import sys
import os
import re
import gc 
import logging
import torch
from datetime import datetime
from celery import shared_task
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# AI-Worker ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# -----------------------------------------------------------
# [1. ëª¨ë¸ ë° ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

if os.path.exists(local_path):
    model_path = local_path
else:
    model_path = docker_path

# ğŸš¨ DB ì¡°íšŒë¥¼ ìœ„í•´ ì¶”ê°€
try:
    from db import engine
    from sqlalchemy import text as sql_text
except ImportError:
    engine = None


# ğŸš¨ ExaoneLLMì€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import (Celery ë¡œë”© ì‹œì  ë¬¸ì œ íšŒí”¼)

# -----------------------------------------------------------
# [2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]
# -----------------------------------------------------------
PROMPT_TEMPLATE = """[|system|]
ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ê¸°ìˆ  ê¸°ì—…ì—ì„œ ì‹ ì… ë° ê²½ë ¥ ì‚¬ì›ì„ ì„ ë°œí•˜ëŠ” ìµœê³ ì˜ {position} ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤.
ì§€ì›ìì˜ ì´ë ¥ì„œ(RAG)ì™€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì™„ë²½íˆ íŒŒì•…í•˜ì—¬, í•´ë‹¹ ì§€ì›ìì—ê²Œë§Œ ë˜ì§ˆ ìˆ˜ ìˆëŠ” **'ì´ˆê°œì¸í™”(Hyper-Personalization)'** ì§ˆë¬¸ì„ ìƒì„±í•˜ë¼.

[ì‘ì„± ì§€ì¹¨ - ì ˆëŒ€ ê·œì¹™]
1. **ë‹¨ ë‘ ë¬¸ì¥, 150ì ì´ë‚´**: ëª¨ë“  ì§ˆë¬¸ì€ ë°˜ë“œì‹œ **ìµœëŒ€ ë‘ ë¬¸ì¥(150ì ì´ë‚´)**ìœ¼ë¡œ ìƒì„±í•˜ë¼.
2. **êµ¬ë¶„ í™•ì‹¤í•œ ì§ˆë¬¸ êµ¬ì„±**: ì§€ì›ìê°€ 'ì‹¤ì œë¡œ ìˆ˜í–‰í•œ ê²½í—˜'ê³¼ 'ì•ìœ¼ë¡œì˜ í¬ë¶€'ë¥¼ ì—„ê²©íˆ êµ¬ë¶„í•˜ë¼. "~ ì–¸ê¸‰í•˜ì‹  í¬ë¶€ë¥¼ ë³´ì•˜ìŠµë‹ˆë‹¤" ë˜ëŠ” "~ í”„ë¡œì íŠ¸ ê²½í—˜ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤"ì™€ ê°™ì´ ê·¼ê±°ë¥¼ ì •í™•íˆ í•˜ë¼.
3. **ì¶œì²˜ ëª…ì‹œ ë° ìì—°ìŠ¤ëŸ¬ìš´ ì‹œì‘**: ì§ˆë¬¸ ì‹œì‘ ì‹œ ë°˜ë“œì‹œ ê·¼ê±° ì¶œì²˜ë¥¼ ë°í˜€ë¼(ì˜ˆ: "ìê¸°ì†Œê°œì„œ 1ë²ˆ ë¬¸í•­ì„ ë³´ë‹ˆ", "ì´ë ¥ì„œì˜ í”„ë¡œì íŠ¸ ê²½í—˜ ì¤‘..."). 
4. **ê¸ˆì§€ ì‚¬í•­ (ìœ„ë°˜ ì‹œ íƒˆë½)**:
    - **ë§ë¨¸ë¦¬ ê¸ˆì§€**: "ë‹µë³€:", "ìš”ì•½:", "ì§ˆë¬¸:", "ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì :", "ë‹µë³€ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤" ê°™ì€ ì‚¬ì¡±ì´ë‚˜ ë¼ë²¨ì„ ì ˆëŒ€ ë¶™ì´ì§€ ë§ˆë¼.
    - **íŠ¹ìˆ˜ë¬¸ì ê¸ˆì§€**: ëŒ€ê´„í˜¸([]), ì†Œê´„í˜¸(()), ë³„í‘œ(**), ë¬¼ê²°(~) ë“± ì–´ë–¤ íŠ¹ìˆ˜ ê¸°í˜¸ë„ ì‚¬ìš©í•˜ì§€ ë§ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œë§Œ ë§í•˜ë¼.
    - **ì¤‘ë³µ í‘œí˜„ ê¸ˆì§€**: "(ì´ì „ ë‹µë³€ì—ì„œ ì–¸ê¸‰í•œ)" ê°™ì€ ë¶€ì—° ì„¤ëª…ì„ ê´„í˜¸ë¡œ ë„£ì§€ ë§ˆë¼. ë¬¸ë§¥ ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë¼.
5. **ê¼¬ë¦¬ ì§ˆë¬¸(followup) ê·œì¹™**: ë°˜ë“œì‹œ ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€ ë‚´ìš©ì„ **í•œ ë¬¸ì¥ìœ¼ë¡œ ì•„ì£¼ ì§§ê²Œ ìš”ì•½**í•œ ë’¤, ê·¸ì™€ ì—°ê´€ëœ ì‹¬ì¸µ ì§ˆë¬¸ì„ ë˜ì ¸ë¼. ì˜ˆ: "ë°©ê¸ˆ ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ë„ì…í–ˆë‹¤ê³  í•˜ì…¨ëŠ”ë°, êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì…¨ë‚˜ìš”?"
[|endofturn|]
[|user|]
# í‰ê°€ ë‹¨ê³„: {stage}
# ì‹œë‚˜ë¦¬ì˜¤ ê°€ì´ë“œ: {guide}
# ì§€ì›ì ê³ ìœ  ì •ë³´ ë° ê·¼ê±° (RAG + ëŒ€í™” ë¡œê·¸):
{context}

# ìš”ì²­:
ì§€ì›ì {name}ë‹˜ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‹¤ë¬´ ì—­ëŸ‰ì„ ê²€ì¦í•  ìˆ˜ ìˆëŠ” **ê¹”ë”í•˜ê³  êµ¬ì²´ì ì¸** ì§ˆë¬¸ 1ê°œë§Œ ìƒì„±í•´ì¤˜. (ì‚¬ì¡± ì—†ì´ ì§ˆë¬¸ë§Œ ì¶œë ¥í•  ê²ƒ)
[|endofturn|]
[|assistant|]
"""

# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ì •ì œ í”„ë¡¬í”„íŠ¸ (Self-Correction)]
# -----------------------------------------------------------
REFINER_PROMPT = """[|system|]
ë„ˆëŠ” ì „ë¬¸ êµì •ê°€ì´ì ë©´ì ‘ ì§€ì› ì „ë¬¸ê°€ì´ë‹¤. ë‹¤ìŒ ë©´ì ‘ ì§ˆë¬¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë¼ë²¨, íŠ¹ìˆ˜ë¬¸ì, ì‚¬ì¡±ì„ ì œê±°í•˜ì—¬ ì™„ë²½í•˜ê³  ê¹”ë”í•œ 'ìˆœìˆ˜ ì§ˆë¬¸ í…ìŠ¤íŠ¸'ë¡œ ë‹¤ë“¬ì–´ë¼.

[ë‹¤ë“¬ê¸° ê·œì¹™ - ì ˆëŒ€ ì—„ìˆ˜]
1. ë§ë¨¸ë¦¬ ë° ë¼ë²¨ ì œê±°: "ë‹µë³€:", "ìš”ì•½:", "ì§ˆë¬¸:", "ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì :", "ë‹µë³€ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤" ë“± ì§ˆë¬¸ ë³¸ë¬¸ ì™¸ì˜ ëª¨ë“  ì„¤ëª…ì„ ì‚­ì œí•˜ë¼.
2. ê¸°í˜¸ ì œê±°: ëŒ€ê´„í˜¸([]), ì†Œê´„í˜¸(()), ë³„í‘œ(**), ë¬¼ê²°(~) ë“± ëª¨ë“  íŠ¹ìˆ˜ ê¸°í˜¸ë¥¼ ì‚­ì œí•˜ë¼. ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì´ ì¤‘ìš”í•˜ë‹¤ë©´ ê´„í˜¸ë¥¼ ë²—ê¸°ê³  ë¬¸ì¥ ì†ì— ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œì¼œë¼.
3. ì‹œì‘ ë¬¸ì¥ êµì •: ë§Œì•½ ì§ˆë¬¸ì´ ì–´ìƒ‰í•˜ê²Œ ì‹œì‘í•œë‹¤ë©´ "ìê¸°ì†Œê°œì„œì—ì„œ ~ë¼ê³  ì–¸ê¸‰í•˜ì‹  ë¶€ë¶„ì„ ë³´ì•˜ìŠµë‹ˆë‹¤." ë˜ëŠ” "ë°©ê¸ˆ ~ë¼ê³  ë‹µë³€í•˜ì…¨ëŠ”ë°," ì™€ ê°™ì´ ìì—°ìŠ¤ëŸ½ê²Œ êµì •í•˜ë¼.
4. ì˜¤ì§ ì§ˆë¬¸ë§Œ ì¶œë ¥: ê°€ê¸‰ì  ë‘ ë¬¸ì¥ ì´ë‚´ì˜ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ë¼.
[|endofturn|]
[|user|]
ì›ë¬¸: {raw_question}
[|endofturn|]
[|assistant|]
"""
# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# [ê¸°ì¡´ ì¼ê´„ ìƒì„± íƒœìŠ¤í¬ ì‚­ì œë¨ - ì‹¤ì‹œê°„ ìƒì„± ëª¨ë“œë¡œ í†µí•©]

# -----------------------------------------------------------
# [5. Celery Task] - ì‹¤ì‹œê°„ 1ê°œì”© ìƒì„±í•˜ëŠ” íƒœìŠ¤í¬ (ìˆ˜ì • ì™„ë£Œ)
# -----------------------------------------------------------
@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    logger.info(f"ğŸ”¥ [START] generate_next_question_task for Interview {interview_id}")
    
    from db import (
        engine, Session, select, save_generated_question,
        Interview, Transcript, Speaker, Question, Resume

    )
    from utils.exaone_llm import get_exaone_llm
    
    with Session(engine) as session:
        interview = session.get(Interview, interview_id)
        if not interview: 
            logger.error(f"Interview {interview_id} not found.")
            return {"status": "error", "message": "Interview not found"}

        # [ì¶”ê°€] ì§ë¬´ ì „í™˜ ì—¬ë¶€ í™•ì¸ ë° ì‹œë‚˜ë¦¬ì˜¤ ë¶„ê¸°
        resume = session.get(Resume, interview.resume_id)
        major = ""
        if resume and resume.structured_data:
            education = resume.structured_data.get("education", [])
            if education and isinstance(education, list) and len(education) > 0:
                major = education[0].get("major", "")
        
        # transition ì—¬ë¶€ íŒë³„ (ë°±ì—”ë“œì™€ ë™ì¼í•œ í‚¤ì›Œë“œ ê¸°ì¤€)
        is_transition = False
        target_role = interview.position or ""
        if major and target_role:
            tech_role_keywords = ['ê°œë°œ', 'ì—”ì§€ë‹ˆì–´', 'í”„ë¡œê·¸ë˜ë¨¸', 'IT', 'SW', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ë°ì´í„°', 'ì¸ê³µì§€ëŠ¥', 'AI', 'ë³´ì•ˆ', 'ì‹œìŠ¤í…œ']
            tech_major_keywords = ['ì»´í“¨í„°', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ì •ë³´í†µì‹ ', 'ì „ê¸°', 'ì „ì', 'IT', 'ë°ì´í„°', 'ì¸ê³µì§€ëŠ¥', 'AI', 'ìˆ˜í•™', 'í†µê³„', 'ì‚°ì—…ê³µí•™']
            is_tech_role = any(kw in target_role for kw in tech_role_keywords)
            is_tech_major = any(kw in major for kw in tech_major_keywords)
            if is_tech_role and not is_tech_major:
                is_transition = True
        
        # ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë“ˆ ì„ íƒì  ì„í¬íŠ¸
        try:
            if is_transition:
                from config.interview_scenario_transition import get_stage_by_name, get_next_stage
                logger.info(f"âœ¨ [AI-WORKER] Transition scenario selected (Major: {major})")
            else:
                from config.interview_scenario import get_stage_by_name, get_next_stage
                logger.info("âœ… [AI-WORKER] Standard scenario selected")
        except ImportError as e:
            logger.error(f"âŒ Scenario import failed: {e}. Falling back to standard scenario.")
            from config.interview_scenario import get_stage_by_name, get_next_stage
            
        # ğŸš¨ [Race Condition ë°©ì§€] ì¤‘ë³µ ìƒì„± ì²´í¬
        # ë§ˆì§€ë§‰ AI ë°œí™” ì´í›„ì— ì‚¬ìš©ì ë‹µë³€ì´ ì•„ì§ ì—†ëŠ” ìƒíƒœì—ì„œ, 
        # ë§ˆì§€ë§‰ AI ë°œí™”ê°€ ë„ˆë¬´ ìµœê·¼(10ì´ˆ ì´ë‚´)ì´ë©´ ì¤‘ë³µ ìƒì„± ìš”ì²­ìœ¼ë¡œ ê°„ì£¼
        stmt_check = select(Transcript).where(
            Transcript.interview_id == interview_id
        ).order_by(Transcript.id.desc())
        last_transcript = session.exec(stmt_check).first()
        
        if last_transcript and last_transcript.speaker == Speaker.AI:
            diff = (datetime.utcnow() - last_transcript.timestamp).total_seconds()
            if diff < 10: # AIê°€ ë°©ê¸ˆ ë§í–ˆëŠ”ë° ë˜ ë§í•˜ë¼ê³  í•˜ë©´ ìŠ¤í‚µ
                logger.warning(f"âš ï¸ [SKIP] AI just spoke {diff:.1f}s ago. Waiting for user response.")
                return {"status": "skipped", "reason": "ai_just_spoke"}


        # ğŸ” í˜„ì¬ ë‹¨ê³„(Stage) íŒë³„ ë¡œì§ ê³ ë„í™”
        # 1. ë§ˆì§€ë§‰ìœ¼ë¡œ 'ì‚¬ìš©ìê°€ ë‹µë³€í•œ' ì§ˆë¬¸ì„ ì°¾ìŒ (ê°€ì¥ ì •í™•í•œ ì§€í‘œ)
        # Transcriptì˜ order í•„ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì‹œë‚˜ë¦¬ì˜¤ ìˆœì„œ ë³´ì¥
        stmt_user = select(Transcript).where(
            Transcript.interview_id == interview_id,
            Transcript.speaker == Speaker.USER
        ).order_by(Transcript.order.desc(), Transcript.id.desc())
        last_user_transcript = session.exec(stmt_user).first()
        
        last_stage_name = None
        if last_user_transcript and last_user_transcript.question_id:
            last_q = session.get(Question, last_user_transcript.question_id)
            if last_q:
                last_stage_name = last_q.question_type
                logger.info(f"Detected Last Answered Stage: {last_stage_name}")

        # 2. ë§Œì•½ ì‚¬ìš©ì ë‹µë³€ì´ ì—†ìœ¼ë©´ (ë©´ì ‘ ê·¹ì´ˆê¸°), ë§ˆì§€ë§‰ AI ì§ˆë¬¸ì„ ì°¸ê³ 
        if not last_stage_name:
            stmt_ai = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.AI
            ).order_by(Transcript.id.desc())
            last_ai_transcript = session.exec(stmt_ai).first()
            if last_ai_transcript and last_ai_transcript.question_id:
                last_q = session.get(Question, last_ai_transcript.question_id)
                if last_q:
                    # AIê°€ ì§ˆë¬¸ë§Œ ë‚´ë±‰ê³  ë‹µë³€ì„ ì•ˆ í•œ ìƒíƒœì´ë¯€ë¡œ, 
                    # í•œ ë‹¨ê³„ ë’¤ë¡œ ë¬¼ëŸ¬ë‚˜ì„œ íŒë‹¨í•˜ê±°ë‚˜ í˜„ì¬ ìƒíƒœë¥¼ ìœ ì§€
                    ai_stage = last_q.question_type
                    
                    # 'intro'ë‚˜ 'motivation'ì€ APIì—ì„œ ë¯¸ë¦¬ ë‘ ê°œë¥¼ ìƒì„±í•˜ë¯€ë¡œ íŠ¹ë³„ ì²˜ë¦¬
                    if ai_stage == "motivation":
                        # ì•„ì§ ì‚¬ìš©ìê°€ ë™ê¸°ë¥¼ ë§ ì•ˆ í–ˆìœ¼ë©´ introê¹Œì§€ë§Œ ëë‚œ ê²ƒìœ¼ë¡œ ê°„ì£¼ ê°€ëŠ¥ (ìƒí™©ì— ë”°ë¼)
                        # ì—¬ê¸°ì„œëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ AIê°€ ë³´ë‚¸ ë§ˆì§€ë§‰ ë‹¨ê³„ ì „ ë‹¨ê³„ë¥¼ íƒìƒ‰
                        last_stage_name = "intro" 
                    else:
                        last_stage_name = ai_stage
                logger.info(f"Detected Last AI-Spoken Stage (Used as Fallback): {last_stage_name}")
        
        # ğŸš¨ [Legacy/Alias ë³´ì •] DBì— ì €ì¥ëœ ì˜ˆì „ ëª…ì¹­ë“¤ì„ ìµœì‹  ì‹œë‚˜ë¦¬ì˜¤ ëª…ì¹­ìœ¼ë¡œ í†µì¼
        mapping_fix = {
            "technical": "skill",
            "personality": "communication",
            "values": "responsibility"
        }
        if last_stage_name in mapping_fix:
            logger.info(f"Applying legacy mapping fix: {last_stage_name} -> {mapping_fix[last_stage_name]}")
            last_stage_name = mapping_fix[last_stage_name]

        # ğŸ” ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
        if not last_stage_name:
            # ì•„ì˜ˆ ê¸°ë¡ì´ ì—†ìœ¼ë©´ introë¶€í„° ì‹œì‘ (ë³´í†µ interviews.pyì—ì„œ ìƒì„±í•˜ë¯€ë¡œ ì—¬ê¸°ì„  motivationì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
            next_stage_data = get_stage_by_name("intro")
        else:
            next_stage_data = get_next_stage(last_stage_name)

        if not next_stage_data:
            logger.info(f"ğŸ Scenario Completed for Interview {interview_id}. Updating status to COMPLETED.")
            try:
                interview.status = "COMPLETED" # InterviewStatus.COMPLETED
                interview.end_time = datetime.utcnow()
                session.add(interview)
                session.commit()
                
                # ë¦¬í¬íŠ¸ ìƒì„± íƒœìŠ¤í¬ ì¦‰ì‹œ íŠ¸ë¦¬ê±°
                from tasks.evaluator import generate_final_report
                generate_final_report.apply_async(args=[interview_id])
                logger.info(f"ğŸ“Š Triggered final report generation for Interview {interview_id}")
            except Exception as e:
                logger.error(f"Failed to update interview status to COMPLETED: {e}")
                
            return {"status": "completed"}

        stage_name = next_stage_data["stage"]

        # ğŸš¨ [ì¤‘ë³µ ìƒì„± ì ˆëŒ€ ë°©ì§€] í•´ë‹¹ ë‹¨ê³„ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬
        stmt_exist = select(Transcript).join(Question).where(
            Transcript.interview_id == interview_id,
            Question.question_type == stage_name
        )
        existing_q = session.exec(stmt_exist).first()
        if existing_q:
            logger.warning(f"âš ï¸ [SKIP] Stage '{stage_name}' already exists for Interview {interview_id}. No need to generate.")
            return {"status": "already_exists", "stage": stage_name}
        
        logger.info(f"Final Target Stage to Generate: {stage_name}")
        stage_type = next_stage_data.get("type", "ai")
        
        if stage_type == "template" or stage_type == "final":
            from utils.interview_helpers import get_candidate_info
            from db import Resume
            resume = session.get(Resume, interview.resume_id)
            c_info = get_candidate_info(resume.structured_data if resume else {})
            tmpl = next_stage_data.get("template", "{candidate_name}ë‹˜, ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤.")
            content = tmpl.format(candidate_name=c_info.get("candidate_name", "ì§€ì›ì"), target_role=interview.position)
            
            # QuestionCategory Enumì— 'general'ì´ ì—†ìœ¼ë¯€ë¡œ 'behavioral' ì‚¬ìš©
            save_generated_question(interview_id, content, "behavioral", stage_name, "")
            return {"status": "success", "stage": stage_name}

        # [LangChain LCEL] AI ìƒì„± íŒŒì´í”„ë¼ì¸
        try:
            # 1. ëª¨ë¸ ë° íŒŒì„œ ì¤€ë¹„
            llm = get_exaone_llm()
            output_parser = StrOutputParser()
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            from .rag_retrieval import get_retriever
            
            # [ìˆ˜ì •] ê¼¬ë¦¬ì§ˆë¬¸ì´ë“  ì¼ë°˜ ì§ˆë¬¸ì´ë“  ê¸°ë³¸ì ìœ¼ë¡œ ì´ë ¥ì„œ(RAG) ë² ì´ìŠ¤ë¼ì¸ì„ ê°€ì ¸ì˜´
            query_tmpl = next_stage_data.get("query_template", "{target_role}")
            if stage_type == "followup" and not next_stage_data.get("query_template"):
                parent_stage_name = next_stage_data.get("parent")
                parent_data = get_stage_by_name(parent_stage_name) if parent_stage_name else None
                query = parent_data.get("query_template", "{target_role}").format(target_role=interview.position) if parent_data else interview.position
            else:
                query = query_tmpl.format(target_role=interview.position)

            # [ìˆ˜ì •] ê¼¬ë¦¬ì§ˆë¬¸ ë° ì´ˆê°œì¸í™”ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            resume = session.get(Resume, interview.resume_id)
            profile_summary = ""
            narrative_context = ""
            
            if resume and resume.structured_data:
                sd = resume.structured_data
                header = sd.get("header", {})
                education = sd.get("education", [])
                edu_info = ""
                if education:
                    latest_edu = education[0]
                    school = latest_edu.get("school", "")
                    major = latest_edu.get("major", "")
                    if school or major:
                        edu_info = f"í•™ë ¥: {school} ({major})"
                
                skills = ", ".join(sd.get("skills", [])[:5])
                profile_summary = f"[ì§€ì›ì ê¸°ë³¸ ì •ë³´]\n- ì„±í•¨: {header.get('name', 'ì§€ì›ì')}\n- ì§€ì› ì§ë¬´: {interview.position}\n- {edu_info}\n- ì£¼ìš” ê¸°ìˆ : {skills}\n\n"

                # [ì¶”ê°€] íŠ¹ì • ë‹¨ê³„ë³„ ìê¸°ì†Œê°œì„œ íŠ¹ì • ë¬¸í•­ ì •ë°€ ë§¤í•‘
                if stage_name == "communication":
                    self_intro = sd.get("self_intro", [])
                    q3_data = next((item for item in self_intro if "[ì§ˆë¬¸3]" in item.get("question", "")), None)
                    if not q3_data and len(self_intro) >= 3: q3_data = self_intro[2]
                    if q3_data:
                        narrative_context = f"ìê¸°ì†Œê°œì„œ ì§ˆë¬¸ 3ë²ˆ ë‚´ìš© (í˜‘ì—…): {q3_data.get('answer')}\n\n"

                elif stage_name == "responsibility":
                    self_intro = sd.get("self_intro", [])
                    q1_data = next((item for item in self_intro if "[ì§ˆë¬¸1]" in item.get("question", "")), None)
                    if not q1_data and len(self_intro) >= 1: q1_data = self_intro[0]
                    if q1_data:
                        narrative_context = f"ìê¸°ì†Œê°œì„œ ì§ˆë¬¸ 1ë²ˆ ë‚´ìš© (ê°€ì¹˜ê´€): {q1_data.get('answer')}\n\n"

                elif stage_name == "growth":
                    self_intro = sd.get("self_intro", [])
                    q2_data = next((item for item in self_intro if "[ì§ˆë¬¸2]" in item.get("question", "")), None)
                    if not q2_data and len(self_intro) >= 2: q2_data = self_intro[1]
                    if q2_data:
                        narrative_context = f"ìê¸°ì†Œê°œì„œ ì§ˆë¬¸ 2ë²ˆ ë‚´ìš© (ì„±ì¥ì˜ì§€): {q2_data.get('answer')}\n\n"

            # Retriever ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            retriever = get_retriever(resume_id=interview.resume_id, top_k=10)
            retrieved_docs = retriever.invoke(query)
            
            # [ìˆ˜ì •] ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì§€ì‹ì˜ ì„±ê²©(ê²½í—˜ vs ê³„íš)ì„ ëª…ì‹œ
            rag_context_list = []
            if retrieved_docs:
                for doc in retrieved_docs:
                    cat = doc.metadata.get('category', 'unknown')
                    # ì¹´í…Œê³ ë¦¬ëª…ì„ ë” ì§ê´€ì ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ LLMì— ì „ë‹¬
                    cat_name = "ê²½í—˜ ë° í™œë™ ê¸°ë¡" if cat in ['project', 'experience', 'activity', 'award'] else "ìê¸°ì†Œê°œ ë° ê³„íšì„œ ë‚´ìš©"
                    rag_context_list.append(f"- {cat_name}: {doc.page_content}")
                rag_context = "\n".join(rag_context_list)
            else:
                rag_context = "ì´ë ¥ì„œ ì„¸ë¶€ ê·¼ê±° ì—†ìŒ"

            # [í•µì‹¬ ë¡œì§] 2. í”„ë¡œí•„ + ì´ë ¥ì„œ(RAG) + 'ë°©ê¸ˆ í•œ ë‹µë³€'ì„ ì„ì–´ì„œ LLMì—ê²Œ ì „ë‹¬
            if stage_type == "followup":
                # ê¼¬ë¦¬ì§ˆë¬¸: í”„ë¡œí•„ + RAG + ì´ì „ ë‹µë³€ ê²°í•©
                user_stmt = select(Transcript).where(
                    Transcript.interview_id == interview_id,
                    Transcript.speaker == Speaker.USER
                ).order_by(Transcript.id.desc())
                last_user_ans = session.exec(user_stmt).first()
                user_ans_text = last_user_ans.text if last_user_ans else "ì´ì „ ë‹µë³€ ì—†ìŒ"
                
                context_text = f"{profile_summary}{narrative_context}[ì´ë ¥ì„œ ì„¸ë¶€ ë‚´ìš©]\n{rag_context}\n\n[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]\n{user_ans_text}"
            else:
                # ì¼ë°˜ AI ì§ˆë¬¸: í”„ë¡œí•„ + RAG ê²°í•©
                context_text = f"{profile_summary}{narrative_context}[ì´ë ¥ì„œ ì„¸ë¶€ ë‚´ìš©]\n{rag_context}"

            # [ì¶”ê°€] ì‹¤ì‹œê°„ ë””ë²„ê¹… ë° ì‚¬ìš©ì í™•ì¸ì„ ìœ„í•œ ë¡œê·¸ ì¶œë ¥
            logger.info("========================================")
            logger.info(f"ğŸ” [LLM INPUT CONTEXT] (Interview ID: {interview_id}, Stage: {stage_name})")
            logger.info(context_text)
            logger.info("========================================")

            # 3. ì§€ì›ì ì •ë³´ ì •ì œ
            resume = session.get(Resume, interview.resume_id)
            candidate_name = "ì§€ì›ì"
            target_role = interview.position
            if resume and resume.structured_data:
                header = resume.structured_data.get("header", {})
                candidate_name = header.get("name") or header.get("candidate_name") or candidate_name
                target_role = header.get("target_role") or target_role

            # 4. LCEL ì²´ì¸ ì •ì˜ ë° ì‹¤í–‰ (1ë‹¨ê³„: ì§ˆë¬¸ ìƒì„±)
            prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
            chain = prompt | llm | output_parser

            logger.info(f"ğŸ”— Executing Generation Chain for stage: {stage_name}")
            raw_content = chain.invoke({
                "context": context_text,
                "position": interview.position,
                "stage": stage_name,
                "guide": next_stage_data.get("guide", ""),
                "name": candidate_name
            })
            
            # 5. [ì¶”ê°€] 2ë‹¨ê³„: ì§ˆë¬¸ ì •ì œ (Self-Correction)
            logger.info(f"âœ¨ Refining generated question (Raw: {raw_content[:30]}...)")
            refine_prompt_template = PromptTemplate.from_template(REFINER_PROMPT)
            refine_chain = refine_prompt_template | llm | output_parser
            
            content = refine_chain.invoke({"raw_question": raw_content})
            logger.info(f"âœ… Refined Question: {content}")

            # [ìµœì¢… ì •ì œ] ê°•ì¡° ê¸°í˜¸ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
            content = re.sub(r'[\*\*_~\[\]\(\)]', '', content)
            content = " ".join(content.split())
            
            if not content or len(content) < 5:
                content = f"{candidate_name}ë‹˜, ì•ì„œ ë§ì”€í•˜ì‹  ê²½í—˜ê³¼ ê´€ë ¨í•˜ì—¬ ë” êµ¬ì²´ì ì¸ ìƒí™©ì„ ì„¤ëª…í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?"
            
            # ğŸ§¹ ë©”ëª¨ë¦¬ ê´€ë¦¬
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # 5. ê²°ê³¼ ì €ì¥
            category_raw = next_stage_data.get("category", "technical")
            category_map = {"certification": "technical", "project": "technical", "narrative": "behavioral", "problem_solving": "situational"}
            db_category = category_map.get(category_raw, "technical")
            
            # [ì¶”ê°€] ë©´ì ‘ ë‹¨ê³„ë³„ í•œêµ­ì–´ ëª…ì¹­ ë° ì•ˆë‚´ ë¬¸êµ¬ ê°€ì ¸ì˜¤ê¸°
            from config.interview_scenario import INTERVIEW_STAGES
            stage_display = "ì‹¬ì¸µ ë©´ì ‘"
            intro_msg = ""
            for s in INTERVIEW_STAGES:
                if s["stage"] == stage_name:
                    stage_display = s.get("display_name", stage_display)
                    intro_msg = s.get("intro_sentence", "")
                    break
            
            # ê¼¬ë¦¬ì§ˆë¬¸ì˜ ê²½ìš° ê³ ì •ëœ ì¸íŠ¸ë¡œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ LLMì—ê²ŒëŠ” ì‹œí‚¤ì§€ ì•ŠìŒ)
            if stage_type == "followup":
                intro_msg = "ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìŠµë‹ˆë‹¤."
            elif intro_msg == "ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìŠµë‹ˆë‹¤.":
                # ë©”ì¸ ì§ˆë¬¸ì¸ë° ì‹œë‚˜ë¦¬ì˜¤ì— ì˜ëª» ë“¤ì–´ê°€ ìˆëŠ” ê²½ìš° ì œê±°
                intro_msg = ""

            # ì§ˆë¬¸ ì•ì— [ë‹¨ê³„] ë° ì•ˆë‚´ ë¬¸êµ¬ ì¶”ê°€
            final_content = f"[{stage_display}] {intro_msg} {content}" if intro_msg else f"[{stage_display}] {content}"
            
            logger.info(f"ğŸ’¾ Saving generated question to DB for Interview {interview_id} (Stage: {stage_name})")
            save_generated_question(interview_id, final_content, db_category, stage_name, next_stage_data.get("guide", ""), session=session)
            return {"status": "success", "stage": stage_name, "question": final_content}
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
        finally:
            gc.collect()