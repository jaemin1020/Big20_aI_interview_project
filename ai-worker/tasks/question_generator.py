import sys
import os
import re
import gc 
import logging
import torch
import json
from datetime import datetime
from celery import shared_task

# 1. ì´ˆê¸° ì„¤ì • ë° ëª¨ë¸ ê²½ë¡œ ìµœì í™”
if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ==========================================
# 2. í˜ë¥´ì†Œë‚˜ ì„¤ì • (Prompt Engineering)
# ==========================================

PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ë² í…Œë‘ ë©´ì ‘ê´€ì´ë©° ì „ë¬¸ ì±„ìš© ìœ„ì›ì¥ì…ë‹ˆë‹¤. 
ì§€ì›ìì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë‚ ì¹´ë¡œìš°ë©´ì„œë„ ì„±ì·¨ ì§€í–¥ì ì¸ ë‹¤ìŒ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ì‹­ì‹œì˜¤.

[ë©´ì ‘ ìƒí™© ê´€ì œ ì •ë³´]
- ì§€ì› ì§ë¬´: {position}
- ì§€ì›ì ì„±ëª…: {name}
- í˜„ì¬ ë‹¨ê³„: {stage_display}
- í•µì‹¬ ê°€ì´ë“œ: {guide}

[ì°¸ê³ : ì§€ì›ì ì´ë ¥ì„œ ë¬¸ë§¥]
{context}

[ì´ì „ ëŒ€í™” ìš”ì•½]
{chat_history}

[ì§ˆë¬¸ ìƒì„± ì§€ì¹¨]
1. ë°˜ë“œì‹œ ë‹¤ìŒ ì§ˆë¬¸ 1ê°œë§Œ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
2. ì§€ì›ìì˜ ë‹µë³€ ë‚´ìš©ì—ì„œ ê¸°ìˆ ì  í‚¤ì›Œë“œë‚˜ ê²½í—˜ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•˜ì‹­ì‹œì˜¤.
3. ì§ˆë¬¸ ëì—ëŠ” "..." ê°™ì€ íŠ¹ìˆ˜ë¬¸ìë¥¼ ë‚¨ë°œí•˜ì§€ ë§ê³  ì •ì¤‘í•œ ë§ˆì¹¨í‘œë‚˜ ë¬¼ìŒí‘œë¡œ ëë‚´ì‹­ì‹œì˜¤.
4. ê¸¸ì´ëŠ” 150ì ì´ë‚´ë¡œ í•µì‹¬ë§Œ ì°Œë¥´ì‹­ì‹œì˜¤.
5. {stage_display} ì„±ê²©ì— ë§ëŠ” ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

ì§ˆë¬¸:"""

# ==========================================
# 3. ë©”ì¸ ì‘ì—…: ì§ˆë¬¸ ìƒì„± íƒœìŠ¤í¬
# ==========================================

@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    """
    [í•¨ìˆ˜ì˜ ì—­í• ] ì¸í„°ë·° ì§„í–‰ ìƒí™©ì„ íŒŒì•…í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ì˜ AI ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ë‚´ë¶€ ì„í¬íŠ¸
    from db import (engine, Session, select, Interview, Transcript, Speaker, Question, save_generated_question)
    from utils.exaone_llm import get_exaone_llm
    from tasks.rag_retrieval import retrieve_context
    
    logger.info(f"ğŸš€ [START] Generating next question for Interview {interview_id}")
    
    with Session(engine) as session:
        # 1. ì¸í„°ë·° ì •ë³´ ë¡œë“œ
        interview = session.get(Interview, interview_id)
        if not interview: 
            logger.error(f"Interview {interview_id} not found")
            return {"status": "error", "message": "Interview not found"}

        # 2. ëŒ€í™” ë‚´ì—­ ì¡°íšŒ (í˜„ì¬ ë‹¨ê³„ íŒŒì•…ìš©)
        stmt = select(Transcript).where(Transcript.interview_id == interview_id).order_by(Transcript.order.desc())
        transcripts = session.exec(stmt).all()
        
        if not transcripts:
            logger.warning(f"No transcripts found for interview {interview_id}. Logic might need initial setup.")
            return {"status": "error", "message": "No transcripts found"}

        last_transcript = transcripts[0]
        
        # 3. ì¤‘ë³µ ìƒì„± ë°©ì§€ (AIê°€ ì§ˆë¬¸í–ˆëŠ”ë° 10ì´ˆ ì´ë‚´ì— ë˜ ìš”ì²­ ì˜¤ë©´ ë¬´ì‹œ)
        if last_transcript.speaker == "AI":
            diff = (datetime.utcnow() - last_transcript.timestamp).total_seconds()
            if diff < 10: 
                logger.info(f"Skipping: Last AI message was just {diff:.1f}s ago.")
                return {"status": "skipped", "reason": "too_soon"}

        # 4. ë‹¤ìŒ ë‹¨ê³„ ê²°ì • (Scenario Logic)
        from utils.interview_helpers import check_if_transition, get_candidate_info
        cand_info = get_candidate_info(session, interview.resume_id)
        is_transition = check_if_transition(cand_info.get("major", ""), interview.position)
        
        # ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œ
        if is_transition:
            import config.interview_scenario_transition as scenario
        else:
            import config.interview_scenario as scenario
            
        # ë§ˆì§€ë§‰ AI ì§ˆë¬¸ì˜ ë‹¨ê³„ë¥¼ ì°¾ê³  ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
        last_ai_transcript = next((t for t in transcripts if t.speaker == "AI"), None)
        last_stage_name = "motivation" # ê¸°ë³¸ê°’
        if last_ai_transcript and last_ai_transcript.question_id:
            q = session.get(Question, last_ai_transcript.question_id)
            if q: last_stage_name = q.question_type or "motivation"
        
        next_stage_data = scenario.get_next_stage(last_stage_name)
        
        if not next_stage_data:
            logger.info(f"Interview {interview_id} finished (No more stages). Status -> COMPLETED")
            interview.status = "COMPLETED"
            session.add(interview)
            session.commit()
            return {"status": "completed"}

        stage_name = next_stage_data["stage"]
        stage_display = next_stage_data.get("display_name", "ì‹¬ì¸µ ë©´ì ‘")
        stage_guide = next_stage_data.get("guide", "ì§€ì›ìì˜ ì—­ëŸ‰ì„ ê²€ì¦í•˜ì‹­ì‹œì˜¤.")
        
        logger.info(f"Target Stage: {stage_name} ({stage_display})")

        # 5. RAG ì»¨í…ìŠ¤íŠ¸ í™•ë³´
        # ì§ë¬´ ì§€ì‹ì´ë‚˜ ê²½í—˜ ì§ˆë¬¸ì¸ ê²½ìš° ì´ë ¥ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ë½‘ì•„ì˜´
        rag_context = ""
        if next_stage_data.get("type") == "ai" or next_stage_data.get("type") == "followup":
            query = next_stage_data.get("query_template", interview.position).format(target_role=interview.position)
            search_results = retrieve_context(query, resume_id=interview.resume_id, top_k=5)
            rag_context = "\n".join([r['text'] for r in search_results])

        # 6. ìµœê·¼ ëŒ€í™” ìš”ì•½ (Context for LLM)
        # ìµœê·¼ 3~4ê°œì˜ ëŒ€í™”ë§Œ ìš”ì•½í•˜ì—¬ ì „ë‹¬
        chat_limit = 5
        recent_chats = transcripts[:chat_limit][::-1] # ì—­ìˆœ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
        chat_history_str = "\n".join([f"{t.speaker}: {t.text}" for t in recent_chats])

        # 7. LLM í˜¸ì¶œ (LangChain LCEL)
        try:
            llm = get_exaone_llm()
            prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
            output_parser = StrOutputParser()
            
            chain = prompt | llm | output_parser
            
            final_content = chain.invoke({
                "position": interview.position,
                "name": cand_info.get("candidate_name", "ì§€ì›ì"),
                "stage_display": stage_display,
                "guide": stage_guide,
                "context": rag_context if rag_context else "ì´ë ¥ì„œ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§ë¬´ ì§€ì‹ì„ ë¬¼ì–´ë³´ì‹­ì‹œì˜¤.",
                "chat_history": chat_history_str
            })
            
            # í›„ì²˜ë¦¬: [ë‹¨ê³„] ë§ë¨¸ë¦¬ê°€ ì´ë¯¸ ìƒì„±ëœ ê²½ìš° ì¤‘ë³µ ë°©ì§€
            if not final_content.startswith('['):
                intro_msg = next_stage_data.get("intro_sentence", "")
                final_content = f"[{stage_display}] {intro_msg} {final_content}" if intro_msg else f"[{stage_display}] {final_content}"

            # 8. ê²°ê³¼ ì €ì¥
            save_generated_question(
                interview_id=interview_id,
                content=final_content,
                category=next_stage_data.get("category", "general"),
                stage=stage_name,
                guide=stage_guide
            )
            
            logger.info(f"âœ… Successfully generated and saved next question for Interview {interview_id}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            return {"status": "success", "stage": stage_name}

        except Exception as llm_err:
            logger.error(f"âŒ LLM generation failed: {llm_err}")
            return {"status": "error", "message": "LLM failed"}
