import os
import logging
from celery import shared_task
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from typing import Optional, List
import torch

# DB í—¬í¼ í•¨ìˆ˜ import
from db import (
    get_best_questions_by_position,  # ì§ë¬´ë³„ ìš°ìˆ˜ ì§ˆë¬¸ ì¡°íšŒ
    increment_question_usage,
    engine
)
from sqlmodel import Session, select

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ì„¤ì •
MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"

class QuestionGenerator:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ìƒì„±ê¸°
    ì „ëµ: DB ì¬í™œìš© (40%) + Few-Shot LLM ìƒì„± (60%)
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        logger.info(f"Loading Question Gen Model: {MODEL_ID}")
        token = os.getenv("HUGGINGFACE_HUB_TOKEN")
        
        # 4-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ìµœì í™”)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4"
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=token)
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="auto",
            token=token
        )
        
        pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=100,
            temperature=0.8, # ì°½ì˜ì„± í™•ë³´
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)
        self._initialized = True
        logger.info("âœ… Question Generator Initialized")

    def generate_questions(self, position: str, interview_id: Optional[int] = None, count: int = 5, reuse_ratio: float = 0.4):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ìƒì„± ë¡œì§ (ì´ë ¥ì„œ ë° íšŒì‚¬ ì •ë³´ ê¸°ë°˜)
        1. DBì—ì„œ ê²€ì¦ëœ ì§ˆë¬¸ ì¼ë¶€ ì¬í™œìš© (Reuse)
        2. ì´ë ¥ì„œ + íšŒì‚¬ ì •ë³´ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        3. ì¬í™œìš©ëœ ì§ˆë¬¸ì„ ì˜ˆì‹œ(Few-Shot)ë¡œ ì‚¼ì•„ ë‚˜ë¨¸ì§€ ì§ˆë¬¸ ìƒì„± (Create)
        
        Args:
            position: ì§€ì› ì§ë¬´
            interview_id: ë©´ì ‘ ID (ì´ë ¥ì„œ/íšŒì‚¬ ì •ë³´ ì¡°íšŒìš©)
            count: ìƒì„±í•  ì´ ì§ˆë¬¸ ìˆ˜
            reuse_ratio: ì¬í™œìš© ë¹„ìœ¨ (0.0 ~ 1.0)
        """
        from tools import ResumeTool, CompanyTool
        
        questions = []
        reuse_count = int(count * reuse_ratio)
        generate_count = count - reuse_count
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì´ë ¥ì„œ + íšŒì‚¬ ì •ë³´)
        context_parts = []
        
        if interview_id:
            # ì´ë ¥ì„œ ì •ë³´
            resume_info = ResumeTool.get_resume_by_interview(interview_id)
            if resume_info.get("has_resume"):
                context_parts.append(ResumeTool.format_for_llm(resume_info))
                logger.info(f"ì´ë ¥ì„œ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {resume_info.get('summary', '')[:50]}...")
            
            # íšŒì‚¬ ì •ë³´
            company_info = CompanyTool.get_company_by_interview(interview_id)
            if company_info.get("has_company"):
                context_parts.append(CompanyTool.format_for_llm(company_info))
                logger.info(f"íšŒì‚¬ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {company_info.get('name', '')}")
        
        context = "\n\n".join(context_parts) if context_parts else ""
        
        # 2. DBì—ì„œ ê¸°ì¡´ ì§ˆë¬¸ ì¬í™œìš© (Reuse)
        if reuse_count > 0:
            reused = self._reuse_questions_from_db(position, reuse_count)
            questions.extend(reused)
            logger.info(f"âœ… DBì—ì„œ {len(reused)}ê°œ ì§ˆë¬¸ ì¬í™œìš©")
        
        # 3. LLMìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ ìƒì„± (Create with Context)
        if generate_count > 0:
            generated = self._generate_new_questions(position, generate_count, questions, context)
            questions.extend(generated)
            logger.info(f"âœ… LLMìœ¼ë¡œ {len(generated)}ê°œ ì§ˆë¬¸ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)")
        
        return questions[:count]  # ì •í™•íˆ countê°œë§Œ ë°˜í™˜
    
    def _reuse_questions_from_db(self, position: str, count: int):
        """DBì—ì„œ ê²€ì¦ëœ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        
        try:
            db_questions = get_questions_by_position(position, limit=count)
            
            # ì¬í™œìš© ì‹œ ì‚¬ìš©ëŸ‰ ì¦ê°€
            for q in db_questions:
                try:
                    increment_question_usage(q.id)
                except Exception as e:
                    logger.warning(f"Question {q.id} ì‚¬ìš©ëŸ‰ ì¦ê°€ ì‹¤íŒ¨: {e}")
            
            return [q.content for q in db_questions]
        except Exception as e:
            logger.warning(f"DB ì§ˆë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {e}. ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []
    
    def _generate_new_questions(self, position: str, count: int, examples: list, context: str = ""):
        """LLMìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ ìƒì„± (Few-Shot + Context)"""
        if not self._initialized:
            self._initialize()
        
        # Few-Shot ì˜ˆì‹œ êµ¬ì„±
        few_shot_examples = "\n".join([f"- {q}" for q in examples[:3]]) if examples else "ì˜ˆì‹œ ì—†ìŒ"
        
        # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        context_section = f"\n\nì¶”ê°€ ì»¨í…ìŠ¤íŠ¸:\n{context}" if context else ""
        
        prompt = f"""ë‹¹ì‹ ì€ ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {position} ì§ë¬´ì— ì í•©í•œ ë©´ì ‘ ì§ˆë¬¸ì„ {count}ê°œ ìƒì„±í•˜ì„¸ìš”.
{context_section}

ê¸°ì¡´ ì§ˆë¬¸ ì˜ˆì‹œ:
{few_shot_examples}

ìš”êµ¬ì‚¬í•­:
1. ê¸°ìˆ ì  ê¹Šì´ì™€ ì‹¤ë¬´ ê²½í—˜ì„ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
2. ì§€ì›ìì˜ ì´ë ¥ì„œ ë‚´ìš©ê³¼ ì—°ê´€ëœ ì§ˆë¬¸ (ì´ë ¥ì„œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
3. íšŒì‚¬ì˜ ì¸ì¬ìƒì— ë¶€í•©í•˜ëŠ”ì§€ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ (íšŒì‚¬ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
4. ê° ì§ˆë¬¸ì€ í•œ ì¤„ë¡œ ì‘ì„±
5. ì§ˆë¬¸ë§Œ ë‚˜ì—´í•˜ê³  ë²ˆí˜¸ë‚˜ ì¶”ê°€ ì„¤ëª… ì—†ì´

ì§ˆë¬¸ {count}ê°œ:
"""
        
        try:
            response = self.llm.invoke(prompt)
            # ì‘ë‹µ íŒŒì‹±
            lines = [line.strip() for line in response.split('\n') if line.strip() and not line.strip().startswith('#')]
            questions = [line.lstrip('- ').lstrip('1234567890. ') for line in lines if len(line) > 10]
            return questions[:count]
        except Exception as e:
            logger.error(f"LLM ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._get_fallback_questions(position, count)
    
    def _get_fallback_questions(self, position: str, count: int) -> List[str]:
        """í´ë°± ì§ˆë¬¸ ìƒì„±"""
        fallback_questions = [
            f"{position} ì§ë¬´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ì—­ëŸ‰ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ìµœê·¼ ê²ªì—ˆë˜ ê°€ì¥ ì–´ë ¤ìš´ ê¸°ìˆ ì  ì±Œë¦°ì§€ëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?",
            f"{position} ì§ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° í•„ìš”í•œ í•µì‹¬ ê¸°ìˆ ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ë‚˜ìš”?",
            "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ì˜ê²¬ ì¶©ëŒì´ ìˆì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?",
            "ë³¸ì¸ì˜ ê°•ì ì„ ì‹¤ë¬´ì—ì„œ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?",
            "ìš°ë¦¬ íšŒì‚¬ì— ì§€ì›í•œ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "5ë…„ í›„ ë³¸ì¸ì˜ ëª¨ìŠµì„ ì–´ë–»ê²Œ ê·¸ë¦¬ê³  ê³„ì‹ ê°€ìš”?",
            "ì‹¤íŒ¨í•œ í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ ê·¸ë¡œë¶€í„° ë°°ìš´ ì ì„ ê³µìœ í•´ì£¼ì„¸ìš”."
        ]
        return fallback_questions[:count]

@shared_task(name="tasks.question_generator.generate_questions")
def generate_questions_task(position: str, interview_id: int = None, count: int = 5):
    try:
        generator = QuestionGenerator()
        return generator.generate_questions(position, interview_id, count)
    except Exception as e:
        logger.error(f"Task Error: {e}")
        return []

# Eager Initialization: Worker ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
# ì´ë ‡ê²Œ í•˜ë©´ ì²« ìš”ì²­ì—ì„œ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
try:
    logger.info("ğŸ”¥ Pre-loading Question Generator model...")
    _warmup_generator = QuestionGenerator()
    logger.info("âœ… Question Generator ready for requests")
except Exception as e:
    logger.warning(f"âš ï¸ Failed to pre-load model (will load on first request): {e}")
