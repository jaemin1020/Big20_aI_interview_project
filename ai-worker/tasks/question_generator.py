import os
import logging
from celery import shared_task
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from typing import Optional, List
import torch
import re

# DB í—¬í¼ í•¨ìˆ˜ import
from db import (
    get_best_questions_by_position,  # ì§ë¬´ë³„ ìš°ìˆ˜ ì§ˆë¬¸ ì¡°íšŒ
    increment_question_usage,
    engine
)
from sqlmodel import Session, select

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ì„¤ì •
MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"

class QuestionGenerator:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ìƒì„±ê¸°
    ì „ëµ: DB ì¬í™œìš© (40%) + Few-Shot LLM ìƒì„± (60%)
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        logger.info(f"Loading Question Gen Model: {MODEL_ID}")
        token = os.getenv("HUGGINGFACE_HUB_TOKEN")
        
        # 4-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ìµœì í™”)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4"
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=token)
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="cuda:0",
            token=token
        )
        
        pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=256,  # [ìµœì í™”] 256í† í°
            temperature=0.5, 
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id,
            return_full_text=False  # ì…ë ¥ í”„ë¡¬í”„íŠ¸ê°€ ì¶œë ¥ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ì„¤ì •
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)
        self._initialized = True
        logger.info("âœ… Question Generator Initialized")

    def generate_questions(self, position: str, interview_id: Optional[int] = None, count: int = 5, reuse_ratio: float = 0.4):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ìƒì„± ë¡œì§ (ì´ë ¥ì„œ ë° íšŒì‚¬ ì •ë³´ ê¸°ë°˜)
        1. DBì—ì„œ ê²€ì¦ëœ ì§ˆë¬¸ ì¼ë¶€ ì¬í™œìš© (Reuse)
        2. ì´ë ¥ì„œ + íšŒì‚¬ ì •ë³´ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        3. ì¬í™œìš©ëœ ì§ˆë¬¸ì„ ì˜ˆì‹œ(Few-Shot)ë¡œ ì‚¼ì•„ ë‚˜ë¨¸ì§€ ì§ˆë¬¸ ìƒì„± (Create)
        
        Args:
            position: ì§€ì› ì§ë¬´
            interview_id: ë©´ì ‘ ID (ì´ë ¥ì„œ/íšŒì‚¬ ì •ë³´ ì¡°íšŒìš©)
            count: ìƒì„±í•  ì´ ì§ˆë¬¸ ìˆ˜
            reuse_ratio: ì¬í™œìš© ë¹„ìœ¨ (0.0 ~ 1.0)
        """
        from tools import ResumeTool, CompanyTool
        
        questions = []
        reuse_count = int(count * reuse_ratio)
        generate_count = count - reuse_count
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì´ë ¥ì„œ + íšŒì‚¬ ì •ë³´)
        context_parts = []
        
        if interview_id:
            # ì´ë ¥ì„œ ì •ë³´
            resume_info = ResumeTool.get_resume_by_interview(interview_id)
            if resume_info.get("has_resume"):
                context_parts.append(ResumeTool.format_for_llm(resume_info))
                logger.info(f"ì´ë ¥ì„œ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {resume_info.get('summary', '')[:50]}...")
            
            # íšŒì‚¬ ì •ë³´
            company_info = CompanyTool.get_company_by_interview(interview_id)
            if company_info.get("has_company"):
                context_parts.append(CompanyTool.format_for_llm(company_info))
                logger.info(f"íšŒì‚¬ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {company_info.get('name', '')}")
        
        context = "\n\n".join(context_parts) if context_parts else ""
        
        # 2. DBì—ì„œ ê¸°ì¡´ ì§ˆë¬¸ ì¬í™œìš© (Reuse)
        if reuse_count > 0:
            reused = self._reuse_questions_from_db(position, reuse_count)
            questions.extend(reused)
            logger.info(f"âœ… DBì—ì„œ {len(reused)}ê°œ ì§ˆë¬¸ ì¬í™œìš©")
        
        # 3. LLMìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ ìƒì„± (Create with Context)
        if generate_count > 0:
            generated = self._generate_new_questions(position, generate_count, questions, context)
            questions.extend(generated)
            logger.info(f"âœ… LLMìœ¼ë¡œ {len(generated)}ê°œ ì§ˆë¬¸ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)")
        
        return questions[:count]  # ì •í™•íˆ countê°œë§Œ ë°˜í™˜
    
    def _reuse_questions_from_db(self, position: str, count: int):
        """DBì—ì„œ ê²€ì¦ëœ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        
        try:
            # db.pyì˜ í•¨ìˆ˜ëª…ì— ë§ì¶° í˜¸ì¶œ
            db_questions = get_best_questions_by_position(position, limit=count)
            
            # ì¬í™œìš© ì‹œ ì‚¬ìš©ëŸ‰ ì¦ê°€
            for q in db_questions:
                try:
                    increment_question_usage(q.id)
                except Exception as e:
                    logger.warning(f"Question {q.id} ì‚¬ìš©ëŸ‰ ì¦ê°€ ì‹¤íŒ¨: {e}")
            
            return [q.content for q in db_questions]
        except Exception as e:
            logger.warning(f"DB ì§ˆë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {e}. ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []
    
    def _generate_new_questions(self, position: str, count: int, examples: list, context: str = ""):
        """LLMìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ ìƒì„± (Few-Shot + Context)"""
        
        
        # Few-Shot ì˜ˆì‹œ êµ¬ì„± (ì˜ˆì‹œê°€ ì—†ìœ¼ë©´ ê°•ë ¥í•œ í•œêµ­ì–´ ê¸°ë³¸ ì˜ˆì‹œ ì œê³µ)
        if examples:
            few_shot_examples = "\n".join([f"- {q}" for q in examples[:3]])
        else:
            few_shot_examples = """
- Reactì˜ Virtual DOMì´ ë¬´ì—‡ì´ë©°, ì´ê²ƒì´ ì„±ëŠ¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì—ì„œ Promiseì™€ async/awaitì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
- ì‚¬ìš©í•´ë³¸ ìƒíƒœ ê´€ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë¬´ì—‡ì´ë©°, ê·¸ ì„ íƒ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
"""
        
        # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        context_section = f"\n\nì¶”ê°€ ì»¨í…ìŠ¤íŠ¸:\n{context}" if context else ""
        
        # ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°
        prompt = [{'role':'system','content':
        (f"""
        ë‹¹ì‹ ì€ í•œêµ­ ê¸°ì—…ì˜ ë©´ì ‘ê´€ì´ì ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {position} ì§ë¬´ì— ì í•©í•œ 'í•œêµ­ì–´ ë©´ì ‘ ì§ˆë¬¸'ì„ {count}ê°œ ìƒì„±í•˜ì„¸ìš”.
        {context_section}
        
        ê¸°ì¡´ ì§ˆë¬¸ ì˜ˆì‹œ:
        {few_shot_examples}
        
        [ì¤‘ìš” ìš”êµ¬ì‚¬í•­]
        1. ëª¨ë“  ì§ˆë¬¸ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ì–´, íƒœêµ­ì–´ ë“± íƒ€ ì–¸ì–´ í˜¼ìš© ê¸ˆì§€)
        2. ê¸°ìˆ ì  ê¹Šì´ì™€ ì‹¤ë¬´ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³´ì„¸ìš”.
        3. ì§€ì›ìì˜ ì´ë ¥ì„œ ë‚´ìš©ê³¼ ì—°ê´€ëœ ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”. (ì´ë ¥ì„œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        4. íšŒì‚¬ì˜ ì¸ì¬ìƒê³¼ ì—°ê²°ëœ ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”. (íšŒì‚¬ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        5. ê° ì§ˆë¬¸ì€ ë²ˆí˜¸ ì—†ì´ í•œ ì¤„ì”©ë§Œ ì‘ì„±í•˜ì„¸ìš”.
        6. ì§ˆë¬¸ì˜ ì–´ì¡°ëŠ” ì •ì¤‘í•˜ê³  ì „ë¬¸ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        7. ê°•ì¡° í‘œì‹œ(**text**) ê¸ˆì§€
        """)}]
        
        try:
            # Llama 3.2 ëª¨ë¸ì„ ìœ„í•œ ì±„íŒ… í…œí”Œë¦¿ ì ìš©
            prompt_str = self.tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)
            
            # ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•´ ë” ê¸´ í† í° í—ˆìš© (return_full_text=False ì„¤ì • ë•ë¶„ì— prompt_strì€ ì œì™¸ë¨)
            response = self.llm.invoke(prompt_str)
            
            # ì‘ë‹µ íŒŒì‹±
            
            # 1. íŠ¹ìˆ˜ í† í° ë° ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì œê±° íŒ¨í„´
            system_patterns = [
                r"<\|.*?\|>",  # íŠ¹ìˆ˜ í† í°
                r"Cutting Knowledge Date",
                r"Today Date",
                r"^system$", # í—¤ë” ì”ì—¬ë¬¼
                r"^user$",
                r"^assistant$",
                r"ë‹¹ì‹ ì€ ë©´ì ‘ ì§ˆë¬¸ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤", # í”„ë¡¬í”„íŠ¸ ì—ì½” ë°©ì§€
                r"ìš”êµ¬ì‚¬í•­:",
                r"ê¸°ì¡´ ì§ˆë¬¸ ì˜ˆì‹œ:",
                r"ì§ˆë¬¸ \d+ê°œ:"
            ]
            
            clean_lines = []
            for line in response.split('\n'):
                line = line.strip()
                if not line:
                    continue
                    
                # ì‹œìŠ¤í…œ ë©”ì‹œì§€ íŒ¨í„´ì´ í¬í•¨ëœ ë¼ì¸ ê±´ë„ˆë›°ê¸°
                if any(re.search(pat, line) for pat in system_patterns):
                    continue
                
                # í”„ë¡¬í”„íŠ¸ì˜ ì§€ì‹œì‚¬í•­ ë¬¸ì¥ê³¼ ìœ ì‚¬í•˜ë©´ ê±´ë„ˆë›°ê¸° (Echo ë°©ì§€ 2ì°¨ í•„í„°)
                if "í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸" in line or "ì´ë ¥ì„œ ë‚´ìš©ê³¼ ì—°ê´€" in line or "í•œ ì¤„ë¡œ ì‘ì„±" in line:
                    continue

                # #ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì„ ë¼ì¸ ê±´ë„ˆë›°ê¸°
                if line.startswith('#'):
                    continue
                    
                clean_lines.append(line)

            # 2. ì§ˆë¬¸ ì¶”ì¶œ ë° ì •ì œ
            questions = []
            for line in clean_lines:
                # ë²ˆí˜¸ ì œê±° (ì˜ˆ: "1. ì§ˆë¬¸" -> "ì§ˆë¬¸", "- ì§ˆë¬¸" -> "ì§ˆë¬¸")
                clean_q = re.sub(r'^[\d\-\.\s]+', '', line)
                
                # Markdown ê°•ì¡° ì œê±° (**text** -> text)
                clean_q = re.sub(r'\*\*(.*?)\*\*', r'\1', clean_q)
                
                # ì•ë’¤ ë”°ì˜´í‘œ ë° ê³µë°± ì œê±°
                clean_q = clean_q.strip('"\' ')
                
                # [í•„í„°ë§ ê°œì„ ] Whitelist ë°©ì‹ì€ ë„ˆë¬´ ì—„ê²©í•˜ì—¬ Blacklist ë°©ì‹ìœ¼ë¡œ ë³€ê²½
                # ì¼ë³¸ì–´(íˆë¼ê°€ë‚˜/ê°€íƒ€ì¹´ë‚˜), í•œì, íƒœêµ­ì–´ ë“±ì´ í¬í•¨ëœ ê²½ìš°ë§Œ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” í—ˆìš©
                # ê¸°ìˆ  ë©´ì ‘ ì§ˆë¬¸ì—ëŠ” ë‹¤ì–‘í•œ íŠ¹ìˆ˜ë¬¸ì(@, #, &, [] ë“±)ê°€ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
                forbidden_pattern = r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\u0E00-\u0E7F]'
                if re.search(forbidden_pattern, clean_q):
                    logger.warning(f"ì œì™¸ëœ ì§ˆë¬¸(ë‹¤êµ­ì–´ í¬í•¨): {clean_q}")
                    continue
                
                # ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì€ ê²ƒì€ ì§ˆë¬¸ì´ ì•„ë‹ í™•ë¥  ë†’ìŒ (10ì ì´ìƒ)
                if len(clean_q) > 10:
                    questions.append(clean_q)
            
            # ë§Œì•½ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ Fallback ì§ˆë¬¸ìœ¼ë¡œ ì±„ì›€
            if len(questions) < count:
                logger.warning(f"ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜ ë¶€ì¡± ({len(questions)}/{count}). Fallbackìœ¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.")
                fallback_needed = count - len(questions)
                fallbacks = self._get_fallback_questions(position, fallback_needed)
                questions.extend(fallbacks)
                
            logger.info(f"ìµœì¢… ë°˜í™˜ ì§ˆë¬¸: {questions[:count]}")
            return questions[:count]
        except Exception as e:
            logger.error(f"LLM ì§ˆë¬¸ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë³´ë‹¨ Fallback ë¦¬í„´
            return self._get_fallback_questions(position, count)
    
    def _get_fallback_questions(self, position: str, count: int) -> List[str]:
        """í´ë°± ì§ˆë¬¸ ìƒì„±"""
        fallback_questions = [
            f"{position} ì§ë¬´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ì—­ëŸ‰ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ìµœê·¼ ê²ªì—ˆë˜ ê°€ì¥ ì–´ë ¤ìš´ ê¸°ìˆ ì  ì±Œë¦°ì§€ëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?",
            f"{position} ì§ë¬´ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° í•„ìš”í•œ í•µì‹¬ ê¸°ìˆ ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ë‚˜ìš”?",
            "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ì˜ê²¬ ì¶©ëŒì´ ìˆì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?",
            "ë³¸ì¸ì˜ ê°•ì ì„ ì‹¤ë¬´ì—ì„œ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆì„ê¹Œìš”?",
            "ìš°ë¦¬ íšŒì‚¬ì— ì§€ì›í•œ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "5ë…„ í›„ ë³¸ì¸ì˜ ëª¨ìŠµì„ ì–´ë–»ê²Œ ê·¸ë¦¬ê³  ê³„ì‹ ê°€ìš”?",
            "ì‹¤íŒ¨í•œ í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ ê·¸ë¡œë¶€í„° ë°°ìš´ ì ì„ ê³µìœ í•´ì£¼ì„¸ìš”."
        ]
        return fallback_questions[:count]

@shared_task(name="tasks.question_generator.generate_questions")
def generate_questions_task(position: str, interview_id: int = None, count: int = 5):
    try:
        generator = QuestionGenerator()
        return generator.generate_questions(position, interview_id, count)
    except Exception as e:
        logger.error(f"Task Error: {e}")
        return []

# Eager Initialization: Worker ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
# ì´ë ‡ê²Œ í•˜ë©´ ì²« ìš”ì²­ì—ì„œ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
try:
    logger.info("ğŸ”¥ Pre-loading Question Generator model...")
    _warmup_generator = QuestionGenerator()
    logger.info("âœ… Question Generator ready for requests")
except Exception as e:
    logger.warning(f"âš ï¸ Failed to pre-load model (will load on first request): {e}")
