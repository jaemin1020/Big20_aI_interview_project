import os
import sys
import json
import gc
import logging
import torch
from datetime import datetime
from celery import shared_task
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1. ì´ˆê¸° ì„¤ì •
if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
model_path = docker_path if os.path.exists(docker_path) else local_path

PROMPT_TEMPLATE = """[|system|]ë‹¹ì‹ ì€ ì§€ì›ìì˜ ì—­ëŸ‰ì„ ì •ë°€í•˜ê²Œ ê²€ì¦í•˜ëŠ” ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
ì œê³µëœ [ì´ë ¥ì„œ ë¬¸ë§¥]ê³¼ [ë©´ì ‘ ì§„í–‰ ìƒí™©]ì„ ë°”íƒ•ìœ¼ë¡œ, ì§€ì›ìì—ê²Œ ë˜ì§ˆ 'ë‹¤ìŒ ì§ˆë¬¸' 1ê°œë§Œ ìƒì„±í•˜ì‹­ì‹œì˜¤.

[ì ˆëŒ€ ê·œì¹™]
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
2. ì§ˆë¬¸ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•˜ë©°, 150ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
3. íŠ¹ìˆ˜ë¬¸ì(JSON ê¸°í˜¸, ì—­ë”°ì˜´í‘œ ë“±)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì˜¤ì§ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
4. "ì§ˆë¬¸:" ì´ë¼ëŠ” ìˆ˜ì‹ì–´ ì—†ì´ ë°”ë¡œ ì§ˆë¬¸ ë³¸ë¬¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
5. ì´ì „ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ í•˜ì‹­ì‹œì˜¤.

[ì´ë ¥ì„œ ë° ë‹µë³€ ë¬¸ë§¥]
{context}

[í˜„ì¬ ë©´ì ‘ ë‹¨ê³„ ì •ë³´]
- ë‹¨ê³„ëª…: {stage_name}
- ê°€ì´ë“œ: {guide}

[|user|]ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.[|endofturn|]
[|assistant|]"""

# ==========================================
# ë©”ì¸ ì‘ì—…: ì§ˆë¬¸ ìƒì„± íƒœìŠ¤í¬
# ==========================================

@shared_task(bind=True, name="tasks.question_generation.generate_next_question")
def generate_next_question_task(self, interview_id: int):
    """
    ì¸í„°ë·° ì§„í–‰ ìƒí™©ì„ íŒŒì•…í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ì˜ AI ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ëŠ¦ì€ ì„í¬íŠ¸ë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
    from db import engine, Session, select, Interview, Transcript, Speaker, Question, save_generated_question
    from utils.exaone_llm import get_exaone_llm
    from tasks.tts import synthesize_task
    from utils.rag_utils import retrieve_context  # RAG í•¨ìˆ˜ ê°€ì •
    from config.interview_logic import check_if_transition, get_next_stage_normal, get_next_stage_transition

    try:
        with Session(engine) as session:
            # 1. ì¸í„°ë·° ì •ë³´ ë¡œë“œ
            interview = session.get(Interview, interview_id)
            if not interview:
                logger.error(f"Interview {interview_id} not found.")
                return {"status": "error", "message": "Interview not found"}

            # 2. ë§ˆì§€ë§‰ ë°œí™” í™•ì¸ ë° ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ë°©ì§€
            stmt_all = select(Transcript).where(Transcript.interview_id == interview_id).order_by(Transcript.order.desc())
            last_transcript = session.exec(stmt_all).first()

            stmt_ai = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.AI
            ).order_by(Transcript.order.desc(), Transcript.id.desc())
            last_ai_transcript = session.exec(stmt_ai).first()

            # ì¤‘ë³µ ìƒì„± ë°©ì§€ (ìµœê·¼ 10ì´ˆ ë‚´ ìƒì„± ì—¬ë¶€)
            if last_ai_transcript:
                diff = (datetime.utcnow() - last_ai_transcript.timestamp).total_seconds()
                if diff < 10:
                    logger.info(f"Skipping duplicate request (diff={diff:.1f}s)")
                    return {"status": "skipped"}

            # 3. ì§€ì›ì ì •ë³´ ì¶”ì¶œ (ì „ê³µ, ì„±í•¨ ë“±)
            major = ""
            candidate_name = "ì§€ì›ì"
            if interview.resume and interview.resume.structured_data:
                sd = interview.resume.structured_data
                if isinstance(sd, str): sd = json.loads(sd)
                
                edu = sd.get("education", [])
                major = next((e.get("major", "") for e in edu if e.get("major", "").strip()), "")
                candidate_name = sd.get("header", {}).get("name", "ì§€ì›ì")

            # 4. ë‹¤ìŒ ë‹¨ê³„ íŒë³„
            is_transition = check_if_transition(major, interview.position)
            get_next_stage_func = get_next_stage_transition if is_transition else get_next_stage_normal
            
            if last_ai_transcript and last_ai_transcript.question_id:
                last_q = session.get(Question, last_ai_transcript.question_id)
                last_stage_name = last_q.stage if last_q else "intro"
            else:
                last_stage_name = "intro"

            next_stage = get_next_stage_func(last_stage_name)

            if not next_stage:
                interview.status = "COMPLETED"
                session.add(interview)
                session.commit()
                return {"status": "completed"}

            # 5. ì§ˆë¬¸ ë‚´ìš© ìƒì„± (Template vs LLM)
            final_question_body = ""
            stage_type = next_stage.get("type", "ai")

            if stage_type == "template":
                target_role = interview.position or "í•´ë‹¹ ì§ë¬´"
                template_vars = {"candidate_name": candidate_name, "target_role": target_role, "major": major}
                tpl = next_stage.get("template", "{candidate_name} ì§€ì›ìë‹˜, ê³„ì†í•´ì£¼ì„¸ìš”.")
                try:
                    final_question_body = tpl.format(**template_vars)
                except KeyError:
                    final_question_body = tpl
            else:
                # RAG ë¬¸ë§¥ í™•ë³´
                query_template = next_stage.get("query_template", interview.position)
                query = query_template.format(target_role=interview.position, major=major)
                rag_results = retrieve_context(query, resume_id=interview.resume_id, top_k=3)
                context_text = "\n".join([r['text'] for r in rag_results]) if rag_results else "ì´ë ¥ì„œ ì •ë³´ ì—†ìŒ"

                if last_transcript and last_transcript.speaker == Speaker.USER:
                    context_text += f"\n[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]: {last_transcript.text}"

                # LLM í˜¸ì¶œ
                llm = get_exaone_llm()
                prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
                chain = prompt | llm | StrOutputParser()
                final_question_body = chain.invoke({
                    "context": context_text,
                    "stage_name": next_stage['display_name'],
                    "guide": next_stage.get('guide', '')
                })

            # 6. ìµœì¢… í…ìŠ¤íŠ¸ ì¡°ë¦½ (ì•ˆë‚´ ë¬¸êµ¬ í¬í•¨)
            stage_display = next_stage.get("display_name", "ë©´ì ‘ì§ˆë¬¸")
            intro_msg = next_stage.get("intro_sentence", "")
            
            # ê¼¬ë¦¬ì§ˆë¬¸ì¼ ê²½ìš° ê°€ì´ë“œ ì¶”ê°€
            if next_stage.get("category") == "followup":
                intro_msg = "ì¶”ê°€ì ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìŠµë‹ˆë‹¤."

            final_content = f"[{stage_display}] {intro_msg} {final_question_body}".strip()

            # 7. DB ì €ì¥ ë° TTS íŠ¸ë¦¬ê±°
            db_category = next_stage.get('category') or 'behavioral'
            q_id = save_generated_question(
                interview_id=interview_id,
                content=final_content,
                category=db_category,
                stage=next_stage['stage'],
                guide=next_stage.get('guide', ''),
                session=session
            )

            if q_id:
                logger.info(f"ğŸ”Š Question {q_id} generated. Triggering TTS.")
                synthesize_task.delay(final_content, language="ko", question_id=q_id)

            return {"status": "success", "stage": next_stage['stage'], "question": final_content}

    except Exception as e:
        logger.error(f"âŒ Error in generate_next_question: {e}")
        # 3íšŒê¹Œì§€ ì¬ì‹œë„
        raise self.retry(exc=e, countdown=5, max_retries=3)
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()