import sys
import os
import time
import gc 
import logging
import torch
from celery import shared_task
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager
from langchain_core.prompts import PromptTemplate

# AI-Worker ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# -----------------------------------------------------------
# [1. ëª¨ë¸ ë° ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

model_path = local_path if os.path.exists(local_path) else docker_path

# ğŸš¨ DB ì¡°íšŒë¥¼ ìœ„í•´ ì¶”ê°€
try:
    from db import engine
    from sqlalchemy import text as sql_text
except ImportError:
    engine = None


# ğŸš¨ ExaoneLLMì€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import (Celery ë¡œë”© ì‹œì  ë¬¸ì œ íšŒí”¼)

# -----------------------------------------------------------
# [2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]
# -----------------------------------------------------------
PROMPT_TEMPLATE = """[|system|]
ë„ˆëŠ” 15ë…„ ì°¨ ë² í…Œë‘ {position} ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤. 
ì§€ê¸ˆì€ **ë©´ì ‘ì´ í•œì°½ ì§„í–‰ ì¤‘ì¸ ìƒí™©**ì´ë‹¤. (ìê¸°ì†Œê°œëŠ” ì´ë¯¸ ëë‚¬ë‹¤.)
ì œê³µëœ [ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±°] ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ë‹¨ê³„({stage})ì— ë§ëŠ” **í•µì‹¬ì ì¸ ì§ˆë¬¸ 1ê°œ**ë§Œ ë˜ì ¸ë¼.

[ì‘ì„± ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­] 
1. **"ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤" ì ˆëŒ€ ê¸ˆì§€.**
2. **"(ì ì‹œ ì¹¨ë¬µ)", "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤"** ê°™ì€ ëŒ€ë³¸ìš© ì§€ë¬¸ì„ ì“°ì§€ ë§ˆë¼.
3. **[í”„ë¡œì íŠ¸], [íšŒì‚¬ ëª…]** ê°™ì€ ìë¦¬í‘œì‹œì(Placeholder)ë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ë§ê³ , ê·¼ê±° ë°ì´í„°ì— ìˆëŠ” ì‹¤ì œ ëª…ì¹­ì„ ì¨ë¼.
4. ì§ˆë¬¸ ì•ë’¤ì— ì‚¬ì¡±ì„ ë¶™ì´ì§€ ë§ê³  **ì§ˆë¬¸ë§Œ ë”± í•œ ë¬¸ì¥(ìµœëŒ€ ë‘ ë¬¸ì¥)**ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.

[ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ]
1. ì‹œì‘ì€ ë°˜ë“œì‹œ **"{name}ë‹˜,"** ìœ¼ë¡œ ë¶€ë¥´ë©° ì‹œì‘í•  ê²ƒ.
2. ì§ˆë¬¸ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ í•µì‹¬ë§Œ ëª…í™•íˆ ë¬¼ì–´ë³¼ ê²ƒ. ê¼¬ì•„ë‚´ì§€ ë§ê³  ì •ê³µë²•ìœ¼ë¡œ ë¬¼ì–´ë³¼ ê²ƒ.
3. ë§íˆ¬ëŠ” ì •ì¤‘í•˜ê²Œ(..í•˜ì…¨ë‚˜ìš”?, ..ë¶€íƒë“œë¦½ë‹ˆë‹¤.) ìœ ì§€í•  ê²ƒ.
[|endofturn|]
[|user|]
# í‰ê°€ ë‹¨ê³„: {stage}
# í‰ê°€ ì˜ë„: {guide}
# ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±° (RAG):
{context}

# ìš”ì²­:
ìœ„ì˜ ê·¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ {name} ì§€ì›ìì—ê²Œ **êµ¬ì²´ì ì´ê³  ë‹¨ë„ì§ì…ì ì¸** ì§ˆë¬¸ì„ ë˜ì ¸ì¤˜.
[|endofturn|]
[|assistant|]
"""

# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# -----------------------------------------------------------
# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# [ê¸°ì¡´ ì¼ê´„ ìƒì„± íƒœìŠ¤í¬ ì‚­ì œë¨ - ì‹¤ì‹œê°„ ìƒì„± ëª¨ë“œë¡œ í†µí•©]

# -----------------------------------------------------------
# [5. Celery Task] - ì‹¤ì‹œê°„ 1ê°œì”© ìƒì„±í•˜ëŠ” íƒœìŠ¤í¬ (ìˆ˜ì • ì™„ë£Œ)
# -----------------------------------------------------------
@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    logger.info(f"ğŸ”¥ [START] generate_next_question_task for Interview {interview_id}")
    
    from db import (
        engine, Session, select, save_generated_question,
        Interview, Transcript, Speaker, Question, Resume

    )
    from config.interview_scenario import get_stage_by_name, get_next_stage
    from utils.exaone_llm import get_exaone_llm
    
    with Session(engine) as session:
        interview = session.get(Interview, interview_id)
        if not interview: 
            logger.error(f"Interview {interview_id} not found.")
            return {"status": "error", "message": "Interview not found"}
            
        # ğŸ” ë§ˆì§€ë§‰ ë‹¨ê³„ íƒì§€ ìµœì í™” (ìˆœì„œ ê¸°ë°˜ì´ ì•„ë‹Œ ID ê¸°ë°˜ ìµœì‹  ë°ì´í„° ì¡°íšŒ)
        stmt = select(Transcript).where(
            Transcript.interview_id == interview_id,
            Transcript.speaker == Speaker.AI
        ).order_by(Transcript.id.desc()) # IDê°€ ê°€ì¥ í° ê²ƒì´ ì ˆëŒ€ì ìœ¼ë¡œ ìµœì‹ 
        last_ai_transcript = session.exec(stmt).first()
        
        last_stage_name = None
        if last_ai_transcript:
            if last_ai_transcript.question_id:
                last_q = session.get(Question, last_ai_transcript.question_id)
                if last_q:
                    # 1ìˆœìœ„: DBì— ì €ì¥ëœ íƒ€ì… ì •ë³´ ì‚¬ìš©
                    last_stage_name = last_q.question_type
                    
                    # 2ìˆœìœ„ (Fallback): ì €ì¥ëœ íƒ€ì…ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ë‚´ìš©ìœ¼ë¡œ ìœ ì¶”
                    if not last_stage_name:
                        content = last_q.content
                        if "ìê¸°ì†Œê°œ" in content: last_stage_name = "intro"
                        elif "ì§€ì› ë™ê¸°" in content or "ì§€ì›í•˜ê²Œ ëœ" in content: last_stage_name = "motivation"
                        elif "ê¸°ìˆ " in content or "ìŠ¤í‚¬" in content or "ë„êµ¬" in content: last_stage_name = "skill"
                        elif "í”„ë¡œì íŠ¸" in content or "ê²½í—˜" in content: last_stage_name = "experience"
                        elif "ì–´ë ¤ì›€" in content or "í•´ê²°" in content: last_stage_name = "problem_solving"
            
            # 3ìˆœìœ„: transcriptì˜ orderë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—­ì¶”ì  (scenarioì˜ orderì™€ ë§¤ì¹­)
            if not last_stage_name and last_ai_transcript.order is not None:
                from config.interview_scenario import INTERVIEW_STAGES
                # transcript.orderëŠ” 0ë¶€í„° ì‹œì‘, scenario orderëŠ” 1ë¶€í„° ì‹œì‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì • í•„ìš”
                # ì—¬ê¸°ì„œëŠ” scenarioì˜ order í•„ë“œë¥¼ ê²€ìƒ‰
                for s in INTERVIEW_STAGES:
                    if s["order"] == last_ai_transcript.order + 1:
                        last_stage_name = s["stage"]
                        break

        # 4ìˆœìœ„: ë§¤í•‘ ë³´ì • (Legacy ë°ì´í„° ë“±)
        if last_stage_name == "technical": last_stage_name = "skill"
        
        if not last_stage_name:
            last_stage_name = "intro"

        logger.info(f"Detected Last Stage: {last_stage_name}")
        
        next_stage_data = get_next_stage(last_stage_name)
        if not next_stage_data:
            logger.info("Scenario Completed.")
            return {"status": "completed"}
            
        stage_name = next_stage_data["stage"]
        stage_type = next_stage_data.get("type", "ai")
        
        if stage_type == "template" or stage_type == "final":
            from utils.interview_helpers import get_candidate_info
            from db import Resume
            resume = session.get(Resume, interview.resume_id)
            c_info = get_candidate_info(resume.structured_data if resume else {})
            tmpl = next_stage_data.get("template", "{candidate_name}ë‹˜, ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤.")
            content = tmpl.format(candidate_name=c_info.get("candidate_name", "ì§€ì›ì"), target_role=interview.position)
            
            # QuestionCategory Enumì— 'general'ì´ ì—†ìœ¼ë¯€ë¡œ 'behavioral' ì‚¬ìš©
            save_generated_question(interview_id, content, "behavioral", stage_name, "")
            return {"status": "success", "stage": stage_name}

        # AI ìƒì„± ë£¨í‹´
        try:
            exaone = get_exaone_llm()
            
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„: ê¼¬ë¦¬ì§ˆë¬¸ vs ì¼ë°˜ AI ì§ˆë¬¸ ëª…í™•íˆ ë¶„ë¦¬
            contexts = []
            if stage_type == "followup":
                # ê¼¬ë¦¬ì§ˆë¬¸: ì˜¤ì§ ì´ì „ ë‹µë³€ë§Œ ì‚¬ìš© (RAG ê²€ìƒ‰ ì•ˆ í•¨)
                user_stmt = select(Transcript).where(
                    Transcript.interview_id == interview_id,
                    Transcript.speaker == Speaker.USER # Enum ê°’ì´ "User"ì´ë¯€ë¡œ ì¼ì¹˜í•¨
                ).order_by(Transcript.id.desc())
                last_user_ans = session.exec(user_stmt).first()
                if last_user_ans:
                    contexts = [{"text": f"ì´ì „ ë‹µë³€: {last_user_ans.text}", "meta": {"category": "followup"}}]
                    logger.info(f"ğŸ“Œ Follow-up context prepared from last answer.")
                else:
                    logger.warning("âš ï¸ No previous answer found for followup question!")
                    contexts = [{"text": "ì´ì „ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "meta": {}}]
            else:
                # ì¼ë°˜ AI ì§ˆë¬¸: ì´ë ¥ì„œ RAG ê²€ìƒ‰
                from .rag_retrieval import retrieve_context
                query_tmpl = next_stage_data.get("query_template", "{target_role}")
                query = query_tmpl.format(target_role=interview.position)
                contexts = retrieve_context(query, resume_id=interview.resume_id, top_k=3)

            
            # ì§€ì›ì ì •ë³´ ë° ì§ë¬´ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ë³´ê°• (JSON header/metadata ìš°ì„ )
            resume = session.get(Resume, interview.resume_id)
            candidate_name = "ì§€ì›ì"
            target_role = interview.position # ê¸°ë³¸ê°’ (ì¸í„°ë·° ì„¸ì…˜ ì„¤ì •ê°’)
            
            if resume and resume.structured_data:
                s_data = resume.structured_data
                header_data = s_data.get("header", {})
                
                # 1. ì´ë¦„ ì¶”ì¶œ (header -> User í…Œì´ë¸” ìˆœ)
                candidate_name = header_data.get("name") or header_data.get("candidate_name")
                if not candidate_name and resume.candidate_id:
                    from db import User
                    user = session.get(User, resume.candidate_id)
                    if user: candidate_name = user.full_name or user.username
                
                # 2. ì§ë¬´ ì¶”ì¶œ (headerì— ìˆìœ¼ë©´ ìµœìš°ì„ )
                target_role = header_data.get("target_role") or target_role

            logger.info(f"Target Candidate Name: {candidate_name}, Role: {target_role}")
            
            # 1. ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ì¡°ë¦½
            context_text = "\n".join([f"- {c['text']}" for c in contexts]) if contexts else "ì´ë ¥ì„œ ê·¼ê±° ë¶€ì¡±"
            
            # 2. PROMPT_TEMPLATEì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
            full_prompt = PROMPT_TEMPLATE.format(
                position=target_role,
                name=candidate_name,
                stage=stage_name,
                guide=next_stage_data.get("guide", "ì—­ëŸ‰ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."),
                context=context_text
            )
            
            logger.info(f"Generated Full Prompt length: {len(full_prompt)}")
            
            # 3. AI ì§ˆë¬¸ ìƒì„± ì‹¤í–‰ (ì—”ì§„ì—ê²ŒëŠ” ìƒì„±ë§Œ ìœ„ì„)
            content = exaone.invoke(full_prompt, max_tokens=256, temperature=0.6)
            
            if not content:
                content = f"{candidate_name}ë‹˜, ì¤€ë¹„í•˜ì‹  ë‚´ìš©ì„ í† ëŒ€ë¡œ í•´ë‹¹ ì—­ëŸ‰ì— ëŒ€í•´ ë” ë§ì”€í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?"
            
            # ì‹œë‚˜ë¦¬ì˜¤ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ DB Enumì— ë§ê²Œ ë§¤í•‘
            category_raw = next_stage_data.get("category", "technical")
            category_map = {
                "certification": "technical",
                "project": "technical",
                "narrative": "behavioral",
                "problem_solving": "situational"
            }
            db_category = category_map.get(category_raw, "technical")
            
            save_generated_question(interview_id, content, db_category, stage_name, next_stage_data.get("guide", ""))
            return {"status": "success", "stage": stage_name, "question": content}
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
        finally:
            gc.collect()
