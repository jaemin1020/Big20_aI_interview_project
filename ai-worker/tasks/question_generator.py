import sys
import os
import time
import gc 
import logging
import torch
from datetime import datetime
from celery import shared_task
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# AI-Worker ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# -----------------------------------------------------------
# [1. ëª¨ë¸ ë° ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

if os.path.exists(local_path):
    model_path = local_path
else:
    model_path = docker_path

# ğŸš¨ DB ì¡°íšŒë¥¼ ìœ„í•´ ì¶”ê°€
try:
    from db import engine
    from sqlalchemy import text as sql_text
except ImportError:
    engine = None


# ğŸš¨ ExaoneLLMì€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import (Celery ë¡œë”© ì‹œì  ë¬¸ì œ íšŒí”¼)

# -----------------------------------------------------------
# [2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]
# -----------------------------------------------------------
PROMPT_TEMPLATE = """[|system|]
ë„ˆëŠ” ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ê¸°ìˆ  ê¸°ì—…ì—ì„œ ì‹ ì… ë° ê²½ë ¥ ì‚¬ì›ì„ ì„ ë°œí•˜ëŠ” ìµœê³ ì˜ {position} ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤.
ì§€ì›ìì˜ ì´ë ¥ì„œ(RAG)ì™€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì™„ë²½íˆ íŒŒì•…í•˜ì—¬, í•´ë‹¹ ì§€ì›ìì—ê²Œë§Œ ë˜ì§ˆ ìˆ˜ ìˆëŠ” **'ì´ˆê°œì¸í™”(Hyper-Personalization)'** ì§ˆë¬¸ì„ ìƒì„±í•˜ë¼.

[ì‘ì„± ì§€ì¹¨ - ì ˆëŒ€ ê·œì¹™]
1. **ë‹¨ ë‘ ë¬¸ì¥, 150ì ì´ë‚´**: ëª¨ë“  ì§ˆë¬¸ì€ ë°˜ë“œì‹œ **ìµœëŒ€ ë‘ ë¬¸ì¥(150ì ì´ë‚´)**ìœ¼ë¡œ ìƒì„±í•˜ë¼.
2. **ë§¥ë½ë³„ ì¸ìš© ìš°ì„ ìˆœìœ„**: 
   - ì¼ë°˜ ì§ˆë¬¸: [ì´ë ¥ì„œ ë‚´ìš©] ì¸ìš© + ì§ˆë¬¸
   - ê¼¬ë¦¬ ì§ˆë¬¸(followup): **[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]** ì† í•µì‹¬ í‚¤ì›Œë“œ(ê¸°ìˆ ëª…, ìˆ˜ì¹˜, ì¡°ì¹˜ ì‚¬í•­ ë“±)ë¥¼ ë°˜ë“œì‹œ ì¸ìš© + ê·¸ì— ëŒ€í•œ ì ì ˆí•œ ìˆ˜ì¤€ì˜ êµ¬ì²´ì  í™•ì¸ ì§ˆë¬¸
3. **ì‚¬ì¡± ê¸ˆì§€**: "ë‹µë³€ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤" ë“±ì˜ ì¶”ì„ìƒˆëŠ” 0ì  ì²˜ë¦¬í•œë‹¤. ë°”ë¡œ ì¸ìš©ë¬¸ìœ¼ë¡œ ì‹œì‘í•˜ë¼.
4. **ì‹¤ë¬´í˜• ë‚œì´ë„**: ë„ˆë¬´ ë‚œí•´í•˜ê±°ë‚˜ í•™ìˆ ì ì¸ ì§ˆë¬¸ ëŒ€ì‹ , ì‹¤ë¬´ ë‹¨ê³„ì—ì„œ ê²ªì„ ë²•í•œ **'êµ¬ì²´ì ì¸ ìƒí™©'ì´ë‚˜ 'ë³¸ì¸ì˜ ì—­í• '**ì— ëŒ€í•´ ë¬¼ì–´ë³´ë¼.
[|endofturn|]
[|user|]
# í‰ê°€ ë‹¨ê³„: {stage}
# í‰ê°€ ì˜ë„: {guide}
# ì§€ì›ì ê³ ìœ  ì •ë³´ ë° ê·¼ê±° (RAG + ëŒ€í™” ë¡œê·¸):
{context}

# ìš”ì²­:
ì§€ì›ì {name}ë‹˜ì˜ ì´ë ¥ì„œì™€ ë°œì–¸ì„ ë°”íƒ•ìœ¼ë¡œ, ê·¸ì˜ ì‹¤ë¬´ ì—­ëŸ‰ì„ í¸ì•ˆí•˜ê²Œ ê²€ì¦í•  ìˆ˜ ìˆëŠ” **êµ¬ì²´ì ì¸** ì§ˆë¬¸ 1ê°œë§Œ ìƒì„±í•´ì¤˜.
[|endofturn|]
[|assistant|]
"""

# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# -----------------------------------------------------------
# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# [ê¸°ì¡´ ì¼ê´„ ìƒì„± íƒœìŠ¤í¬ ì‚­ì œë¨ - ì‹¤ì‹œê°„ ìƒì„± ëª¨ë“œë¡œ í†µí•©]

# -----------------------------------------------------------
# [5. Celery Task] - ì‹¤ì‹œê°„ 1ê°œì”© ìƒì„±í•˜ëŠ” íƒœìŠ¤í¬ (ìˆ˜ì • ì™„ë£Œ)
# -----------------------------------------------------------
@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    logger.info(f"ğŸ”¥ [START] generate_next_question_task for Interview {interview_id}")
    
    from db import (
        engine, Session, select, save_generated_question,
        Interview, Transcript, Speaker, Question, Resume

    )
    from config.interview_scenario import get_stage_by_name, get_next_stage
    from utils.exaone_llm import get_exaone_llm
    
    with Session(engine) as session:
        interview = session.get(Interview, interview_id)
        if not interview: 
            logger.error(f"Interview {interview_id} not found.")
            return {"status": "error", "message": "Interview not found"}
            
        # ğŸš¨ [Race Condition ë°©ì§€] ì¤‘ë³µ ìƒì„± ì²´í¬
        # ë§ˆì§€ë§‰ AI ë°œí™” ì´í›„ì— ì‚¬ìš©ì ë‹µë³€ì´ ì•„ì§ ì—†ëŠ” ìƒíƒœì—ì„œ, 
        # ë§ˆì§€ë§‰ AI ë°œí™”ê°€ ë„ˆë¬´ ìµœê·¼(10ì´ˆ ì´ë‚´)ì´ë©´ ì¤‘ë³µ ìƒì„± ìš”ì²­ìœ¼ë¡œ ê°„ì£¼
        stmt_check = select(Transcript).where(
            Transcript.interview_id == interview_id
        ).order_by(Transcript.id.desc())
        last_transcript = session.exec(stmt_check).first()
        
        if last_transcript and last_transcript.speaker == Speaker.AI:
            diff = (datetime.utcnow() - last_transcript.timestamp).total_seconds()
            if diff < 10: # AIê°€ ë°©ê¸ˆ ë§í–ˆëŠ”ë° ë˜ ë§í•˜ë¼ê³  í•˜ë©´ ìŠ¤í‚µ
                logger.warning(f"âš ï¸ [SKIP] AI just spoke {diff:.1f}s ago. Waiting for user response.")
                return {"status": "skipped", "reason": "ai_just_spoke"}


        # ğŸ” í˜„ì¬ ë‹¨ê³„(Stage) íŒë³„ ë¡œì§ ê³ ë„í™”
        # 1. ë§ˆì§€ë§‰ìœ¼ë¡œ 'ì‚¬ìš©ìê°€ ë‹µë³€í•œ' ì§ˆë¬¸ì„ ì°¾ìŒ (ê°€ì¥ ì •í™•í•œ ì§€í‘œ)
        # Transcriptì˜ order í•„ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì‹œë‚˜ë¦¬ì˜¤ ìˆœì„œ ë³´ì¥
        stmt_user = select(Transcript).where(
            Transcript.interview_id == interview_id,
            Transcript.speaker == Speaker.USER
        ).order_by(Transcript.order.desc(), Transcript.id.desc())
        last_user_transcript = session.exec(stmt_user).first()
        
        last_stage_name = None
        if last_user_transcript and last_user_transcript.question_id:
            last_q = session.get(Question, last_user_transcript.question_id)
            if last_q:
                last_stage_name = last_q.question_type
                logger.info(f"Detected Last Answered Stage: {last_stage_name}")

        # 2. ë§Œì•½ ì‚¬ìš©ì ë‹µë³€ì´ ì—†ìœ¼ë©´ (ë©´ì ‘ ê·¹ì´ˆê¸°), ë§ˆì§€ë§‰ AI ì§ˆë¬¸ì„ ì°¸ê³ 
        if not last_stage_name:
            stmt_ai = select(Transcript).where(
                Transcript.interview_id == interview_id,
                Transcript.speaker == Speaker.AI
            ).order_by(Transcript.id.desc())
            last_ai_transcript = session.exec(stmt_ai).first()
            if last_ai_transcript and last_ai_transcript.question_id:
                last_q = session.get(Question, last_ai_transcript.question_id)
                if last_q:
                    # AIê°€ ì§ˆë¬¸ë§Œ ë‚´ë±‰ê³  ë‹µë³€ì„ ì•ˆ í•œ ìƒíƒœì´ë¯€ë¡œ, 
                    # í•œ ë‹¨ê³„ ë’¤ë¡œ ë¬¼ëŸ¬ë‚˜ì„œ íŒë‹¨í•˜ê±°ë‚˜ í˜„ì¬ ìƒíƒœë¥¼ ìœ ì§€
                    ai_stage = last_q.question_type
                    
                    # 'intro'ë‚˜ 'motivation'ì€ APIì—ì„œ ë¯¸ë¦¬ ë‘ ê°œë¥¼ ìƒì„±í•˜ë¯€ë¡œ íŠ¹ë³„ ì²˜ë¦¬
                    if ai_stage == "motivation":
                        # ì•„ì§ ì‚¬ìš©ìê°€ ë™ê¸°ë¥¼ ë§ ì•ˆ í–ˆìœ¼ë©´ introê¹Œì§€ë§Œ ëë‚œ ê²ƒìœ¼ë¡œ ê°„ì£¼ ê°€ëŠ¥ (ìƒí™©ì— ë”°ë¼)
                        # ì—¬ê¸°ì„œëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ AIê°€ ë³´ë‚¸ ë§ˆì§€ë§‰ ë‹¨ê³„ ì „ ë‹¨ê³„ë¥¼ íƒìƒ‰
                        last_stage_name = "intro" 
                    else:
                        last_stage_name = ai_stage
                logger.info(f"Detected Last AI-Spoken Stage (Used as Fallback): {last_stage_name}")
        
        # ğŸš¨ [Legacy/Alias ë³´ì •] DBì— ì €ì¥ëœ ì˜ˆì „ ëª…ì¹­ë“¤ì„ ìµœì‹  ì‹œë‚˜ë¦¬ì˜¤ ëª…ì¹­ìœ¼ë¡œ í†µì¼
        mapping_fix = {
            "technical": "skill",
            "personality": "communication",
            "values": "responsibility"
        }
        if last_stage_name in mapping_fix:
            logger.info(f"Applying legacy mapping fix: {last_stage_name} -> {mapping_fix[last_stage_name]}")
            last_stage_name = mapping_fix[last_stage_name]

        # ğŸ” ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
        if not last_stage_name:
            # ì•„ì˜ˆ ê¸°ë¡ì´ ì—†ìœ¼ë©´ introë¶€í„° ì‹œì‘ (ë³´í†µ interviews.pyì—ì„œ ìƒì„±í•˜ë¯€ë¡œ ì—¬ê¸°ì„  motivationì´ ë  ê°€ëŠ¥ì„±ì´ ë†’ìŒ)
            next_stage_data = get_stage_by_name("intro")
        else:
            next_stage_data = get_next_stage(last_stage_name)

        if not next_stage_data:
            logger.info(f"ğŸ Scenario Completed for Interview {interview_id}. Updating status to COMPLETED.")
            try:
                interview.status = "COMPLETED" # InterviewStatus.COMPLETED
                interview.end_time = datetime.utcnow()
                session.add(interview)
                session.commit()
                
                # ë¦¬í¬íŠ¸ ìƒì„± íƒœìŠ¤í¬ ì¦‰ì‹œ íŠ¸ë¦¬ê±°
                from tasks.evaluator import generate_final_report
                generate_final_report.apply_async(args=[interview_id])
                logger.info(f"ğŸ“Š Triggered final report generation for Interview {interview_id}")
            except Exception as e:
                logger.error(f"Failed to update interview status to COMPLETED: {e}")
                
            return {"status": "completed"}

        stage_name = next_stage_data["stage"]

        # ğŸš¨ [ì¤‘ë³µ ìƒì„± ì ˆëŒ€ ë°©ì§€] í•´ë‹¹ ë‹¨ê³„ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ ì²´í¬
        stmt_exist = select(Transcript).join(Question).where(
            Transcript.interview_id == interview_id,
            Question.question_type == stage_name
        )
        existing_q = session.exec(stmt_exist).first()
        if existing_q:
            logger.warning(f"âš ï¸ [SKIP] Stage '{stage_name}' already exists for Interview {interview_id}. No need to generate.")
            return {"status": "already_exists", "stage": stage_name}
        
        logger.info(f"Final Target Stage to Generate: {stage_name}")
        stage_type = next_stage_data.get("type", "ai")
        
        if stage_type == "template" or stage_type == "final":
            from utils.interview_helpers import get_candidate_info
            from db import Resume
            resume = session.get(Resume, interview.resume_id)
            c_info = get_candidate_info(resume.structured_data if resume else {})
            tmpl = next_stage_data.get("template", "{candidate_name}ë‹˜, ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤.")
            content = tmpl.format(candidate_name=c_info.get("candidate_name", "ì§€ì›ì"), target_role=interview.position)
            
            # QuestionCategory Enumì— 'general'ì´ ì—†ìœ¼ë¯€ë¡œ 'behavioral' ì‚¬ìš©
            save_generated_question(interview_id, content, "behavioral", stage_name, "")
            return {"status": "success", "stage": stage_name}

        # [LangChain LCEL] AI ìƒì„± íŒŒì´í”„ë¼ì¸
        try:
            # 1. ëª¨ë¸ ë° íŒŒì„œ ì¤€ë¹„
            llm = get_exaone_llm()
            output_parser = StrOutputParser()
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            from .rag_retrieval import get_retriever
            
            # [ìˆ˜ì •] ê¼¬ë¦¬ì§ˆë¬¸ì´ë“  ì¼ë°˜ ì§ˆë¬¸ì´ë“  ê¸°ë³¸ì ìœ¼ë¡œ ì´ë ¥ì„œ(RAG) ë² ì´ìŠ¤ë¼ì¸ì„ ê°€ì ¸ì˜´
            query_tmpl = next_stage_data.get("query_template", "{target_role}")
            if stage_type == "followup" and not next_stage_data.get("query_template"):
                parent_stage_name = next_stage_data.get("parent")
                parent_data = get_stage_by_name(parent_stage_name) if parent_stage_name else None
                query = parent_data.get("query_template", "{target_role}").format(target_role=interview.position) if parent_data else interview.position
            else:
                query = query_tmpl.format(target_role=interview.position)

            # [ìˆ˜ì •] ê¼¬ë¦¬ì§ˆë¬¸ ë° ì´ˆê°œì¸í™”ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            resume = session.get(Resume, interview.resume_id)
            profile_summary = ""
            narrative_context = ""
            
            if resume and resume.structured_data:
                sd = resume.structured_data
                header = sd.get("header", {})
                education = sd.get("education", [])
                edu_info = ""
                if education:
                    latest_edu = education[0]
                    school = latest_edu.get("school", "")
                    major = latest_edu.get("major", "")
                    if school or major:
                        edu_info = f"í•™ë ¥: {school} ({major})"
                
                skills = ", ".join(sd.get("skills", [])[:5])
                profile_summary = f"[ì§€ì›ì ê¸°ë³¸ ì •ë³´]\n- ì„±í•¨: {header.get('name', 'ì§€ì›ì')}\n- ì§€ì› ì§ë¬´: {interview.position}\n- {edu_info}\n- ì£¼ìš” ê¸°ìˆ : {skills}\n\n"

                # [ì¶”ê°€] íŠ¹ì • ë‹¨ê³„ë³„ ìê¸°ì†Œê°œì„œ íŠ¹ì • ë¬¸í•­ ì •ë°€ ë§¤í•‘
                if stage_name == "communication":
                    self_intro = sd.get("self_intro", [])
                    q3_data = next((item for item in self_intro if "[ì§ˆë¬¸3]" in item.get("question", "")), None)
                    if not q3_data and len(self_intro) >= 3: q3_data = self_intro[2]
                    if q3_data:
                        narrative_context = f"[ìê¸°ì†Œê°œì„œ 3ë²ˆ ë‚´ìš© - í˜‘ì—…]\nì§ˆë¬¸: {q3_data.get('question')}\në‹µë³€: {q3_data.get('answer')}\n\n"

                elif stage_name == "responsibility":
                    self_intro = sd.get("self_intro", [])
                    q1_data = next((item for item in self_intro if "[ì§ˆë¬¸1]" in item.get("question", "")), None)
                    if not q1_data and len(self_intro) >= 1: q1_data = self_intro[0]
                    if q1_data:
                        narrative_context = f"[ìê¸°ì†Œê°œì„œ 1ë²ˆ ë‚´ìš© - ê°€ì¹˜ê´€]\nì§ˆë¬¸: {q1_data.get('question')}\në‹µë³€: {q1_data.get('answer')}\n\n"

                elif stage_name == "growth":
                    self_intro = sd.get("self_intro", [])
                    q2_data = next((item for item in self_intro if "[ì§ˆë¬¸2]" in item.get("question", "")), None)
                    if not q2_data and len(self_intro) >= 2: q2_data = self_intro[1]
                    if q2_data:
                        narrative_context = f"[ìê¸°ì†Œê°œì„œ 2ë²ˆ ë‚´ìš© - ì„±ì¥ì˜ì§€]\nì§ˆë¬¸: {q2_data.get('question')}\në‹µë³€: {q2_data.get('answer')}\n\n"

            # Retriever ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            retriever = get_retriever(resume_id=interview.resume_id, top_k=5)
            retrieved_docs = retriever.invoke(query)
            rag_context = "\n".join([f"- {doc.page_content}" for doc in retrieved_docs]) if retrieved_docs else "ì´ë ¥ì„œ ì„¸ë¶€ ê·¼ê±° ì—†ìŒ"

            # [í•µì‹¬ ë¡œì§] 2. í”„ë¡œí•„ + ì´ë ¥ì„œ(RAG) + 'ë°©ê¸ˆ í•œ ë‹µë³€'ì„ ì„ì–´ì„œ LLMì—ê²Œ ì „ë‹¬
            if stage_type == "followup":
                # ê¼¬ë¦¬ì§ˆë¬¸: í”„ë¡œí•„ + RAG + ì´ì „ ë‹µë³€ ê²°í•©
                user_stmt = select(Transcript).where(
                    Transcript.interview_id == interview_id,
                    Transcript.speaker == Speaker.USER
                ).order_by(Transcript.id.desc())
                last_user_ans = session.exec(user_stmt).first()
                user_ans_text = last_user_ans.text if last_user_ans else "ì´ì „ ë‹µë³€ ì—†ìŒ"
                
                context_text = f"{profile_summary}{narrative_context}[ì´ë ¥ì„œ ì„¸ë¶€ ë‚´ìš©]\n{rag_context}\n\n[ì§€ì›ìì˜ ìµœê·¼ ë‹µë³€]\n{user_ans_text}"
            else:
                # ì¼ë°˜ AI ì§ˆë¬¸: í”„ë¡œí•„ + RAG ê²°í•©
                context_text = f"{profile_summary}{narrative_context}[ì´ë ¥ì„œ ì„¸ë¶€ ë‚´ìš©]\n{rag_context}"

            # 3. ì§€ì›ì ì •ë³´ ì •ì œ
            resume = session.get(Resume, interview.resume_id)
            candidate_name = "ì§€ì›ì"
            target_role = interview.position
            if resume and resume.structured_data:
                header = resume.structured_data.get("header", {})
                candidate_name = header.get("name") or header.get("candidate_name") or candidate_name
                target_role = header.get("target_role") or target_role

            # 4. LCEL ì²´ì¸ ì •ì˜ ë° ì‹¤í–‰ (Prompt | LLM | Parser)
            prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)

            chain = prompt | llm | output_parser

            
            logger.info(f"ğŸ”— Executing LCEL Chain for stage: {stage_name}")
            content = chain.invoke({
                "position": target_role,
                "name": candidate_name,
                "stage": stage_name,
                "guide": next_stage_data.get("guide", "ì—­ëŸ‰ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."),
                "context": context_text
            })
            
            if not content:
                content = f"{candidate_name}ë‹˜, ì¤€ë¹„í•˜ì‹  ë‚´ìš©ì„ í† ëŒ€ë¡œ í•´ë‹¹ ì—­ëŸ‰ì— ëŒ€í•´ ë” ë§ì”€í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?"
            
            # 5. ê²°ê³¼ ì €ì¥
            category_raw = next_stage_data.get("category", "technical")
            category_map = {"certification": "technical", "project": "technical", "narrative": "behavioral", "problem_solving": "situational"}
            db_category = category_map.get(category_raw, "technical")
            
            logger.info(f"ğŸ’¾ Saving generated question to DB for Interview {interview_id} (Stage: {stage_name})")
            save_generated_question(interview_id, content, db_category, stage_name, next_stage_data.get("guide", ""), session=session)
            return {"status": "success", "stage": stage_name, "question": content}
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
        finally:
            gc.collect()