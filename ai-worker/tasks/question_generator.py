import os
import logging
from celery import shared_task
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ë¡œë“œ (HuggingFace Pipeline ì‚¬ìš©)
MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"

class QuestionGenerator:
    """
    Llama 3.2-3B ëª¨ë¸ì„ ì‚¬ìš©í•œ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸°
    4-bit ì–‘ìí™”ë¡œ VRAM ì‚¬ìš©ëŸ‰ ìµœì†Œí™” (~4GB)
    """
    _instance = None  # ì‹±ê¸€í†¤ íŒ¨í„´
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        logger.info(f"Loading Llama model with 4-bit quantization: {MODEL_ID}")
        token = os.getenv("HUGGINGFACE_HUB_TOKEN")
        
        # BitsAndBytes 4-bit ì–‘ìí™” ì„¤ì • (VRAM ì‚¬ìš©ëŸ‰: ~4GBë¡œ ì¶•ì†Œ)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,                    # 4ë¹„íŠ¸ ì–‘ìí™” í™œì„±í™”
            bnb_4bit_compute_dtype=torch.float16, # ì—°ì‚°ì€ FP16ìœ¼ë¡œ ìˆ˜í–‰
            bnb_4bit_use_double_quant=True,       # ì¤‘ì²© ì–‘ìí™” (ë©”ëª¨ë¦¬ ì¶”ê°€ ì ˆê°)
            bnb_4bit_quant_type="nf4"             # NormalFloat4 (LLMì— ìµœì í™”)
        )
        
        logger.info("Initializing tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=token)
        
        logger.info("Loading 4-bit quantized model (this may take 1-2 minutes)...")
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="auto",                    # GPU ìë™ í• ë‹¹
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,               # CPU ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œí™”
            token=token
        )
        
        logger.info("âœ… Model loaded successfully with 4-bit quantization")
        logger.info(f"ğŸ“Š Estimated VRAM usage: ~4GB (original: ~16GB)")
        
        # Pipeline ìƒì„±
        pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=80,  # ì§ˆë¬¸ë§Œ ìƒì„±í•˜ë„ë¡ í† í° ìˆ˜ ì¶”ê°€ ê°ì†Œ
            temperature=0.7,  # ì¼ê´€ì„± í–¥ìƒ
            top_p=0.9,
            repetition_penalty=1.3,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id  # íŒ¨ë”© í† í° ëª…ì‹œ
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)
        self._initialized = True
        
    def generate_questions(self, position: str, count: int = 5, previous_qa: list = None):
        """
        ë©´ì ‘ ì§ˆë¬¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            position: ì§€ì› ì§ë¬´ (ì˜ˆ: "Frontend ê°œë°œì")
            count: ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜
            previous_qa: ì´ì „ ì§ˆë¬¸-ë‹µë³€ ìŒ ë¦¬ìŠ¤íŠ¸ [{"question": "...", "answer": "..."}]
        
        Returns:
            list: ìƒì„±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        questions = []
        
        for i in range(count):
            # ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = ""
            if previous_qa and len(previous_qa) > 0:
                context = "\n### ì´ì „ ëŒ€í™”:\n"
                for qa in previous_qa[-3:]:  # ìµœê·¼ 3ê°œë§Œ ì°¸ì¡°
                    context += f"ë©´ì ‘ê´€: {qa['question']}\n"
                    context += f"ì§€ì›ì: {qa['answer']}\n"
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í•œêµ­ì–´ ê°•ì œ, ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜)
            if i == 0 and not previous_qa:
                # ì²« ì§ˆë¬¸: ì§ë¬´ ê´€ë ¨ ê¸°ë³¸ ì§ˆë¬¸
                prompt_template = """### ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­:
ë‹¹ì‹ ì€ {position} ì§ë¬´ì˜ ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
ì§€ì›ìì—ê²Œ í•  ë©´ì ‘ ì§ˆë¬¸ í•˜ë‚˜ë§Œ ì‘ì„±í•˜ì„¸ìš”.

### ê·œì¹™:
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±
2. ì§ˆë¬¸ í•˜ë‚˜ë§Œ ì‘ì„± (ë‹µë³€ ì‘ì„± ê¸ˆì§€)
3. ì‹¤ë¬´ ì¤‘ì‹¬ì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸
4. ì§ˆë¬¸ì€ "~í•´ì£¼ì„¸ìš”" ë˜ëŠ” "~ë¬´ì—‡ì¸ê°€ìš”?" í˜•ì‹ìœ¼ë¡œ ëë‚  ê²ƒ
5. ì§ˆë¬¸ ì™¸ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

### ë©´ì ‘ê´€ ì§ˆë¬¸:
"""
            else:
                prompt_template = """### ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­:
ë‹¹ì‹ ì€ {position} ì§ë¬´ì˜ ì „ë¬¸ ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
{context}
ì§€ì›ìì—ê²Œ í•  ë‹¤ìŒ ë©´ì ‘ ì§ˆë¬¸ í•˜ë‚˜ë§Œ ì‘ì„±í•˜ì„¸ìš”.

### ê·œì¹™:
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±
2. ì§ˆë¬¸ í•˜ë‚˜ë§Œ ì‘ì„± (ë‹µë³€ ì‘ì„± ê¸ˆì§€)
3. ì´ì „ ë‹µë³€ê³¼ ì—°ê´€ëœ ì‹¬í™” ì§ˆë¬¸ ë˜ëŠ” ìƒˆë¡œìš´ ê°ë„ì˜ ì§ˆë¬¸
4. ì§ˆë¬¸ì€ "~í•´ì£¼ì„¸ìš”" ë˜ëŠ” "~ë¬´ì—‡ì¸ê°€ìš”?" í˜•ì‹ìœ¼ë¡œ ëë‚  ê²ƒ
5. ì§ˆë¬¸ ì™¸ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

### ë©´ì ‘ê´€ ì§ˆë¬¸:
"""
            
            prompt = PromptTemplate.from_template(prompt_template)
            chain = prompt | self.llm | StrOutputParser()
            
            try:
                result = chain.invoke({
                    "position": position,
                    "context": context
                })
                
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ ì¶”ì¶œ (ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°)
                question = self._extract_question(result)
                
                if question:
                    questions.append(question)
                    logger.info(f"Generated question {i+1}/{count}: {question}")
                else:
                    # ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ ì‹œ í´ë°±
                    fallback = self._get_fallback_question(position, i)
                    questions.append(fallback)
                    logger.warning(f"Using fallback question {i+1}/{count}")
                    
            except Exception as e:
                logger.error(f"Question generation error: {e}")
                fallback = self._get_fallback_question(position, i)
                questions.append(fallback)
        
        return questions
    
    def _extract_question(self, raw_output: str) -> str:
        """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ì§ˆë¬¸ë§Œ ì¶”ì¶œ"""
        # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬
        lines = [line.strip() for line in raw_output.split('\n') if line.strip()]
        
        # ì ‘ë‘ì‚¬ ì œê±° ë° ì •ë¦¬
        cleaned_lines = []
        for line in lines:
            # "ë©´ì ‘ê´€:", "ì§ˆë¬¸:", "###" ë“± ì ‘ë‘ì‚¬ ì œê±°
            line = line.replace("ë©´ì ‘ê´€:", "").replace("ì§ˆë¬¸:", "").replace("###", "").strip()
            # "ì§€ì›ì:", "ë‹µë³€:" ë“±ì´ í¬í•¨ëœ ì¤„ì€ ì œì™¸ (ë‹µë³€ ìƒì„± ë°©ì§€)
            if any(keyword in line for keyword in ["ì§€ì›ì:", "ë‹µë³€:", "ì˜ˆì‹œ:", "A:", "Answer:"]):
                continue
            # ì§ˆë¬¸ í˜•ì‹ìœ¼ë¡œ ëë‚˜ëŠ” ë¬¸ì¥ë§Œ ì„ íƒ
            if line.endswith(("?", "ê°€ìš”?", "ë‚˜ìš”?", "ì„¸ìš”?", "ì£¼ì„¸ìš”.", "ì£¼ì„¸ìš”?")):
                cleaned_lines.append(line)
        
        # ê°€ì¥ ê¸´ ì§ˆë¬¸ ë¬¸ì¥ ì„ íƒ
        if cleaned_lines:
            question = max(cleaned_lines, key=len)
            # ìµœì†Œ ê¸¸ì´ ê²€ì¦
            if len(question) > 10 and len(question) < 200:
                return question
        
        # ì •ì œëœ ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return ""
    
    def _get_fallback_question(self, position: str, index: int) -> str:
        """ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ì§ˆë¬¸"""
        fallback_questions = [
            f"{position} ì§ë¬´ì— ì§€ì›í•˜ê²Œ ëœ ë™ê¸°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            f"{position} ë¶„ì•¼ì—ì„œ ê°€ì¥ ìì‹  ìˆëŠ” ê¸°ìˆ ì´ë‚˜ ê²½í—˜ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ìµœê·¼ ì§„í–‰í•œ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ê¸°ìˆ ì  ë¬¸ì œë¥¼ í•´ê²°í–ˆë˜ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì„¸ìš”.",
            "íŒ€ í˜‘ì—… ê³¼ì •ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªì—ˆë˜ ê²½í—˜ê³¼ í•´ê²° ë°©ë²•ì„ ë§ì”€í•´ì£¼ì„¸ìš”."
        ]
        return fallback_questions[index % len(fallback_questions)]


# Celery íƒœìŠ¤í¬ ì •ì˜
@shared_task(
    name="tasks.question_generator.generate_questions",
    bind=True,
    max_retries=3,
    default_retry_delay=10
)
def generate_questions_task(self, position: str, count: int = 5, previous_qa: list = None):
    """
    Celery íƒœìŠ¤í¬: ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
    
    Args:
        position: ì§€ì› ì§ë¬´
        count: ìƒì„±í•  ì§ˆë¬¸ ê°œìˆ˜
        previous_qa: ì´ì „ ì§ˆë¬¸-ë‹µë³€ ìŒ (ì„ íƒ)
    
    Returns:
        list: ìƒì„±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    try:
        logger.info(f"Starting question generation for position: {position}, count: {count}")
        generator = QuestionGenerator()
        questions = generator.generate_questions(position, count, previous_qa)
        logger.info(f"Successfully generated {len(questions)} questions")
        return questions
    except Exception as e:
        logger.error(f"Question generation task failed: {str(e)}")
        # ì¬ì‹œë„ ë¡œì§
        raise self.retry(exc=e)
