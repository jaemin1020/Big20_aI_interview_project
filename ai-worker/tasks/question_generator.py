import sys
import os
import time
import gc 
import logging
import torch
<<<<<<< HEAD
<<<<<<< HEAD
from datetime import datetime
=======
>>>>>>> bcab0a98e56e154aae50f9fad3ffa7ac7d936acf
=======
from datetime import datetime
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
from celery import shared_task
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager
from langchain_core.prompts import PromptTemplate
<<<<<<< HEAD
<<<<<<< HEAD
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
=======
>>>>>>> bcab0a98e56e154aae50f9fad3ffa7ac7d936acf
=======
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea

# AI-Worker ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# -----------------------------------------------------------
# [1. ëª¨ë¸ ë° ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
if os.path.exists(local_path):
    model_path = local_path
else:
    model_path = docker_path
<<<<<<< HEAD
=======
model_path = local_path if os.path.exists(local_path) else docker_path
>>>>>>> bcab0a98e56e154aae50f9fad3ffa7ac7d936acf
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea

# ğŸš¨ DB ì¡°íšŒë¥¼ ìœ„í•´ ì¶”ê°€
try:
    from db import engine
    from sqlalchemy import text as sql_text
except ImportError:
    engine = None


# ğŸš¨ ExaoneLLMì€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import (Celery ë¡œë”© ì‹œì  ë¬¸ì œ íšŒí”¼)

# -----------------------------------------------------------
# [2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]
# -----------------------------------------------------------
PROMPT_TEMPLATE = """[|system|]
ë„ˆëŠ” 15ë…„ ì°¨ ë² í…Œë‘ {position} ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤. 
ì§€ê¸ˆì€ **ë©´ì ‘ì´ í•œì°½ ì§„í–‰ ì¤‘ì¸ ìƒí™©**ì´ë‹¤. (ìê¸°ì†Œê°œëŠ” ì´ë¯¸ ëë‚¬ë‹¤.)
ì œê³µëœ [ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±°] ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ë‹¨ê³„({stage})ì— ë§ëŠ” **í•µì‹¬ì ì¸ ì§ˆë¬¸ 1ê°œ**ë§Œ ë˜ì ¸ë¼.

[ì‘ì„± ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­] 
1. **"ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤" ì ˆëŒ€ ê¸ˆì§€.**
2. **"(ì ì‹œ ì¹¨ë¬µ)", "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤"** ê°™ì€ ëŒ€ë³¸ìš© ì§€ë¬¸ì„ ì“°ì§€ ë§ˆë¼.
3. **[í”„ë¡œì íŠ¸], [íšŒì‚¬ ëª…]** ê°™ì€ ìë¦¬í‘œì‹œì(Placeholder)ë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ë§ê³ , ê·¼ê±° ë°ì´í„°ì— ìˆëŠ” ì‹¤ì œ ëª…ì¹­ì„ ì¨ë¼.
4. ì§ˆë¬¸ ì•ë’¤ì— ì‚¬ì¡±ì„ ë¶™ì´ì§€ ë§ê³  **ì§ˆë¬¸ë§Œ ë”± í•œ ë¬¸ì¥(ìµœëŒ€ ë‘ ë¬¸ì¥)**ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.

[ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ]
1. ì‹œì‘ì€ ë°˜ë“œì‹œ **"{name}ë‹˜,"** ìœ¼ë¡œ ë¶€ë¥´ë©° ì‹œì‘í•  ê²ƒ.
2. ì§ˆë¬¸ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ í•µì‹¬ë§Œ ëª…í™•íˆ ë¬¼ì–´ë³¼ ê²ƒ. ê¼¬ì•„ë‚´ì§€ ë§ê³  ì •ê³µë²•ìœ¼ë¡œ ë¬¼ì–´ë³¼ ê²ƒ.
3. ë§íˆ¬ëŠ” ì •ì¤‘í•˜ê²Œ(..í•˜ì…¨ë‚˜ìš”?, ..ë¶€íƒë“œë¦½ë‹ˆë‹¤.) ìœ ì§€í•  ê²ƒ.
[|endofturn|]
[|user|]
# í‰ê°€ ë‹¨ê³„: {stage}
# í‰ê°€ ì˜ë„: {guide}
# ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±° (RAG):
{context}

# ìš”ì²­:
ìœ„ì˜ ê·¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ {name} ì§€ì›ìì—ê²Œ **êµ¬ì²´ì ì´ê³  ë‹¨ë„ì§ì…ì ì¸** ì§ˆë¬¸ì„ ë˜ì ¸ì¤˜.
[|endofturn|]
[|assistant|]
"""

# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# -----------------------------------------------------------
# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
<<<<<<< HEAD
<<<<<<< HEAD
# [ê¸°ì¡´ ì¼ê´„ ìƒì„± íƒœìŠ¤í¬ ì‚­ì œë¨ - ì‹¤ì‹œê°„ ìƒì„± ëª¨ë“œë¡œ í†µí•©]
=======
# -----------------------------------------------------------
def generate_human_like_question(exaone, name, position, stage, guide, context_list):
    """
    ExaoneLLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ìƒì„±
    """
    if not context_list:
        return f"âŒ (ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•´ ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"

    texts = [item['text'] for item in context_list] if isinstance(context_list[0], dict) else context_list
    context_text = "\n".join([f"- {txt}" for txt in texts])
    
    try:
        # ExaoneLLMì˜ generate_questions ë©”ì„œë“œ í™œìš© (ë‹¨ì¼ ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•´ count=1)
        # ë³´ë‹¤ ì •êµí•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•´ ì§ì ‘ generate í˜¸ì¶œ ê°€ëŠ¥í•˜ë‚˜, ì—¬ê¸°ì„œëŠ” ì¼ê´€ì„±ì„ ìœ„í•´ ë©í•‘
        system_msg = f"ë‹¹ì‹ ì€ 15ë…„ ì°¨ ë² í…Œë‘ {position} ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤. ì§€ê¸ˆì€ ë©´ì ‘ì´ í•œì°½ ì§„í–‰ ì¤‘ì¸ ìƒí™©ì´ë‹¤."
        user_msg = f"""ì§€ì›ì {name}ë‹˜ì—ê²Œ {stage} ë‹¨ê³„ì˜ ë©´ì ‘ ì§ˆë¬¸ì„ ë˜ì§€ì„¸ìš”.

[í‰ê°€ ì˜ë„ - ë°˜ë“œì‹œ ì´ ê´€ì ìœ¼ë¡œ ì§ˆë¬¸í•  ê²ƒ]
{guide}

[ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±°]
{context_text}

[ìš”êµ¬ì‚¬í•­]
1. ì‹œì‘ì€ ë°˜ë“œì‹œ "{name}ë‹˜," ìœ¼ë¡œ ë¶€ë¥¼ ê²ƒ.
2. **ì´ë ¥ì„œì— ë‚˜ì˜¨ êµ¬ì²´ì ì¸ í”„ë¡œì íŠ¸ëª…/íšŒì‚¬ëª…/ê¸°ìˆ ëª…ì„ ë°˜ë“œì‹œ ì–¸ê¸‰**í•  ê²ƒ.
   ì˜ˆ: "{name}ë‹˜, ì´ë ¥ì„œë¥¼ ë³´ë‹ˆ ì˜¤í”ˆì†ŒìŠ¤ ê¸°ë°˜ ì¹¨ì… íƒì§€ í”„ë¡œì íŠ¸ë¥¼ í•˜ì…¨ë„¤ìš”~"
3. **í‰ê°€ ì˜ë„(guide)ì— ë§ëŠ” ì§ˆë¬¸**ì„ í•  ê²ƒ.
   - ì˜ˆ: guideê°€ "êµ¬ì²´ì ì¸ ì—­í• ê³¼ ê¸°ì—¬ë„"ë¼ë©´ â†’ "ì´ í”„ë¡œì íŠ¸ì—ì„œ ë‹¬ì„±í•œ êµ¬ì²´ì ì¸ ì—­í• ê³¼ ê¸°ì—¬ë„ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
4. ë°˜ë“œì‹œ **150ì ì´ë‚´(ë‘ ë¬¸ì¥ ì´ë‚´)**ë¡œ ì§§ê³  ëª…í™•í•˜ê²Œ ë¬¼ì–´ë³¼ ê²ƒ.
5. [í”„ë¡œì íŠ¸], [íšŒì‚¬ëª…] ê°™ì€ ìë¦¬í‘œì‹œìë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ.
"""

        prompt = exaone._create_prompt(system_msg, user_msg)
        output = exaone.llm(
            prompt,
            max_tokens=512,
            stop=["[|endofturn|]", "[|user|]"],
            temperature=0.4,
            echo=False
        )
        return output['choices'][0]['text'].strip()
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ë©´ì ‘ì„ ì´ì–´ê°€ê² ìŠµë‹ˆë‹¤. {name}ë‹˜, ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤."

# -----------------------------------------------------------
# [4. Celery Task] - ê¸°ì¡´ ì¼ê´„ ìƒì„± íƒœìŠ¤í¬ (í•„ìš” ì‹œ ìœ ì§€)
# -----------------------------------------------------------
@shared_task(name="tasks.question_generation.generate_questions")
def generate_questions_task(interview_id, count=5, resume_id=None):
    from db import engine, Session, Resume, Interview
    from utils.exaone_llm import get_exaone_llm
    
    exaone = get_exaone_llm()
    
    with Session(engine) as session:
        # 1. ì¸í„°ë·° ì •ë³´ ë¡œë“œ (ëª…ì‹œì ì¸ resume_idê°€ ì—†ìœ¼ë©´ ì¸í„°ë·° ë ˆì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
        if not resume_id:
            interview = session.get(Interview, interview_id)
            if interview: resume_id = interview.resume_id
            
        if not resume_id:
            logger.error(f"Resume ID not found for interview {interview_id}")
            return exaone.generate_questions("ì¼ë°˜", count=count)

        resume = session.get(Resume, resume_id)
        if not resume:
            return exaone.generate_questions("ì¼ë°˜", count=count)

        # 2. ì´ë ¥ì„œ íŒŒì‹± ë°ì´í„°(header -> target_role) ì¶”ì¶œ (ë°ì´í„°ì˜ ìœ ì¼í•œ ì›ì²œ)
        s_data = resume.structured_data or {}
        header = s_data.get("header", {})
        target_role = header.get("target_role") or "ì¼ë°˜"
        
        # 3. ì´ë ¥ì„œ ì „ë¬¸(extracted_text) ê°€ì ¸ì˜¤ê¸°
        resume_context = resume.extracted_text or ""
        
        logger.info(f"ğŸš€ [Core Data] Name: {header.get('name')}, Target Role: {target_role}")
        
    return exaone.generate_questions(target_role, context=resume_context, count=count)
>>>>>>> bcab0a98e56e154aae50f9fad3ffa7ac7d936acf
=======
# [ê¸°ì¡´ ì¼ê´„ ìƒì„± íƒœìŠ¤í¬ ì‚­ì œë¨ - ì‹¤ì‹œê°„ ìƒì„± ëª¨ë“œë¡œ í†µí•©]
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea

# -----------------------------------------------------------
# [5. Celery Task] - ì‹¤ì‹œê°„ 1ê°œì”© ìƒì„±í•˜ëŠ” íƒœìŠ¤í¬ (ìˆ˜ì • ì™„ë£Œ)
# -----------------------------------------------------------
@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    logger.info(f"ğŸ”¥ [START] generate_next_question_task for Interview {interview_id}")
    
    from db import (
        engine, Session, select, save_generated_question,
        Interview, Transcript, Speaker, Question, Resume

    )
    from config.interview_scenario import get_stage_by_name, get_next_stage
    from utils.exaone_llm import get_exaone_llm
    
    with Session(engine) as session:
        interview = session.get(Interview, interview_id)
        if not interview: 
            logger.error(f"Interview {interview_id} not found.")
            return {"status": "error", "message": "Interview not found"}
            
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
        # ğŸš¨ [Race Condition ë°©ì§€] ì¤‘ë³µ ìƒì„± ì²´í¬
        # ë§ˆì§€ë§‰ AI ë°œí™” ì´í›„ì— ì‚¬ìš©ì ë‹µë³€ì´ ì•„ì§ ì—†ëŠ” ìƒíƒœì—ì„œ, 
        # ë§ˆì§€ë§‰ AI ë°œí™”ê°€ ë„ˆë¬´ ìµœê·¼(10ì´ˆ ì´ë‚´)ì´ë©´ ì¤‘ë³µ ìƒì„± ìš”ì²­ìœ¼ë¡œ ê°„ì£¼
        stmt_check = select(Transcript).where(
            Transcript.interview_id == interview_id
        ).order_by(Transcript.id.desc())
        last_transcript = session.exec(stmt_check).first()
        
        if last_transcript and last_transcript.speaker == Speaker.AI:
            diff = (datetime.utcnow() - last_transcript.timestamp).total_seconds()
            if diff < 10: # AIê°€ ë°©ê¸ˆ ë§í–ˆëŠ”ë° ë˜ ë§í•˜ë¼ê³  í•˜ë©´ ìŠ¤í‚µ
                logger.warning(f"âš ï¸ [SKIP] AI just spoke {diff:.1f}s ago. Waiting for user response.")
                return {"status": "skipped", "reason": "ai_just_spoke"}


<<<<<<< HEAD
=======
>>>>>>> bcab0a98e56e154aae50f9fad3ffa7ac7d936acf
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
        # ğŸ” ë§ˆì§€ë§‰ ë‹¨ê³„ íƒì§€ ìµœì í™” (ìˆœì„œ ê¸°ë°˜ì´ ì•„ë‹Œ ID ê¸°ë°˜ ìµœì‹  ë°ì´í„° ì¡°íšŒ)
        stmt = select(Transcript).where(
            Transcript.interview_id == interview_id,
            Transcript.speaker == Speaker.AI
        ).order_by(Transcript.id.desc()) # IDê°€ ê°€ì¥ í° ê²ƒì´ ì ˆëŒ€ì ìœ¼ë¡œ ìµœì‹ 
        last_ai_transcript = session.exec(stmt).first()
        
        last_stage_name = None
        if last_ai_transcript:
            if last_ai_transcript.question_id:
                last_q = session.get(Question, last_ai_transcript.question_id)
                if last_q:
                    # 1ìˆœìœ„: DBì— ì €ì¥ëœ íƒ€ì… ì •ë³´ ì‚¬ìš©
                    last_stage_name = last_q.question_type
                    
                    # 2ìˆœìœ„ (Fallback): ì €ì¥ëœ íƒ€ì…ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ë‚´ìš©ìœ¼ë¡œ ìœ ì¶”
                    if not last_stage_name:
                        content = last_q.content
                        if "ìê¸°ì†Œê°œ" in content: last_stage_name = "intro"
                        elif "ì§€ì› ë™ê¸°" in content or "ì§€ì›í•˜ê²Œ ëœ" in content: last_stage_name = "motivation"
                        elif "ê¸°ìˆ " in content or "ìŠ¤í‚¬" in content or "ë„êµ¬" in content: last_stage_name = "skill"
                        elif "í”„ë¡œì íŠ¸" in content or "ê²½í—˜" in content: last_stage_name = "experience"
                        elif "ì–´ë ¤ì›€" in content or "í•´ê²°" in content: last_stage_name = "problem_solving"
            
            # 3ìˆœìœ„: transcriptì˜ orderë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—­ì¶”ì  (scenarioì˜ orderì™€ ë§¤ì¹­)
            if not last_stage_name and last_ai_transcript.order is not None:
                from config.interview_scenario import INTERVIEW_STAGES
                # transcript.orderëŠ” 0ë¶€í„° ì‹œì‘, scenario orderëŠ” 1ë¶€í„° ì‹œì‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì • í•„ìš”
                # ì—¬ê¸°ì„œëŠ” scenarioì˜ order í•„ë“œë¥¼ ê²€ìƒ‰
                for s in INTERVIEW_STAGES:
                    if s["order"] == last_ai_transcript.order + 1:
                        last_stage_name = s["stage"]
                        break

        # 4ìˆœìœ„: ë§¤í•‘ ë³´ì • (Legacy ë°ì´í„° ë“±)
        if last_stage_name == "technical": last_stage_name = "skill"
        
        if not last_stage_name:
            last_stage_name = "intro"

        logger.info(f"Detected Last Stage: {last_stage_name}")
        
        next_stage_data = get_next_stage(last_stage_name)
        if not next_stage_data:
            logger.info("Scenario Completed.")
            return {"status": "completed"}
            
        stage_name = next_stage_data["stage"]
        stage_type = next_stage_data.get("type", "ai")
        
        if stage_type == "template" or stage_type == "final":
            from utils.interview_helpers import get_candidate_info
            from db import Resume
            resume = session.get(Resume, interview.resume_id)
            c_info = get_candidate_info(resume.structured_data if resume else {})
            tmpl = next_stage_data.get("template", "{candidate_name}ë‹˜, ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤.")
            content = tmpl.format(candidate_name=c_info.get("candidate_name", "ì§€ì›ì"), target_role=interview.position)
            
            # QuestionCategory Enumì— 'general'ì´ ì—†ìœ¼ë¯€ë¡œ 'behavioral' ì‚¬ìš©
            save_generated_question(interview_id, content, "behavioral", stage_name, "")
            return {"status": "success", "stage": stage_name}

<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
        # [LangChain LCEL] AI ìƒì„± íŒŒì´í”„ë¼ì¸
        try:
            # 1. ëª¨ë¸ ë° íŒŒì„œ ì¤€ë¹„
            llm = get_exaone_llm()
            output_parser = StrOutputParser()
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            from .rag_retrieval import get_retriever
            
            # [ìˆ˜ì •] ê¼¬ë¦¬ì§ˆë¬¸ì´ë“  ì¼ë°˜ ì§ˆë¬¸ì´ë“  ê¸°ë³¸ì ìœ¼ë¡œ ì´ë ¥ì„œ(RAG) ë² ì´ìŠ¤ë¼ì¸ì„ ê°€ì ¸ì˜´
            query_tmpl = next_stage_data.get("query_template", "{target_role}")
            if stage_type == "followup" and not next_stage_data.get("query_template"):
                parent_stage_name = next_stage_data.get("parent")
                parent_data = get_stage_by_name(parent_stage_name) if parent_stage_name else None
                query = parent_data.get("query_template", "{target_role}").format(target_role=interview.position) if parent_data else interview.position
            else:
                query = query_tmpl.format(target_role=interview.position)

            # Retriever ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            retriever = get_retriever(resume_id=interview.resume_id, top_k=2)
            retrieved_docs = retriever.invoke(query)
            rag_context = "\n".join([f"- {doc.page_content}" for doc in retrieved_docs]) if retrieved_docs else "ì´ë ¥ì„œ ê·¼ê±° ë¶€ì¡±"

            if stage_type == "followup":
                # ê¼¬ë¦¬ì§ˆë¬¸: RAG ì»¨í…ìŠ¤íŠ¸ + ì´ì „ ë‹µë³€ ê²°í•©
                user_stmt = select(Transcript).where(
                    Transcript.interview_id == interview_id,
                    Transcript.speaker == Speaker.USER
                ).order_by(Transcript.id.desc())
                last_user_ans = session.exec(user_stmt).first()
                user_ans_text = last_user_ans.text if last_user_ans else "ì´ì „ ë‹µë³€ ì—†ìŒ"
                
                context_text = f"[ì§€ì›ì ì´ë ¥ì„œ ê´€ë ¨ ì •ë³´]\n{rag_context}\n\n[ì§€ì›ìì˜ ì´ì „ ë‹µë³€]\n{user_ans_text}"
            else:
                # ì¼ë°˜ AI ì§ˆë¬¸: RAG ì»¨í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ í™œìš©
                context_text = rag_context

            # 3. ì§€ì›ì ì •ë³´ ì •ì œ
            resume = session.get(Resume, interview.resume_id)
            candidate_name = "ì§€ì›ì"
            target_role = interview.position
            if resume and resume.structured_data:
                header = resume.structured_data.get("header", {})
                candidate_name = header.get("name") or header.get("candidate_name") or candidate_name
                target_role = header.get("target_role") or target_role

            # 4. LCEL ì²´ì¸ ì •ì˜ ë° ì‹¤í–‰ (Prompt | LLM | Parser)
            prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
            
<<<<<<< HEAD
            # AI ì§ˆë¬¸ ìƒì„± ì‹¤í–‰
            content = exaone.generate_human_like_question(
                name=candidate_name,
<<<<<<< HEAD
=======
                position=target_role,
>>>>>>> 3c3c7ad852cb791ad6eea3c101528407d064e29d
                stage=stage_name,
                guide=next_stage_data.get("guide", "ì—­ëŸ‰ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."),
                context_list=contexts
            )
=======
            chain = prompt | llm | output_parser
>>>>>>> ë¦°_phase4
            
            logger.info(f"ğŸ”— Executing LCEL Chain for stage: {stage_name}")
            content = chain.invoke({
                "position": target_role,
                "name": candidate_name,
                "stage": stage_name,
                "guide": next_stage_data.get("guide", "ì—­ëŸ‰ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."),
                "context": context_text
            })
            
            if not content:
                content = f"{candidate_name}ë‹˜, ì¤€ë¹„í•˜ì‹  ë‚´ìš©ì„ í† ëŒ€ë¡œ í•´ë‹¹ ì—­ëŸ‰ì— ëŒ€í•´ ë” ë§ì”€í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?"
            
            # 5. ê²°ê³¼ ì €ì¥
            category_raw = next_stage_data.get("category", "technical")
            category_map = {"certification": "technical", "project": "technical", "narrative": "behavioral", "problem_solving": "situational"}
            db_category = category_map.get(category_raw, "technical")
            
            logger.info(f"ğŸ’¾ Saving generated question to DB for Interview {interview_id} (Stage: {stage_name})")
<<<<<<< HEAD
=======
        # AI ìƒì„± ë£¨í‹´
        try:
            exaone = get_exaone_llm()
            
            # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„: ê¼¬ë¦¬ì§ˆë¬¸ vs ì¼ë°˜ AI ì§ˆë¬¸ ëª…í™•íˆ ë¶„ë¦¬
            contexts = []
            if stage_type == "followup":
                # ê¼¬ë¦¬ì§ˆë¬¸: ì˜¤ì§ ì´ì „ ë‹µë³€ë§Œ ì‚¬ìš© (RAG ê²€ìƒ‰ ì•ˆ í•¨)
                user_stmt = select(Transcript).where(
                    Transcript.interview_id == interview_id,
                    Transcript.speaker == Speaker.USER # Enum ê°’ì´ "User"ì´ë¯€ë¡œ ì¼ì¹˜í•¨
                ).order_by(Transcript.id.desc())
                last_user_ans = session.exec(user_stmt).first()
                if last_user_ans:
                    contexts = [{"text": f"ì´ì „ ë‹µë³€: {last_user_ans.text}", "meta": {"category": "followup"}}]
                    logger.info(f"ğŸ“Œ Follow-up context prepared from last answer.")
                else:
                    logger.warning("âš ï¸ No previous answer found for followup question!")
                    contexts = [{"text": "ì´ì „ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "meta": {}}]
            else:
                # ì¼ë°˜ AI ì§ˆë¬¸: ì´ë ¥ì„œ RAG ê²€ìƒ‰
                from .rag_retrieval import retrieve_context
                query_tmpl = next_stage_data.get("query_template", "{target_role}")
                query = query_tmpl.format(target_role=interview.position)
                contexts = retrieve_context(query, resume_id=interview.resume_id, top_k=3)

            
            # ì§€ì›ì ì •ë³´ ë° ì§ë¬´ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ë³´ê°• (JSON header/metadata ìš°ì„ )
            resume = session.get(Resume, interview.resume_id)
            candidate_name = "ì§€ì›ì"
            target_role = interview.position # ê¸°ë³¸ê°’ (ì¸í„°ë·° ì„¸ì…˜ ì„¤ì •ê°’)
            
            if resume and resume.structured_data:
                s_data = resume.structured_data
                header_data = s_data.get("header", {})
                
                # 1. ì´ë¦„ ì¶”ì¶œ (header -> User í…Œì´ë¸” ìˆœ)
                candidate_name = header_data.get("name") or header_data.get("candidate_name")
                if not candidate_name and resume.candidate_id:
                    from db import User
                    user = session.get(User, resume.candidate_id)
                    if user: candidate_name = user.full_name or user.username
                
                # 2. ì§ë¬´ ì¶”ì¶œ (headerì— ìˆìœ¼ë©´ ìµœìš°ì„ )
                target_role = header_data.get("target_role") or target_role

            logger.info(f"Target Candidate Name: {candidate_name}, Role: {target_role}")
            
            # AI ì§ˆë¬¸ ìƒì„± ì‹¤í–‰
            content = exaone.generate_human_like_question(
                name=candidate_name,
                position=target_role,
                stage=stage_name,
                guide=next_stage_data.get("guide", "ì—­ëŸ‰ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."),
                context_list=contexts
            )
            
            # ì‹œë‚˜ë¦¬ì˜¤ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ DB Enumì— ë§ê²Œ ë§¤í•‘
            category_raw = next_stage_data.get("category", "technical")
            category_map = {
                "certification": "technical",
                "project": "technical",
                "narrative": "behavioral",
                "problem_solving": "situational"
            }
            db_category = category_map.get(category_raw, "technical")
            
>>>>>>> bcab0a98e56e154aae50f9fad3ffa7ac7d936acf
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
            save_generated_question(interview_id, content, db_category, stage_name, next_stage_data.get("guide", ""))
            return {"status": "success", "stage": stage_name, "question": content}
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
        finally:
            gc.collect()
