import sys
import os
import time
import gc 
import logging
import torch
from datetime import datetime
from celery import shared_task
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# AI-Worker ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì•„ sys.pathì— ì¶”ê°€
if "/app" not in sys.path:
    sys.path.insert(0, "/app")

logger = logging.getLogger("AI-Worker-QuestionGen")

# -----------------------------------------------------------
# [1. ëª¨ë¸ ë° ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

if os.path.exists(local_path):
    model_path = local_path
else:
    model_path = docker_path

# ğŸš¨ DB ì¡°íšŒë¥¼ ìœ„í•´ ì¶”ê°€
try:
    from db import engine
    from sqlalchemy import text as sql_text
except ImportError:
    engine = None


# ğŸš¨ ExaoneLLMì€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import (Celery ë¡œë”© ì‹œì  ë¬¸ì œ íšŒí”¼)

# -----------------------------------------------------------
# [2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]
# -----------------------------------------------------------
PROMPT_TEMPLATE = """[|system|]
ë„ˆëŠ” 15ë…„ ì°¨ ë² í…Œë‘ {position} ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤. 
ì§€ê¸ˆì€ **ë©´ì ‘ì´ í•œì°½ ì§„í–‰ ì¤‘ì¸ ìƒí™©**ì´ë‹¤. (ìê¸°ì†Œê°œëŠ” ì´ë¯¸ ëë‚¬ë‹¤.)
ì œê³µëœ [ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±°] ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ë‹¨ê³„({stage})ì— ë§ëŠ” **í•µì‹¬ì ì¸ ì§ˆë¬¸ 1ê°œ**ë§Œ ë˜ì ¸ë¼.

[ì‘ì„± ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­] 
1. **"ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤" ì ˆëŒ€ ê¸ˆì§€.**
2. **"(ì ì‹œ ì¹¨ë¬µ)", "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤"** ê°™ì€ ëŒ€ë³¸ìš© ì§€ë¬¸ì„ ì“°ì§€ ë§ˆë¼.
3. **[í”„ë¡œì íŠ¸], [íšŒì‚¬ ëª…]** ê°™ì€ ìë¦¬í‘œì‹œì(Placeholder)ë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ë§ê³ , ê·¼ê±° ë°ì´í„°ì— ìˆëŠ” ì‹¤ì œ ëª…ì¹­ì„ ì¨ë¼.
4. ì§ˆë¬¸ ì•ë’¤ì— ì‚¬ì¡±ì„ ë¶™ì´ì§€ ë§ê³  **ì§ˆë¬¸ë§Œ ë”± í•œ ë¬¸ì¥(ìµœëŒ€ ë‘ ë¬¸ì¥)**ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.

[ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ]
1. ì‹œì‘ì€ ë°˜ë“œì‹œ **"{name}ë‹˜,"** ìœ¼ë¡œ ë¶€ë¥´ë©° ì‹œì‘í•  ê²ƒ.
2. ì§ˆë¬¸ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ í•µì‹¬ë§Œ ëª…í™•íˆ ë¬¼ì–´ë³¼ ê²ƒ. ê¼¬ì•„ë‚´ì§€ ë§ê³  ì •ê³µë²•ìœ¼ë¡œ ë¬¼ì–´ë³¼ ê²ƒ.
3. ë§íˆ¬ëŠ” ì •ì¤‘í•˜ê²Œ(..í•˜ì…¨ë‚˜ìš”?, ..ë¶€íƒë“œë¦½ë‹ˆë‹¤.) ìœ ì§€í•  ê²ƒ.
[|endofturn|]
[|user|]
# í‰ê°€ ë‹¨ê³„: {stage}
# í‰ê°€ ì˜ë„: {guide}
# ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±° (RAG):
{context}

# ìš”ì²­:
ìœ„ì˜ ê·¼ê±° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ {name} ì§€ì›ìì—ê²Œ **êµ¬ì²´ì ì´ê³  ë‹¨ë„ì§ì…ì ì¸** ì§ˆë¬¸ì„ ë˜ì ¸ì¤˜.
[|endofturn|]
[|assistant|]
"""

# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# -----------------------------------------------------------
# -----------------------------------------------------------
# [3. ì§ˆë¬¸ ìƒì„± í•µì‹¬ í•¨ìˆ˜]
# [ê¸°ì¡´ ì¼ê´„ ìƒì„± íƒœìŠ¤í¬ ì‚­ì œë¨ - ì‹¤ì‹œê°„ ìƒì„± ëª¨ë“œë¡œ í†µí•©]

# -----------------------------------------------------------
# [5. Celery Task] - ì‹¤ì‹œê°„ 1ê°œì”© ìƒì„±í•˜ëŠ” íƒœìŠ¤í¬ (ìˆ˜ì • ì™„ë£Œ)
# -----------------------------------------------------------
@shared_task(name="tasks.question_generation.generate_next_question")
def generate_next_question_task(interview_id: int):
    logger.info(f"ğŸ”¥ [START] generate_next_question_task for Interview {interview_id}")
    
    from db import (
        engine, Session, select, save_generated_question,
        Interview, Transcript, Speaker, Question, Resume

    )
    from config.interview_scenario import get_stage_by_name, get_next_stage
    from utils.exaone_llm import get_exaone_llm
    
    with Session(engine) as session:
        interview = session.get(Interview, interview_id)
        if not interview: 
            logger.error(f"Interview {interview_id} not found.")
            return {"status": "error", "message": "Interview not found"}
            
        # ğŸš¨ [Race Condition ë°©ì§€] ì¤‘ë³µ ìƒì„± ì²´í¬
        # ë§ˆì§€ë§‰ AI ë°œí™” ì´í›„ì— ì‚¬ìš©ì ë‹µë³€ì´ ì•„ì§ ì—†ëŠ” ìƒíƒœì—ì„œ, 
        # ë§ˆì§€ë§‰ AI ë°œí™”ê°€ ë„ˆë¬´ ìµœê·¼(10ì´ˆ ì´ë‚´)ì´ë©´ ì¤‘ë³µ ìƒì„± ìš”ì²­ìœ¼ë¡œ ê°„ì£¼
        # ğŸš¨ [Race Condition Fix] Retry waiting for User transcript
        # ë°±ì—”ë“œì—ì„œ User Transcriptê°€ ì»¤ë°‹ë˜ì—ˆìœ¼ë‚˜, ì›Œì»¤ì—ì„œ ì•„ì§ ë³´ì´ì§€ ì•ŠëŠ” ê²½ìš° ëŒ€ë¹„ (Replication Lag or Transaction Isolation)
        last_transcript = None
        for attempt in range(5): # Max 2.5 seconds delay
            stmt_check = select(Transcript).where(
                Transcript.interview_id == interview_id
            ).order_by(Transcript.id.desc())
            last_transcript = session.exec(stmt_check).first()
            
            if not last_transcript:
                break
                
            if last_transcript.speaker == Speaker.USER:
                logger.info(f"âœ… Found User Answer (ID: {last_transcript.id}). Proceeding to generate next question.")
                break
            
            if last_transcript.speaker == Speaker.AI:
                 logger.warning(f"â³ Attempt {attempt+1}/5: Last speaker is AI (ID: {last_transcript.id}). Waiting for User answer to appear...")
                 time.sleep(0.5)
                 session.expire_all() # Clear session cache to get fresh data
        
        if last_transcript:
             logger.info(f"ğŸ§ [Check] Final Last Transcript ID: {last_transcript.id} | Speaker: {last_transcript.speaker} | Time: {last_transcript.timestamp} | Text: {last_transcript.text[:20]}...")
        
        # Retry í›„ì—ë„ ì—¬ì „íˆ AIê°€ ë§ˆì§€ë§‰ì´ê³ , ì‹œê°„ì´ ì§§ë‹¤ë©´ ìŠ¤í‚µ
        if last_transcript and last_transcript.speaker == Speaker.AI:
            diff = (datetime.utcnow() - last_transcript.timestamp).total_seconds()
            if diff < 5: 
                logger.warning(f"âš ï¸ [SKIP] AI just spoke {diff:.1f}s ago. Waiting for user response.")
                return {"status": "skipped", "reason": "ai_just_spoke"}


        # ğŸ” ë§ˆì§€ë§‰ ë‹¨ê³„ íƒì§€ ìµœì í™” (ìˆœì„œ ê¸°ë°˜ì´ ì•„ë‹Œ ID ê¸°ë°˜ ìµœì‹  ë°ì´í„° ì¡°íšŒ)
        stmt = select(Transcript).where(
            Transcript.interview_id == interview_id,
            Transcript.speaker == Speaker.AI
        ).order_by(Transcript.id.desc()) # IDê°€ ê°€ì¥ í° ê²ƒì´ ì ˆëŒ€ì ìœ¼ë¡œ ìµœì‹ 
        last_ai_transcript = session.exec(stmt).first()
        
        last_stage_name = None
        if last_ai_transcript:
            if last_ai_transcript.question_id:
                last_q = session.get(Question, last_ai_transcript.question_id)
                if last_q:
                    # 1ìˆœìœ„: DBì— ì €ì¥ëœ íƒ€ì… ì •ë³´ ì‚¬ìš©
                    last_stage_name = last_q.question_type
                    
                    # 2ìˆœìœ„ (Fallback): ì €ì¥ëœ íƒ€ì…ì´ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ë‚´ìš©ìœ¼ë¡œ ìœ ì¶”
                    if not last_stage_name:
                        content = last_q.content
                        if "ìê¸°ì†Œê°œ" in content: last_stage_name = "intro"
                        elif "ì§€ì› ë™ê¸°" in content or "ì§€ì›í•˜ê²Œ ëœ" in content: last_stage_name = "motivation"
                        elif "ê¸°ìˆ " in content or "ìŠ¤í‚¬" in content or "ë„êµ¬" in content: last_stage_name = "skill"
                        elif "í”„ë¡œì íŠ¸" in content or "ê²½í—˜" in content: last_stage_name = "experience"
                        elif "ì–´ë ¤ì›€" in content or "í•´ê²°" in content: last_stage_name = "problem_solving"
            
            # 3ìˆœìœ„: transcriptì˜ orderë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—­ì¶”ì  (scenarioì˜ orderì™€ ë§¤ì¹­)
            if not last_stage_name and last_ai_transcript.order is not None:
                from config.interview_scenario import INTERVIEW_STAGES
                # transcript.orderëŠ” 0ë¶€í„° ì‹œì‘, scenario orderëŠ” 1ë¶€í„° ì‹œì‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³´ì • í•„ìš”
                # ì—¬ê¸°ì„œëŠ” scenarioì˜ order í•„ë“œë¥¼ ê²€ìƒ‰
                for s in INTERVIEW_STAGES:
                    if s["order"] == last_ai_transcript.order + 1:
                        last_stage_name = s["stage"]
                        break

        # 4ìˆœìœ„: ë§¤í•‘ ë³´ì • (Legacy ë°ì´í„° ë“±)
        if last_stage_name == "technical": last_stage_name = "skill"
        
        if not last_stage_name:
            last_stage_name = "intro"

        logger.info(f"Detected Last Stage: {last_stage_name}")
        
        next_stage_data = get_next_stage(last_stage_name)
        if not next_stage_data:
            logger.info("Scenario Completed.")
            return {"status": "completed"}
            
        stage_name = next_stage_data["stage"]
        stage_type = next_stage_data.get("type", "ai")
        
        if stage_type == "template" or stage_type == "final":
            from utils.interview_helpers import get_candidate_info
            from db import Resume
            resume = session.get(Resume, interview.resume_id)
            c_info = get_candidate_info(resume.structured_data if resume else {})
            tmpl = next_stage_data.get("template", "{candidate_name}ë‹˜, ë‹¤ìŒ ì§ˆë¬¸ì…ë‹ˆë‹¤.")
            content = tmpl.format(candidate_name=c_info.get("candidate_name", "ì§€ì›ì"), target_role=interview.position)
            
            # QuestionCategory Enumì— 'general'ì´ ì—†ìœ¼ë¯€ë¡œ 'behavioral' ì‚¬ìš©
            save_generated_question(interview_id, content, "behavioral", stage_name, "", session=session)
            return {"status": "success", "stage": stage_name}

        # [LangChain LCEL] AI ìƒì„± íŒŒì´í”„ë¼ì¸
        try:
            # 1. ëª¨ë¸ ë° íŒŒì„œ ì¤€ë¹„
            llm = get_exaone_llm()
            output_parser = StrOutputParser()
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ë° í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            from .rag_retrieval import get_retriever
            
            # [ìˆ˜ì •] ê¼¬ë¦¬ì§ˆë¬¸ì´ë“  ì¼ë°˜ ì§ˆë¬¸ì´ë“  ê¸°ë³¸ì ìœ¼ë¡œ ì´ë ¥ì„œ(RAG) ë² ì´ìŠ¤ë¼ì¸ì„ ê°€ì ¸ì˜´
            query_tmpl = next_stage_data.get("query_template", "{target_role}")
            if stage_type == "followup" and not next_stage_data.get("query_template"):
                parent_stage_name = next_stage_data.get("parent")
                parent_data = get_stage_by_name(parent_stage_name) if parent_stage_name else None
                query = parent_data.get("query_template", "{target_role}").format(target_role=interview.position) if parent_data else interview.position
            else:
                query = query_tmpl.format(target_role=interview.position)

            # Retriever ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            retriever = get_retriever(resume_id=interview.resume_id, top_k=2)
            retrieved_docs = retriever.invoke(query)
            rag_context = "\n".join([f"- {doc.page_content}" for doc in retrieved_docs]) if retrieved_docs else "ì´ë ¥ì„œ ê·¼ê±° ë¶€ì¡±"

            if stage_type == "followup":
                # ê¼¬ë¦¬ì§ˆë¬¸: RAG ì»¨í…ìŠ¤íŠ¸ + ì´ì „ ë‹µë³€ ê²°í•©
                user_stmt = select(Transcript).where(
                    Transcript.interview_id == interview_id,
                    Transcript.speaker == Speaker.USER
                ).order_by(Transcript.id.desc())
                last_user_ans = session.exec(user_stmt).first()
                user_ans_text = last_user_ans.text if last_user_ans else "ì´ì „ ë‹µë³€ ì—†ìŒ"
                
                context_text = f"[ì§€ì›ì ì´ë ¥ì„œ ê´€ë ¨ ì •ë³´]\n{rag_context}\n\n[ì§€ì›ìì˜ ì´ì „ ë‹µë³€]\n{user_ans_text}"
            else:
                # ì¼ë°˜ AI ì§ˆë¬¸: RAG ì»¨í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ í™œìš©
                context_text = rag_context

            # 3. ì§€ì›ì ì •ë³´ ì •ì œ
            resume = session.get(Resume, interview.resume_id)
            candidate_name = "ì§€ì›ì"
            target_role = interview.position
            if resume and resume.structured_data:
                header = resume.structured_data.get("header", {})
                candidate_name = header.get("name") or header.get("candidate_name") or candidate_name
                target_role = header.get("target_role") or target_role

            # 4. LCEL ì²´ì¸ ì •ì˜ ë° ì‹¤í–‰ (Prompt | LLM | Parser)
            prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
            # LCEL ì²´ì¸ ì •ì˜ (Prompt | LLM | Parser)
            chain = prompt | llm | output_parser
            
            logger.info(f"ğŸ”— Executing LCEL Chain for stage: {stage_name}")
            content = chain.invoke({
                "position": target_role,
                "name": candidate_name,
                "stage": stage_name,
                "guide": next_stage_data.get("guide", "ì—­ëŸ‰ì„ í™•ì¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."),
                "context": context_text
            })
            
            if not content:
                content = f"{candidate_name}ë‹˜, ì¤€ë¹„í•˜ì‹  ë‚´ìš©ì„ í† ëŒ€ë¡œ í•´ë‹¹ ì—­ëŸ‰ì— ëŒ€í•´ ë” ë§ì”€í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?"
            
            # 5. ê²°ê³¼ ì €ì¥
            category_raw = next_stage_data.get("category", "technical")
            category_map = {"certification": "technical", "project": "technical", "narrative": "behavioral", "problem_solving": "situational"}
            db_category = category_map.get(category_raw, "technical")
            
            logger.info(f"ğŸ’¾ Saving generated question to DB for Interview {interview_id} (Stage: {stage_name})")
            save_generated_question(interview_id, content, db_category, stage_name, next_stage_data.get("guide", ""), session=session)
            return {"status": "success", "stage": stage_name, "question": content}
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {"status": "error", "error": str(e)}
        finally:
            gc.collect()