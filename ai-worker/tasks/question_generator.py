import os
import logging
from celery import shared_task
from typing import Optional, List

# DB í—¬í¼ í•¨ìˆ˜ import
from db import (
    get_best_questions_by_position,
    increment_question_usage,
    engine
)

# EXAONE LLM import
from utils.exaone_llm import get_exaone_llm

logger = logging.getLogger("AI-Worker-QuestionGen")

class QuestionGenerator:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ìƒì„±ê¸° (EXAONE-3.5-7.8B-Instruct ì‚¬ìš©)
    ì „ëµ: DB ì¬í™œìš© (40%) + Few-Shot LLM ìƒì„± (60%)
    """
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
            
        logger.info("Initializing Question Generator with EXAONE model")
        self.llm = get_exaone_llm()
        self._initialized = True

    def generate_questions(self, position: str, interview_id: Optional[int] = None, count: int = 5, reuse_ratio: float = 0.4):
        from tools import ResumeTool, CompanyTool

        # 1. ì´ë ¥ì„œ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        resume_summary = ""
        if interview_id:
            resume_info = ResumeTool.get_resume_by_interview(interview_id)
            if resume_info.get("has_resume"):
                context_parts.append(ResumeTool.format_for_llm(resume_info))
                logger.info(f"ì´ë ¥ì„œ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {resume_info.get('summary', '')[:50]}...")
            
            # íšŒì‚¬ ì •ë³´
            company_info = CompanyTool.get_company_by_interview(interview_id)
            if company_info.get("has_company"):
                context_parts.append(CompanyTool.format_for_llm(company_info))
                logger.info(f"íšŒì‚¬ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {company_info.get('name', '')}")
        
        context = "\n\n".join(context_parts) if context_parts else ""
        
        # 2. DBì—ì„œ ê¸°ì¡´ ì§ˆë¬¸ ì¬í™œìš© (Reuse)
        if reuse_count > 0:
            reused = self._reuse_questions_from_db(position, reuse_count)
            questions.extend(reused)
            logger.info(f"âœ… DBì—ì„œ {len(reused)}ê°œ ì§ˆë¬¸ ì¬í™œìš©")
        
        # 3. EXAONE LLMìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ ìƒì„± (Create with Context)
        if generate_count > 0:
            generated = self.llm.generate_questions(
                position=position,
                context=context,
                examples=questions,  # Few-shot ì˜ˆì‹œë¡œ ì¬í™œìš©ëœ ì§ˆë¬¸ ì‚¬ìš©
                count=generate_count
            )
            questions.extend(generated)
            logger.info(f"âœ… EXAONEìœ¼ë¡œ {len(generated)}ê°œ ì§ˆë¬¸ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)")
        
        return questions[:count]  # ì •í™•íˆ countê°œë§Œ ë°˜í™˜
    
    def _reuse_questions_from_db(self, position: str, count: int):
        try:
            db_questions = get_best_questions_by_position(position, limit=count)

            # ì¬í™œìš© ì‹œ ì‚¬ìš©ëŸ‰ ì¦ê°€
            for q in db_questions:
                try:
                    increment_question_usage(q.id)
                except:
                    pass
            return [q.content for q in db_questions]
        except Exception as e:
            logger.warning(f"DB ì§ˆë¬¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def generate_deep_dive_question(self, history: str, current_answer: str):
        """ë™ì  ê¼¬ë¦¬ì§ˆë¬¸(Deep-Dive) ìƒì„± í”„ë¡¬í”„íŠ¸ ê³ ë„í™” (BS Detection ê°•í™”)"""
        if not self.llm: return "ì¶”ê°€ ì§ˆë¬¸ì„ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
# Role
ë‹¹ì‹ ì€ ì§€ì›ìì˜ ë‹µë³€ì—ì„œ í—ˆì„¸(BS)ë¥¼ ì°¾ì•„ë‚´ê³  ê¸°ìˆ ì  ë°‘ë°”ë‹¥ì„ í™•ì¸í•˜ëŠ” 20ë…„ ì°¨ ë² í…Œë‘ í…Œí¬ ë¦¬ë“œì…ë‹ˆë‹¤.

# Mission (Strict)
1. **ë¶„ì„**: ë‹µë³€ì„ ìš”ì•½í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ëŒ€ì‹  "êµ¬ì²´ì  ìˆ˜ì¹˜ ë¶€ì¬", "ì›ë¡ ì ì¸ ê°œë… ë‚˜ì—´", "ì§ì ‘ êµ¬í˜„ ì—¬ë¶€ ë¶ˆë¶„ëª…" ë“± **ê¸°ìˆ ì  í—ˆì **ì„ ë°˜ë“œì‹œ í•œ ì¤„ë¡œ ì§€ì í•˜ì‹­ì‹œì˜¤.
2. **ì§ˆë¬¸**: ë¶„ì„í•œ í—ˆì ì„ íŒŒê³ ë“¤ì–´, ì§€ì›ìê°€ ì‹¤ì œ ê²½í—˜í–ˆëŠ”ì§€ ì¦ëª…í•˜ê²Œ ë§Œë“œëŠ” ë‚ ì¹´ë¡œìš´ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë˜ì§€ì‹­ì‹œì˜¤.

# Persona & Guidelines
- ë§íˆ¬ëŠ” ë°˜ë“œì‹œ ëƒ‰ì² í•œ ê²©ì‹ì²´(~í•˜ì‹­ì‹œì˜¤ì²´)ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
- ì§ˆë¬¸ ì‹œì‘ì€ ë°˜ë“œì‹œ "ì•ì„œ ë§ì”€í•˜ì‹  [íŠ¹ì • í‚¤ì›Œë“œ] ë¶€ë¶„ê³¼ ê´€ë ¨í•˜ì—¬..."ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
- ë¶ˆí•„ìš”í•œ ì„œë¡ /ë¯¸ì‚¬ì—¬êµ¬ëŠ” ì ˆëŒ€ ë°°ì œí•˜ì‹­ì‹œì˜¤.

# Example
ì§€ì›ì ë‹µë³€: "ì„œë²„ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì¸ë±ìŠ¤ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì—¬ ì†ë„ë¥¼ ë§ì´ ê°œì„ í–ˆìŠµë‹ˆë‹¤."
[ë¶„ì„]: ì–´ë–¤ ì¸ë±ìŠ¤ êµ¬ì¡°ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ì™€ êµ¬ì²´ì ì¸ ì„±ëŠ¥ ê°œì„  ì§€í‘œ(TPS, Latency)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.
[ì§ˆë¬¸]: ì•ì„œ ë§ì”€í•˜ì‹  ì¸ë±ìŠ¤ ìµœì í™” ë¶€ë¶„ê³¼ ê´€ë ¨í•˜ì—¬, ë‹¹ì‹œ ì‚¬ìš©í•œ ì¸ë±ìŠ¤ êµ¬ì¡°ì™€ ì¿¼ë¦¬ ì‘ë‹µ ì†ë„ë¥¼ ëª‡ msì—ì„œ ëª‡ msë¡œ ê°œì„ í•˜ì…¨ëŠ”ì§€ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ë§ì”€í•´ ì£¼ì‹­ì‹œì˜¤.<|eot_id|><|start_header_id|>user<|end_header_id|>
# Input Data
- [History]: {history}
- [Answer]: {current_answer}

[ë¶„ì„]:
[ì§ˆë¬¸]:<|eot_id|><|start_header_id|>assistant<|end_header_id|>
[ë¶„ì„]: """

        try:
            response = self.llm.invoke(prompt)
            # ê¹”ë”í•˜ê²Œ [ë¶„ì„]ë¶€í„° ì‹œì‘í•˜ë„ë¡ ë³´ì •
            full_response = "[ë¶„ì„]: " + response.strip()

            # ì¤„ë°”ê¿ˆ ì •ì œ (ìµœëŒ€í•œ ìƒìœ„ 2ê°œ ë¼ì¸ë§Œ ìœ ì§€)
            lines = [l.strip() for l in full_response.split('\n') if l.strip()]
            valid_lines = [l for l in lines if l.startswith('[ë¶„ì„]:') or l.startswith('[ì§ˆë¬¸]:')]

            if len(valid_lines) >= 2:
                return "\n".join(valid_lines[:2])

            return "\n".join(lines[:2])

        except Exception as e:
            logger.error(f"Deep-Dive ìƒì„± ì‹¤íŒ¨: {e}")
            return "[ë¶„ì„]: ë‹µë³€ ë‚´ìš©ì´ ì¶”ìƒì ì´ë©° ê¸°ìˆ ì  ê·¼ê±°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.\n[ì§ˆë¬¸]: ì•ì„œ ë§ì”€í•˜ì‹  ë‚´ìš© ì¤‘ ë³¸ì¸ì´ ì§ì ‘ ì„¤ê³„í•˜ê³  êµ¬í˜„í•œ êµ¬ì²´ì ì¸ ë¡œì§ì— ëŒ€í•´ ì„¤ëª…í•´ ì£¼ì‹­ì‹œì˜¤."

@shared_task(name="tasks.question_generator.generate_questions")
def generate_questions_task(position: str, interview_id: int = None, count: int = 5):
    try:
        generator = QuestionGenerator()
        return generator.generate_questions(position, interview_id, count)
    except Exception as e:
        logger.error(f"Task Error: {e}")
        return []

# Eager Initialization: Worker ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
try:
    logger.info("ğŸ”¥ Pre-loading Question Generator with EXAONE...")
    _warmup_generator = QuestionGenerator()
    logger.info("âœ… Question Generator ready for requests")
except Exception as e:
    logger.warning(f"âš ï¸ Failed to pre-load model (will load on first request): {e}")
