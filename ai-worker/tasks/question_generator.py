import os
import logging
import re
from celery import shared_task
from typing import Optional, List, Dict
import re

# DB í—¬í¼ í•¨ìˆ˜ import
from db import engine
from sqlmodel import Session, select

# EXAONE LLM import
from utils.exaone_llm import get_exaone_llm

logger = logging.getLogger("AI-Worker-QuestionGen")

class QuestionGenerator:
    """
    RAG ê¸°ë°˜ ì‹¬ì¸µ ë©´ì ‘ ì§ˆë¬¸ ìƒì„±ê¸° (EXAONE-3.5-7.8B-Instruct ì‚¬ìš©)
    ë©´ì ‘ ë‹¨ê³„ë³„ ì‹œë‚˜ë¦¬ì˜¤(Plan)ì— ë”°ë¼ ì´ë ¥ì„œ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ì§ˆë¬¸ ìƒì„±

    Attributes:
        _instance (QuestionGenerator): ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
        _initialized (bool): ì´ˆê¸°í™” ì—¬ë¶€
        llm (ExaoneLLM): EXAONE LLM ì¸ìŠ¤í„´ìŠ¤
    """
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        logger.info("Initializing RAG Question Generator")
        self.llm = get_exaone_llm()
        self._initialized = True

    def generate_questions(self, position: str, interview_id: Optional[int] = None, count: int = 5) -> List[str]:
        """
        ë©´ì ‘ ë‹¨ê³„ë³„ RAG ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±

        Args:
            position: ì§€ì› ì§ë¬´
            interview_id: ë©´ì ‘ ID
            count: ìƒì„±í•  ì´ ì§ˆë¬¸ ìˆ˜
        """
        questions = []

        # 1. ì´ë ¥ì„œ ID ë° ì§€ì›ì ì •ë³´ ì¡°íšŒ
        resume_id = None
        candidate_name = "ì§€ì›ì"

        if interview_id:
            try:
                with Session(engine) as session:
                    # Circular import ë°©ì§€ë¥¼ ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ import
                    from db import Interview, Resume
                    interview = session.get(Interview, interview_id)
                    if interview and interview.resume_id:
                        resume_id = interview.resume_id
                        resume = session.get(Resume, resume_id)
                        # ì´ë ¥ì„œì—ì„œ ì§€ì›ì ì´ë¦„ ì¶”ì¶œ ì‹œë„ (í—¤ë” ì •ë³´ ë“±)
                        if resume and resume.structured_data:
                            candidate_name = resume.structured_data.get("target_company", {}).get("name", "ì§€ì›ì")
                            # structured_data êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                            if isinstance(resume.structured_data.get("header"), dict):
                                candidate_name = resume.structured_data["header"].get("name", "ì§€ì›ì")
            except Exception as e:
                logger.warning(f"ì´ë ¥ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # 2. RAG ë¶ˆê°€ëŠ¥ ì‹œ ê¸°ë³¸ ìƒì„± ë¡œì§ìœ¼ë¡œ Fallback
        if not resume_id:
            logger.info("Resume ID not found. Using generic generation.")
            return self.llm.generate_questions(position=position, count=count)

        # 3. ë©´ì ‘ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜ (Step 8 ê¸°ë°˜)
        interview_plan = [
            {
                "stage": "1. ì§ë¬´ ì§€ì‹ í‰ê°€",
                "search_query": f"{position} í•µì‹¬ ê¸°ìˆ  ìŠ¤í‚¬ ë„êµ¬ ì›ë¦¬",
                "filter_category": "metric", # ìê²©ì¦/ìŠ¤í‚¬
                "guide": "ì§€ì›ìê°€ ì‚¬ìš©í•œ ê¸°ìˆ (Tool, Language)ì˜ êµ¬ì²´ì ì¸ ì„¤ì •ë²•ì´ë‚˜, ê¸°ìˆ ì  ì›ë¦¬(Deep Dive)ë¥¼ ë¬¼ì–´ë³¼ ê²ƒ."
            },
            {
                "stage": "2. ì§ë¬´ ê²½í—˜ í‰ê°€",
                "search_query": "í”„ë¡œì íŠ¸ ì„±ê³¼ ë‹¬ì„± ë¬¸ì œí•´ê²°",
                "filter_category": "project",
                "guide": "í”„ë¡œì íŠ¸ì—ì„œ ë‹¬ì„±í•œ ìˆ˜ì¹˜ì  ì„±ê³¼(%)ì˜ ê²°ì •ì  ìš”ì¸ì´ ë¬´ì—‡ì¸ì§€, êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë°ì´í„°ë¥¼ ë‹¤ë¤˜ëŠ”ì§€ ë¬¼ì–´ë³¼ ê²ƒ."
            },
            {
                "stage": "3. ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ í‰ê°€",
                "search_query": "ê¸°ìˆ ì  ë‚œê´€ ê·¹ë³µ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…",
                "filter_category": "project",
                "guide": "ì§ë©´í•œ í•œê³„ì ì´ë‚˜ ë¬¸ì œ ìƒí™©ì„ ì–´ë–»ê²Œ ì •ì˜í–ˆê³ , ì–´ë–¤ ë…¼ë¦¬ì  ì‚¬ê³  ê³¼ì •ì„ í†µí•´ í•´ê²°ì±…ì„ ë„ì¶œí–ˆëŠ”ì§€ ë¬¼ì–´ë³¼ ê²ƒ."
            },
            {
                "stage": "4. ì˜ì‚¬ì†Œí†µ ë° í˜‘ì—… í‰ê°€",
                "search_query": "í˜‘ì—… ê°ˆë“± í•´ê²° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
                "filter_category": "narrative",
                "guide": "íŒ€ì›ê³¼ì˜ ì˜ê²¬ ëŒ€ë¦½ ìƒí™©ì—ì„œ ë³¸ì¸ì˜ ì£¼ì¥ì„ ê´€ì² ì‹œí‚¤ê¸° ìœ„í•´ ì–´ë–¤ ê°ê´€ì  ê·¼ê±°ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ ëŒ€í™” ê³¼ì •ì„ ë¬¼ì–´ë³¼ ê²ƒ."
            },
            {
                "stage": "5. ì§ë¬´ ì í•©ì„± ë° ì„±ì¥ ê°€ëŠ¥ì„±",
                "search_query": f"{position} íŠ¸ë Œë“œ ì„±ì¥ ê³„íš",
                "filter_category": "narrative",
                "guide": "ì§ë¬´ì™€ ê´€ë ¨ëœ ìµœì‹  íŠ¸ë Œë“œë¥¼ ì–´ë–»ê²Œ í•™ìŠµí•˜ê³  ìˆìœ¼ë©°, ì´ë¥¼ ì‹¤ë¬´ì— ì–´ë–»ê²Œ ì ìš©í•  ê²ƒì¸ì§€ ë¬¼ì–´ë³¼ ê²ƒ."
            }
        ]

        # 4. ì‹œë‚˜ë¦¬ì˜¤ ë°˜ë³µí•˜ë©° ì§ˆë¬¸ ìƒì„±
        # countê°€ planë³´ë‹¤ í¬ë©´ planì„ ë°˜ë³µ, ì‘ìœ¼ë©´ ì•ì—ì„œë¶€í„° ìë¦„
        generated_count = 0
        plan_idx = 0

        while generated_count < count:
            step = interview_plan[plan_idx % len(interview_plan)]
            plan_idx += 1

            # RAG ê²€ìƒ‰
            contexts = self._retrieve_context(
                resume_id=resume_id,
                query=step['search_query'],
                filter_category=step['filter_category'],
                top_k=2
            )

            # ì§ˆë¬¸ ìƒì„±
            if contexts:
                q = self.llm.generate_human_like_question(
                    name=candidate_name,
                    stage=step['stage'],
                    guide=step['guide'] + f" (ì§€ì› ì§ë¬´: {position})",
                    context_list=contexts
                )
                if q not in questions: # ì¤‘ë³µ ë°©ì§€
                    questions.append(q)
                    generated_count += 1
            else:
                # ì»¨í…ìŠ¤íŠ¸ ì—†ìœ¼ë©´ Fallback ì§ˆë¬¸ í•˜ë‚˜ ì¶”ê°€
                logger.info(f"ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ: {step['stage']}")
                # ê·¸ëƒ¥ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°€ê±°ë‚˜ ê¸°ë³¸ ì§ˆë¬¸ ì¶”ê°€
                # ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰ (while loop)
                # ë¬´í•œ ë£¨í”„ ë°©ì§€: ì‹œë„ë¥¼ ë„ˆë¬´ ë§ì´ í•˜ë©´ ì¤‘ë‹¨
                if plan_idx > count * 3:
                     break

        # ë¶€ì¡±ë¶„ ì±„ìš°ê¸°
        if len(questions) < count:
            logger.warning(f"ìƒì„±ëœ ì§ˆë¬¸ ìˆ˜ ë¶€ì¡± ({len(questions)}/{count}). Fallbackìœ¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.")
            fallback_candidates = self.llm._get_fallback_questions(position, count + 5) # ì¶©ë¶„íˆ ê°€ì ¸ì˜¤ê¸°

            for fq in fallback_candidates:
                if len(questions) >= count:
                    break
                if fq not in questions:
                    questions.append(fq)

        return questions[:count]

    def _retrieve_context(self, resume_id: int, query: str, filter_category: str, top_k: int = 2) -> List[Dict]:
        """ë‚´ë¶€ RAG ê²€ìƒ‰ ë¡œì§"""
        try:
            from db import ResumeSectionEmbedding, ResumeSectionType
            from utils.vector_utils import get_embedding_generator

            # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            category_map = {
                "metric": [ResumeSectionType.CERTIFICATION, ResumeSectionType.SKILL, ResumeSectionType.LANGUAGE, ResumeSectionType.EDUCATION],
                "project": [ResumeSectionType.PROJECT, ResumeSectionType.EXPERIENCE],
                "narrative": [ResumeSectionType.SELF_INTRODUCTION]
            }
            target_types = category_map.get(filter_category)

            # ì„ë² ë”©
            generator = get_embedding_generator()
            query_vector = generator.encode_query(query)

            # ê²€ìƒ‰
            with Session(engine) as session:
                dist_expr = ResumeSectionEmbedding.embedding.cosine_distance(query_vector)
                stmt = select(ResumeSectionEmbedding, dist_expr.label("distance")).where(
                    ResumeSectionEmbedding.resume_id == resume_id,
                    ResumeSectionEmbedding.embedding.isnot(None)
                )
                if target_types:
                    stmt = stmt.where(ResumeSectionEmbedding.section_type.in_(target_types))

                stmt = stmt.order_by(dist_expr).limit(top_k)
                rows = session.exec(stmt).all()

                return [{"text": row[0].content, "similarity": 1 - (row[1]/2)} for row in rows]

        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def generate_deep_dive_question(self, history: str, current_answer: str):
        """ë™ì  ê¼¬ë¦¬ì§ˆë¬¸(Deep-Dive) ìƒì„± - ë©´ì ‘ê´€ í†¤ìœ¼ë¡œ ìƒì„±"""
        if not self.llm: return "ì¶”ê°€ ì§ˆë¬¸ì„ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        prompt = f"""ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ë“£ê³ , ë” êµ¬ì²´ì ì¸ í™•ì¸ì´ í•„ìš”í•œ ë¶€ë¶„ì— ëŒ€í•´ ì •ì¤‘í•˜ê³  ë‚ ì¹´ë¡œìš´ ê¼¬ë¦¬ì§ˆë¬¸ì„ í•˜ë‚˜ë§Œ ë˜ì ¸ì£¼ì„¸ìš”.

ì´ì „ ì§ˆë¬¸: {history}
ì§€ì›ì ë‹µë³€: {current_answer}

ì§€ì :
- ë§íˆ¬ëŠ” ì •ì¤‘í•œ ê²©ì‹ì²´(~ìŠµë‹ˆê¹Œ?, ~í•˜ì‹­ì‹œì˜¤)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ë¶ˆí•„ìš”í•œ ë¶„ì„ì´ë‚˜ ë©”íƒ€ ì •ë³´ ì—†ì´ 'ì§ˆë¬¸ ë¬¸ì¥'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ì§ˆë¬¸:"""

        try:
            response = self.llm.invoke(prompt)
            # ì§ˆë¬¸ë§Œ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ
            lines = response.strip().split('\n')
            for line in lines:
                line = line.strip()
                if line and not line.startswith('[') and not line.startswith('###'):
                    # "ì§ˆë¬¸:" ì´ë‚˜ ìˆ«ì ë“±ì´ í¬í•¨ëœ ê²½ìš° ì œê±°
                    clean_q = re.sub(r'^(ì§ˆë¬¸|ì§ˆë¬¸ ë‚´ìš©|Q|A|1|2|3|4|5)[\.\s:]+', '', line).strip()
                    # ëì— ë¶™ëŠ” ë…¸ì´ì¦ˆ(ë‚ ì§œ, ì ìˆ˜ ë“±) ì œê±°
                    clean_q = re.sub(r'\s*\|.*\|?\s*$', '', clean_q).strip()
                    if len(clean_q) > 10:
                        return clean_q

            return response.strip()

        except Exception as e:
            logger.error(f"Deep-Dive ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë°©ê¸ˆ ë§ì”€í•˜ì‹  ë¶€ë¶„ê³¼ ê´€ë ¨í•˜ì—¬, ì‹¤ë¬´ì—ì„œ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì…¨ëŠ”ì§€ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"

    def generate_answer_analysis(self, history: str, current_answer: str):
        """[Task 3] ë‹µë³€ ì •ë°€ ë¶„ì„ - ì„¹ì…˜ ê¸°ë°˜ íŒŒì‹± (ì¤„ë°”ê¿ˆ í—ˆìš©)"""
        if not self.llm: return "ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        prompt = f"""ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ ê¸°ìˆ ì  êµ¬ì²´ì„±, ìˆ˜ì¹˜ ë° ì„±ê³¼, ë…¼ë¦¬ì  ì •í•©ì„±, ì‹¤ë¬´ ì ìš©ì„±, ì¢…í•© í‰ê°€ 5ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì‹­ì‹œì˜¤.
ê° í•­ëª©ì— ëŒ€í•´ ì ìˆ˜([ì ìˆ˜/5])ì™€ êµ¬ì²´ì ì¸ ì´ìœ ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

ì§ˆë¬¸: {history}
ë‹µë³€: {current_answer}

[í‰ê°€ ì‹œì‘]
- ê¸°ìˆ ì  êµ¬ì²´ì„±:"""

        try:
            response = self.llm.invoke(prompt).strip()
            # ë¡œê·¸ë¡œ ì›ë³¸ í™•ì¸
            logger.info(f"Raw Analysis Response:\n{response}")

            full_response = "- ê¸°ìˆ ì  êµ¬ì²´ì„±: " + response

            import re
            categories = ["ê¸°ìˆ ì  êµ¬ì²´ì„±", "ìˆ˜ì¹˜ ë° ì„±ê³¼", "ë…¼ë¦¬ì  ì •í•©ì„±", "ì‹¤ë¬´ ì ìš©ì„±", "ì¢…í•© í‰ê°€"]

            # 1. ê° ì¹´í…Œê³ ë¦¬ì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
            # (?:-|\d+\.|#)? \s* ì¹´í…Œê³ ë¦¬ëª… \s* :?
            # ìœ„ íŒ¨í„´ìœ¼ë¡œ ê° ì¹´í…Œê³ ë¦¬ê°€ í…ìŠ¤íŠ¸ ë‚´ ì–´ë””ì— ìˆëŠ”ì§€ ì¸ë±ì‹±
            indices = []
            for cat in categories:
                # ìœ ì—°í•œ ë§¤ì¹­: ì•ë¶€ë¶„ ê¸°í˜¸(-, 1., # ë“±) í—ˆìš©, ì½œë¡  í—ˆìš©
                pattern = r'(?:-|\d+\.|#)?\s*' + re.escape(cat) + r'\s*:?'
                match = re.search(pattern, full_response)
                if match:
                    indices.append((match.start(), cat))

            # ìœ„ì¹˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            indices.sort()

            results = []

            for i, (start_idx, cat) in enumerate(indices):
                # í˜„ì¬ ì¹´í…Œê³ ë¦¬ë¶€í„° ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì‹œì‘ ì „ê¹Œì§€ê°€ ë‚´ìš©
                end_idx = indices[i+1][0] if i + 1 < len(indices) else len(full_response)

                content_chunk = full_response[start_idx:end_idx]

                # ì¹´í…Œê³ ë¦¬ í—¤ë” ì œê±° (ì˜ˆ: "- ê¸°ìˆ ì  êµ¬ì²´ì„±:")
                # ì²« ë²ˆì§¸ ì½œë¡ (:) ì´í›„ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê±°ë‚˜, í—¤ë” ê¸¸ì´ë§Œí¼ ìë¦„
                split_content = content_chunk.split(':', 1)
                if len(split_content) > 1:
                    content_body = split_content[1]
                else:
                    # ì½œë¡ ì´ ì—†ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ ì´ë¦„ ê¸¸ì´ë§Œí¼ ë„˜ê¹€ (ëŒ€ì¶© ì²˜ë¦¬)
                    content_body = content_chunk[len(cat):]

                # ì ìˆ˜ ì°¾ê¸°
                score_pattern = r'(\d+(?:\.\d+)?)\s*/\s*5|(\d+(?:\.\d+)?)\s*/\s*10|(\d+(?:\.\d+)?)\s*ì '
                score_match = re.search(score_pattern, content_body)

                score_txt = "0"
                if score_match:
                    val = 0.0
                    if score_match.group(1): val = float(score_match.group(1))
                    elif score_match.group(2): val = float(score_match.group(2)) / 2
                    elif score_match.group(3): val = float(score_match.group(3))

                    if val > 5.0: val = 5.0 # ë³´ì •
                    score_txt = str(int(val)) if val.is_integer() else str(val)

                    # ì ìˆ˜ ë¶€ë¶„ ì œê±° (ë‚´ìš© ì •ì œìš©)
                    content_body = content_body.replace(score_match.group(0), '')

                # ë‚´ìš© ì •ì œ
                # ëŒ€ê´„í˜¸ ì”ì—¬ë¬¼ [ ] ì œê±°
                content_body = re.sub(r'\[\s*\]', '', content_body)
                content_body = re.sub(r'\[\s*/\s*5\s*\]', '', content_body)

                # ë¶ˆí•„ìš”í•œ ê³µë°±/ì¤„ë°”ê¿ˆ ì••ì¶•
                reason = " ".join(content_body.split()).strip()
                # ì•ë¶€ë¶„ ê¸°í˜¸ ì œê±° (. , - )
                reason = re.sub(r'^[\.\,\-\s]+', '', reason)

                if reason:
                    results.append(f"- {cat}: {score_txt}/5ì . {reason}")

            if len(results) >= 3:
                return "\n".join(results)

            return full_response[:300]

        except Exception as e:
            logger.error(f"ë‹µë³€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@shared_task(name="tasks.question_generator.generate_questions")
def generate_questions_task(position: str, interview_id: int = None, count: int = 1):
    """
    ì§ˆë¬¸ ìƒì„± Task (TTS í¬í•¨)

    Args:
        position (str): ì§€ì› ì§ë¬´
        interview_id (int, optional): ë©´ì ‘ ID. Defaults to None.
        count (int, optional): ìƒì„±í•  ì§ˆë¬¸ ìˆ˜. Defaults to 5.

    Returns:
        List[Dict[str, str]]: ìƒì„±ëœ ì§ˆë¬¸ ë° ì˜¤ë””ì˜¤ URL ë¦¬ìŠ¤íŠ¸
        Example: [{"text": "ì§ˆë¬¸ë‚´ìš©", "audio_url": "/static/audio/..."}]

    Raises:
        Exception: ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨

    ìƒì„±ì: ejm
    ìƒì„±ì¼ì: 2026-02-04
    """
    import uuid
    from .tts import tts_engine, load_tts_engine

    try:
        generator = QuestionGenerator()
        questions = generator.generate_questions(position, interview_id, count)

        # TTS Integration
        results = []

        # Ensure TTS engine is ready
        if tts_engine is None or tts_engine.tts is None:
            load_tts_engine()

        # Save directory inside container
        save_dir = "/app/uploads/audio"
        os.makedirs(save_dir, exist_ok=True)

        for q in questions:
            audio_url = None
            if tts_engine and tts_engine.tts:
                try:
                    filename = f"q_{uuid.uuid4().hex}.wav"
                    filepath = os.path.join(save_dir, filename)

                    if tts_engine.synthesize(q, filepath):
                        # URL path for backend (mounted as /static)
                        audio_url = f"/static/audio/{filename}"
                except Exception as ex:
                    logger.error(f"TTS failing for question '{q[:20]}...': {ex}")

            results.append({"text": q, "audio_url": audio_url})

        return results

    except Exception as e:
        logger.error(f"Task Error: {e}")
        return []

# Eager Initialization: Worker ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
try:
    logger.info("ğŸ”¥ Pre-loading Question Generator with EXAONE...")
    _warmup_generator = QuestionGenerator()
    logger.info("âœ… Question Generator ready for requests")
except Exception as e:
    logger.warning(f"âš ï¸ Failed to pre-load model (will load on first request): {e}")
