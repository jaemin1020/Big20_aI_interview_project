import os
import logging
from celery import shared_task
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch

# DB í—¬í¼ í•¨ìˆ˜ import
from db import (
    get_best_questions_by_position,  # ì§ë¬´ë³„ ìš°ìˆ˜ ì§ˆë¬¸ ì¡°íšŒ
    increment_question_usage,
    engine
)
from sqlmodel import Session, select

logger = logging.getLogger("AI-Worker-QuestionGen")

# ëª¨ë¸ ì„¤ì •
MODEL_ID = "meta-llama/Llama-3.2-3B-Instruct"

class QuestionGenerator:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ìƒì„±ê¸°
    ì „ëµ: DB ì¬í™œìš© (40%) + Few-Shot LLM ìƒì„± (60%)
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        logger.info(f"Loading Question Gen Model: {MODEL_ID}")
        token = os.getenv("HUGGINGFACE_HUB_TOKEN")
        
        # 4-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ìµœì í™”)
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4"
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=token)
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            quantization_config=quantization_config,
            device_map="auto",
            token=token
        )
        
        pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=100,
            temperature=0.8, # ì°½ì˜ì„± í™•ë³´
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)
        self._initialized = True
        logger.info("âœ… Question Generator Initialized")

    def generate_questions(self, position: str, count: int = 5, reuse_ratio: float = 0.4):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì§ˆë¬¸ ìƒì„± ë¡œì§
        1. DBì—ì„œ ê²€ì¦ëœ ì§ˆë¬¸ ì¼ë¶€ ì¬í™œìš© (Reuse)
        2. ì¬í™œìš©ëœ ì§ˆë¬¸ì„ ì˜ˆì‹œ(Few-Shot)ë¡œ ì‚¼ì•„ ë‚˜ë¨¸ì§€ ì§ˆë¬¸ ìƒì„± (Create)
        """
        questions = []
        reuse_count = int(count * reuse_ratio)
        generate_count = count - reuse_count
        
        # 1. DB ì¬í™œìš© (ê²€ì¦ëœ ì§ˆë¬¸)
        # avg_scoreê°€ ë†’ê³  usage_countê°€ ì ì ˆí•œ ì§ˆë¬¸ ì¡°íšŒ
        db_questions = get_best_questions_by_position(position, limit=reuse_count * 2)
        
        # (ê°„ë‹¨í•œ ë¡œì§: ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì„ íƒ)
        selected_db_questions = db_questions[:reuse_count]
        
        for q in selected_db_questions:
            questions.append(q.content)
            increment_question_usage(q.id)
            logger.info(f"â™»ï¸ Reused Question (ID={q.id}): {q.content[:30]}...")

        # 2. Few-Shot í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        # DB ì§ˆë¬¸ë“¤ì„ ì˜ˆì‹œë¡œ ì œê³µí•˜ì—¬ ìŠ¤íƒ€ì¼ì„ ëª¨ë°©í•˜ê²Œ í•¨
        few_shot_examples = ""
        if db_questions:
            examples = [f"- {q.content}" for q in db_questions[:3]]
            few_shot_examples = "\n".join(examples)
        
        prompt_template = """### Role:
You are an expert technical interviewer for the '{position}' role.

### Task:
Generate a single, sharp, and practical interview question in Korean.

### Context (Reference Style):
Here are some good examples of questions for this role:
{examples}

### Instructions:
1. Write ONLY the question in Korean.
2. Do not include any introductory text like "Here is a question".
3. The question should be technical and specific to the role.
4. Avoid duplicating the reference examples.

### Question:
"""
        prompt = PromptTemplate.from_template(prompt_template)
        chain = prompt | self.llm | StrOutputParser()

        # 3. LLM ìƒì„± (ë‚˜ë¨¸ì§€ ê°œìˆ˜ë§Œí¼)
        for i in range(generate_count):
            try:
                raw_result = chain.invoke({
                    "position": position,
                    "examples": few_shot_examples
                })
                
                # í›„ì²˜ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°)
                generated_q = raw_result.strip().replace('"', '')
                
                if len(generated_q) > 10:
                    questions.append(generated_q)
                    logger.info(f"âœ¨ Generated Question: {generated_q[:30]}...")
                else:
                    # ì‹¤íŒ¨ ì‹œ í´ë°±
                    questions.append(self._get_fallback_question(position, i))
                    
            except Exception as e:
                logger.error(f"Generation failed: {e}")
                questions.append(self._get_fallback_question(position, i))

        return questions

    def _get_fallback_question(self, position, index):
        backups = [
            f"{position} ì§ë¬´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ì—­ëŸ‰ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ìµœê·¼ ê²ªì—ˆë˜ ê°€ì¥ ì–´ë ¤ìš´ ê¸°ìˆ ì  ì±Œë¦°ì§€ëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?",
            "ìš°ë¦¬ íšŒì‚¬ ì„œë¹„ìŠ¤ ì¤‘ ê°œì„ í•˜ê³  ì‹¶ì€ ë¶€ë¶„ì´ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë™ë£Œì™€ ê¸°ìˆ ì  ê²¬í•´ ì°¨ì´ê°€ ìˆì„ ë•Œ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?"
        ]
        return backups[index % len(backups)]

@shared_task(name="tasks.question_generator.generate_questions")
def generate_questions_task(position: str, count: int = 5):
    try:
        generator = QuestionGenerator()
        return generator.generate_questions(position, count)
    except Exception as e:
        logger.error(f"Task Error: {e}")
        return []

# Eager Initialization: Worker ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ
# ì´ë ‡ê²Œ í•˜ë©´ ì²« ìš”ì²­ì—ì„œ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
try:
    logger.info("ğŸ”¥ Pre-loading Question Generator model...")
    _warmup_generator = QuestionGenerator()
    logger.info("âœ… Question Generator ready for requests")
except Exception as e:
    logger.warning(f"âš ï¸ Failed to pre-load model (will load on first request): {e}")
