import sys
import os
import json
from sqlalchemy import text as sql_text

# -----------------------------------------------------------
# [ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_core_docker = "/backend-core"
backend_core_local = os.path.join(current_dir, "backend-core")

if os.path.exists(backend_core_docker):
    sys.path.append(backend_core_docker)
elif os.path.exists(backend_core_local):
    sys.path.append(backend_core_local)

# ğŸš¨ db.pyì—ì„œ engine ë¶ˆëŸ¬ì˜¤ê¸°
try:
    from db import engine
except ImportError as e:
    print(f"âŒ db.pyë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    sys.exit(1)

# -----------------------------------------------------------
# ë²¡í„° ë°ì´í„° ì €ì¥ í•¨ìˆ˜
# -----------------------------------------------------------
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
from langchain_community.vectorstores import PGVector
from langchain_core.documents import Document

# -----------------------------------------------------------
# [ëª¨ë¸ ì„¤ì •] Step 5ì™€ ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•¨
# -----------------------------------------------------------
try:
    from .embedding import get_embedder
except ImportError:
    from embedding import get_embedder

# -----------------------------------------------------------
# ë²¡í„° ë°ì´í„° ì €ì¥ í•¨ìˆ˜ (LangChain PGVector í™œìš©)
# -----------------------------------------------------------
def store_embeddings(resume_id, embedded_chunks):
    """
    LangChainì˜ PGVectorë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
<<<<<<< HEAD
=======
def store_embeddings(resume_id, embedded_chunks):
>>>>>>> bcab0a98e56e154aae50f9fad3ffa7ac7d936acf
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
    if not embedded_chunks:
        print("âŒ ì €ì¥í•  ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea
    print(f"\n[STEP6] DB ì €ì¥ ì‹œì‘ (Resume ID: {resume_id}, LangChain PGVector í™œìš©)...")

    # 1. ë¬¸ì„œí™” (Document ê°ì²´ ìƒì„±)
    documents = []
    for item in embedded_chunks:
        # ë©”íƒ€ë°ì´í„°ì— resume_id ê°•ì œ ì‚½ì… (í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ìš©)
        metadata = item.get("metadata", {})
        metadata["resume_id"] = resume_id
        metadata["chunk_type"] = item.get("type", "unknown")
        
        doc = Document(
            page_content=item["text"],
            metadata=metadata
        )
        documents.append(doc)

    # 2. PGVector ì—°ê²° ì„¤ì •
    # database.pyì˜ DATABASE_URLì„ ì‚¬ìš© (psycopg:// í˜•ì‹ì´ì–´ì•¼ í•¨)
    connection_string = os.getenv("DATABASE_URL", "postgresql+psycopg://postgres:1234@db:5432/interview_db")
    
    # 3. ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    import torch
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    embeddings = get_embedder(device)

    try:
        # 4. ì €ì¥ (collection_nameì„ í†µí•´ ë…¼ë¦¬ì  ë¶„ë¦¬ ê°€ëŠ¥)
        # ì—¬ê¸°ì„œëŠ” ë‹¨ì¼ í…Œì´ë¸”ì—ì„œ metadata í•„í„°ë§ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í‘œì¤€í™”
        vector_store = PGVector.from_documents(
            embedding=embeddings,
            documents=documents,
            collection_name="resume_all_embeddings",
            connection_string=connection_string,
            pre_delete_collection=False, # ì „ì²´ ì»¬ë ‰ì…˜ì„ ì§€ìš°ì§€ ì•ŠìŒ
        )
        
        # 5. ê¸°ì¡´ ë™ì¼ resume_id ë°ì´í„° ê´€ë¦¬
        # langchain_community ë²„ì „ì—ì„œëŠ” delete ê¸°ëŠ¥ì„ metadata filterì™€ í•¨ê»˜ ì“°ê¸° ê¹Œë‹¤ë¡œì›€
        # ë”°ë¼ì„œ í˜„ì¬ëŠ” ì¶”ê°€(Append) ëª¨ë“œë¡œ ë™ì‘í•˜ë©°, ì¶”í›„ ê´€ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
        print(f"[STEP6] âœ… ì´ {len(documents)}ê°œì˜ ì²­í¬ê°€ LangChain PGVectorì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ LangChain PGVector ì €ì¥ ì‹¤íŒ¨: {e}")
<<<<<<< HEAD
=======
    # 1. ë²¡í„° ì°¨ì› í™•ì¸ (ì˜ˆ: 768 or 1024)
    # ì²« ë²ˆì§¸ ì²­í¬ì˜ ë²¡í„° ê¸¸ì´ë¥¼ í™•ì¸í•˜ì—¬ í…Œì´ë¸” ìƒì„± ì‹œ ì‚¬ìš©
    vector_dim = len(embedded_chunks[0]["vector"])
    print(f"\n[STEP6] DB ì €ì¥ ì‹œì‘ (Resume ID: {resume_id}, ì°¨ì›: {vector_dim})...")

    try:
        with engine.begin() as conn:
            # 2. pgvector í™•ì¥ ì„¤ì¹˜ (í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë©´ ë¨)
            conn.execute(sql_text("CREATE EXTENSION IF NOT EXISTS vector;"))

            # 3. í…Œì´ë¸” ìƒì„±
            # resume_id: Step 3ì˜ resumes í…Œì´ë¸” idì™€ ì—°ê²°ë˜ëŠ” ì™¸ë˜í‚¤ ì—­í•  (Integer)
            create_table_sql = sql_text(f"""
                CREATE TABLE IF NOT EXISTS resume_embeddings (
                    id SERIAL PRIMARY KEY,
                    resume_id INTEGER, 
                    chunk_type TEXT,
                    chunk_text TEXT,
                    metadata JSONB,
                    embedding vector({vector_dim}),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
            """)
            conn.execute(create_table_sql)

            # 4. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¤‘ë³µ ì ì¬ ë°©ì§€)
            # í•´ë‹¹ ì´ë ¥ì„œ(resume_id)ì˜ ê¸°ì¡´ ë²¡í„°ë“¤ì„ ì§€ìš°ê³  ìƒˆë¡œ ë„£ìŠµë‹ˆë‹¤.
            conn.execute(sql_text("DELETE FROM resume_embeddings WHERE resume_id = :rid"), {"rid": resume_id})

            # 5. ë°ì´í„° ì‚½ì…
            insert_sql = sql_text("""
                INSERT INTO resume_embeddings 
                (resume_id, chunk_type, chunk_text, metadata, embedding)
                VALUES (:rid, :ctype, :ctext, :meta, :vec)
            """)

            for item in embedded_chunks:
                conn.execute(insert_sql, {
                    "rid": resume_id,
                    "ctype": item["type"],
                    "ctext": item["text"],
                    "meta": json.dumps(item["metadata"], ensure_ascii=False),
                    "vec": str(item["vector"]) # ë²¡í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ "[0.1, 0.2, ...]"ë¡œ ë³€í™˜
                })

        print(f"[STEP6] âœ… ì´ {len(embedded_chunks)}ê°œì˜ ì„ë² ë”© ë°ì´í„° ì €ì¥ ì™„ë£Œ!")

    except Exception as e:
        print(f"\nâŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        # ì°¨ì› ë¶ˆì¼ì¹˜ ì—ëŸ¬ì¼ ê²½ìš° íŒíŠ¸ ì¶œë ¥
        if "dimensions" in str(e):
            print("ğŸ’¡ íŒíŠ¸: DBì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”ì˜ ë²¡í„° ì°¨ì›ê³¼ í˜„ì¬ ëª¨ë¸ì˜ ì°¨ì›ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("   (í•´ê²°ì±…: DROP TABLE resume_embeddings; ëª…ë ¹ì–´ë¡œ í…Œì´ë¸”ì„ ì§€ìš°ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.)")
>>>>>>> bcab0a98e56e154aae50f9fad3ffa7ac7d936acf
=======
>>>>>>> d4e80d6d076861616e2c5afc84a50bbc841db3ea

# -----------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰: ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
# -----------------------------------------------------------
if __name__ == "__main__":
    try:
        # Step 1(load_resume)ì€ ì œê±°ë¨
        from parse_resume import parse_resume_final
        from chunking import chunk_resume
        from embedding import embed_chunks
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ Import ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # 1. íŒŒì¼ ê²½ë¡œ í™•ì¸
    target_pdf = "resume.pdf"
    if not os.path.exists(target_pdf):
        target_pdf = "/app/resume.pdf"
    
    if os.path.exists(target_pdf):
        print(f"ğŸš€ [Pipeline ì‹œì‘] íŒŒì¼: {target_pdf}")
        
        # Step 2: íŒŒì‹±
        parsed = parse_resume_final(target_pdf)
        
        if parsed:
            # Step 4: ì²­í‚¹
            chunks = chunk_resume(parsed)
            
            if chunks:
                # Step 5: ì„ë² ë”©
                embedded_data = embed_chunks(chunks)
                
                if embedded_data:
                    # Step 6: DB ì €ì¥
                    # Step 3ì—ì„œ resume_id=1ë¡œ ì €ì¥í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œë„ 1ë¡œ ë§ì¶¤
                    store_embeddings(resume_id=1, embedded_chunks=embedded_data)
                else:
                    print("âŒ ì„ë² ë”© ì‹¤íŒ¨")
            else:
                print("âŒ ì²­í‚¹ ë°ì´í„° ì—†ìŒ")
        else:
            print("âŒ íŒŒì‹± ì‹¤íŒ¨")
    else:
        print("âŒ íŒŒì¼ ì—†ìŒ")
