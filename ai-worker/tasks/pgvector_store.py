import sys
import os
import json
from sqlalchemy import text as sql_text

# -----------------------------------------------------------
# [ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_core_docker = "/backend-core"
backend_core_local = os.path.join(current_dir, "backend-core")

if os.path.exists(backend_core_docker):
    sys.path.append(backend_core_docker)
elif os.path.exists(backend_core_local):
    sys.path.append(backend_core_local)

# ğŸš¨ db.pyì—ì„œ engine ë¶ˆëŸ¬ì˜¤ê¸°
try:
    from db import engine
except ImportError as e:
    print(f"âŒ db.pyë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
    sys.exit(1)

# -----------------------------------------------------------
# ë²¡í„° ë°ì´í„° ì €ì¥ í•¨ìˆ˜
# -----------------------------------------------------------
from langchain_community.vectorstores import PGVector
from langchain_core.documents import Document

# -----------------------------------------------------------
# [ëª¨ë¸ ì„¤ì •] Step 5ì™€ ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•¨
# -----------------------------------------------------------
try:
    from .embedding import get_embedder
except ImportError:
    from embedding import get_embedder

# -----------------------------------------------------------
# ë²¡í„° ë°ì´í„° ì €ì¥ í•¨ìˆ˜ (LangChain PGVector í™œìš©)
# -----------------------------------------------------------
def store_embeddings(resume_id, embedded_chunks):
    """
    LangChainì˜ PGVectorë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if not embedded_chunks:
        print("âŒ ì €ì¥í•  ì„ë² ë”© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\n[STEP6] DB ì €ì¥ ì‹œì‘ (Resume ID: {resume_id}, LangChain PGVector í™œìš©)...")

    # 1. ë¬¸ì„œí™” (Document ê°ì²´ ìƒì„±)
    documents = []
    for item in embedded_chunks:
        # ë©”íƒ€ë°ì´í„°ì— resume_id ê°•ì œ ì‚½ì… (í•˜ì´ë¸Œë¦¬ë“œ í•„í„°ë§ìš©)
        metadata = item.get("metadata", {})
        metadata["resume_id"] = resume_id
        metadata["chunk_type"] = item.get("type", "unknown")
        
        doc = Document(
            page_content=item["text"],
            metadata=metadata
        )
        documents.append(doc)

    # 2. PGVector ì—°ê²° ì„¤ì •
    # database.pyì˜ DATABASE_URLì„ ì‚¬ìš© (psycopg:// í˜•ì‹ì´ì–´ì•¼ í•¨)
    connection_string = os.getenv("DATABASE_URL", "postgresql+psycopg://postgres:1234@db:5432/interview_db")
    
    # 3. ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    import torch
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    embeddings = get_embedder(device)

    try:
        # 4. ì €ì¥ (collection_nameì„ í†µí•´ ë…¼ë¦¬ì  ë¶„ë¦¬ ê°€ëŠ¥)
        # ì—¬ê¸°ì„œëŠ” ë‹¨ì¼ í…Œì´ë¸”ì—ì„œ metadata í•„í„°ë§ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í‘œì¤€í™”
        vector_store = PGVector.from_documents(
            embedding=embeddings,
            documents=documents,
            collection_name="resume_all_embeddings",
            connection_string=connection_string,
            pre_delete_collection=False, # ì „ì²´ ì»¬ë ‰ì…˜ì„ ì§€ìš°ì§€ ì•ŠìŒ
        )
        
        # 5. ê¸°ì¡´ ë™ì¼ resume_id ë°ì´í„° ê´€ë¦¬
        # langchain_community ë²„ì „ì—ì„œëŠ” delete ê¸°ëŠ¥ì„ metadata filterì™€ í•¨ê»˜ ì“°ê¸° ê¹Œë‹¤ë¡œì›€
        # ë”°ë¼ì„œ í˜„ì¬ëŠ” ì¶”ê°€(Append) ëª¨ë“œë¡œ ë™ì‘í•˜ë©°, ì¶”í›„ ê´€ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŒ
        print(f"[STEP6] âœ… ì´ {len(documents)}ê°œì˜ ì²­í¬ê°€ LangChain PGVectorì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ LangChain PGVector ì €ì¥ ì‹¤íŒ¨: {e}")

# -----------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰: ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
# -----------------------------------------------------------
if __name__ == "__main__":
    try:
        # Step 1(load_resume)ì€ ì œê±°ë¨
        from parse_resume import parse_resume_final
        from chunking import chunk_resume
        from embedding import embed_chunks
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ Import ì‹¤íŒ¨: {e}")
        sys.exit(1)

    # 1. íŒŒì¼ ê²½ë¡œ í™•ì¸
    target_pdf = "resume.pdf"
    if not os.path.exists(target_pdf):
        target_pdf = "/app/resume.pdf"
    
    if os.path.exists(target_pdf):
        print(f"ğŸš€ [Pipeline ì‹œì‘] íŒŒì¼: {target_pdf}")
        
        # Step 2: íŒŒì‹±
        parsed = parse_resume_final(target_pdf)
        
        if parsed:
            # Step 4: ì²­í‚¹
            chunks = chunk_resume(parsed)
            
            if chunks:
                # Step 5: ì„ë² ë”©
                embedded_data = embed_chunks(chunks)
                
                if embedded_data:
                    # Step 6: DB ì €ì¥
                    # Step 3ì—ì„œ resume_id=1ë¡œ ì €ì¥í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œë„ 1ë¡œ ë§ì¶¤
                    store_embeddings(resume_id=1, embedded_chunks=embedded_data)
                else:
                    print("âŒ ì„ë² ë”© ì‹¤íŒ¨")
            else:
                print("âŒ ì²­í‚¹ ë°ì´í„° ì—†ìŒ")
        else:
            print("âŒ íŒŒì‹± ì‹¤íŒ¨")
    else:
        print("âŒ íŒŒì¼ ì—†ìŒ")
