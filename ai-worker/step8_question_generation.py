import sys
import os
import time
import gc 
from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager
from langchain_core.prompts import PromptTemplate

# -----------------------------------------------------------
# [ëª¨ë¸ ê²½ë¡œ ì„¤ì •]
# -----------------------------------------------------------
local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
docker_path = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

if os.path.exists(local_path):
    model_path = local_path
elif os.path.exists(docker_path):
    model_path = docker_path
else:
    print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

 # -----------------------------------------------------------
# [í”„ë¡¬í”„íŠ¸ ìˆ˜ì •] â˜… ì—¬ê¸°ê°€ ì›ì¸ì…ë‹ˆë‹¤! ì•„ë˜ ë‚´ìš©ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”.
# -----------------------------------------------------------
PROMPT_TEMPLATE = """[|system|]
ë„ˆëŠ” 15ë…„ ì°¨ ë² í…Œë‘ 'ë³´ì•ˆ ì§ë¬´ ë©´ì ‘ê´€'ì´ë‹¤. 
ì§€ê¸ˆì€ **ë©´ì ‘ì´ í•œì°½ ì§„í–‰ ì¤‘ì¸ ìƒí™©**ì´ë‹¤. (ìê¸°ì†Œê°œëŠ” ì´ë¯¸ ëë‚¬ë‹¤.)
ì œê³µëœ [ì´ë ¥ì„œ ë‚´ìš©]ì„ ê·¼ê±°ë¡œ, í•´ë‹¹ ë‹¨ê³„({stage})ì— ë§ëŠ” **ë‚ ì¹´ë¡œìš´ ì§ˆë¬¸ 1ê°œ**ë§Œ ë˜ì ¸ë¼.

[ì‘ì„± ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­] 
1. **"ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤" ì ˆëŒ€ ê¸ˆì§€.** (ì´ë¯¸ í–ˆë‹¤ê³  ê°€ì •)
2. **"(ì ì‹œ ì¹¨ë¬µ)", "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤"** ê°™ì€ ê°€ìƒì˜ ì§€ë¬¸ì´ë‚˜ ëŒ€ë³¸ì„ ì“°ì§€ ë§ˆë¼.
3. ì§ˆë¬¸ ì•ë’¤ì— ì‚¬ì¡±ì„ ë¶™ì´ì§€ ë§ê³  **ì§ˆë¬¸ë§Œ ê¹”ë”í•˜ê²Œ** ì¶œë ¥í•˜ë¼.

[ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ]
1. ì‹œì‘ì€ ë°˜ë“œì‹œ **"{name}ë‹˜,"** ìœ¼ë¡œ ë¶€ë¥´ë©° ì‹œì‘í•  ê²ƒ.
2. **"ì´ë ¥ì„œë¥¼ ë³´ë‹ˆ...", "ìì†Œì„œë¥¼ ì½ì–´ë³´ë‹ˆ ~ë¼ê³  í•˜ì…¨ëŠ”ë°..."** ì²˜ëŸ¼ ê·¼ê±°ë¥¼ ëª…í™•íˆ ëŒˆ ê²ƒ.
3. ë§íˆ¬ëŠ” ì •ì¤‘í•˜ë©´ì„œë„ ì˜ˆë¦¬í•œ ë©´ì ‘ê´€ í†¤(..í•˜ì…¨ëŠ”ë°, ..ì„¤ëª…í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?)ì„ ìœ ì§€í•  ê²ƒ.
[|endofturn|]
[|user|]
# í‰ê°€ ë‹¨ê³„: {stage}
# í‰ê°€ ì˜ë„: {guide}
# ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±°:
{context}

# ìš”ì²­:
ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {name} ì§€ì›ìì—ê²Œ **ë‹¨ë„ì§ì…ì ìœ¼ë¡œ** ì§ˆë¬¸ì„ ë˜ì ¸ì¤˜.
[|endofturn|]
[|assistant|]
"""

def generate_human_like_question(llm, name, stage, guide, context_list):
    if not context_list:
        return "âŒ (ê´€ë ¨ ì´ë ¥ì„œ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•´ ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"

    texts = [item['text'] for item in context_list] if isinstance(context_list[0], dict) else context_list
    context_text = "\n".join([f"- {txt}" for txt in texts])
    
    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
    chain = prompt | llm
    
    try:
        result = chain.invoke({
            "name": name,
            "stage": stage,
            "guide": guide,
            "context": context_text
        })
        return str(result).strip()
    except Exception as e:
        return f"ì—ëŸ¬ ë°œìƒ: {e}"

# -----------------------------------------------------------
# [ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜]
# -----------------------------------------------------------
def main():
    # 1. LLM ì´ˆê¸°í™”
    callback_manager = CallbackManager([])
    
    print(f"Loading Model: {model_path} ...")
    llm = LlamaCpp(
        model_path=model_path,
        temperature=0.4,
        n_ctx=4096,
        max_tokens=512,
        n_gpu_layers=-1,
        verbose=False,
        stop=["[|endofturn|]", "[|assistant|]"]
    )

    # 2. Step 7 ëª¨ë“ˆ ë¡œë“œ
    try:
        # â˜… ì£¼ì˜: ì´ì „ì— ë“œë¦° 'í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰'ì´ ì ìš©ëœ step7_retrieve.py ì—¬ì•¼ í•©ë‹ˆë‹¤!
        from step7_rag_retrieval import retrieve_context 
    except ImportError:
        print("âŒ step7_rag_retrieval.py ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. ì§€ì›ì ì •ë³´ ë° ì‹œë‚˜ë¦¬ì˜¤
    candidate_name = "ìµœìŠ¹ìš°"
    resume_id = 1

    generic_interview_plan = [
        {
            "stage": "1. ì§ë¬´ ì§€ì‹ í‰ê°€",
            "search_query": "ë³´ì•ˆ ê¸°ìˆ  ìŠ¤í‚¬ ë„êµ¬ í™œìš© ëŠ¥ë ¥",
            "filter_category": "certification",  # ìê²©ì¦/ìŠ¤í‚¬ì—ì„œ ì°¾ê¸°
            "guide": "ì§€ì›ìê°€ ì‚¬ìš©í•œ ê¸°ìˆ (Tool, Language)ì˜ êµ¬ì²´ì ì¸ ì„¤ì •ë²•ì´ë‚˜, ê¸°ìˆ ì  ì›ë¦¬(Deep Dive)ë¥¼ ë¬¼ì–´ë³¼ ê²ƒ."
        },
        {
            "stage": "2. ì§ë¬´ ê²½í—˜ í‰ê°€",
            "search_query": "í”„ë¡œì íŠ¸ ì„±ê³¼ ë‹¬ì„± ê²½í—˜ ê²°ê³¼",
            "filter_category": "project",       # í”„ë¡œì íŠ¸ì—ì„œë§Œ ì°¾ê¸° (ì¤‘ìš”!)
            "guide": "í”„ë¡œì íŠ¸ì—ì„œ ë‹¬ì„±í•œ ìˆ˜ì¹˜ì  ì„±ê³¼(%)ì˜ ê²°ì •ì  ìš”ì¸ì´ ë¬´ì—‡ì¸ì§€, êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ë°ì´í„°ë¥¼ ë‹¤ë¤˜ëŠ”ì§€ ë¬¼ì–´ë³¼ ê²ƒ."
        },
        {
            "stage": "3. ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ í‰ê°€",
            "search_query": "ë¬¸ì œ í•´ê²° ê¸°ìˆ ì  ë‚œê´€ ê·¹ë³µ",
            # í•„í„° ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì°¾ìŒ (None)
            "guide": "ì§€ì›ìê°€ ì§ë©´í•œ í•œê³„ì ì´ë‚˜ ë¬¸ì œ ìƒí™©ì„ ì–´ë–»ê²Œ ì •ì˜í–ˆê³ , ì–´ë–¤ ë…¼ë¦¬ì  ì‚¬ê³  ê³¼ì •ì„ í†µí•´ í•´ê²°ì±…ì„ ë„ì¶œí–ˆëŠ”ì§€ ë¬¼ì–´ë³¼ ê²ƒ."
        },
        {
            "stage": "4. ì˜ì‚¬ì†Œí†µ ë° í˜‘ì—… í‰ê°€",
            "search_query": "í˜‘ì—… ê°ˆë“± í•´ê²° ì„¤ë“",
            "filter_category": "narrative",     # ìì†Œì„œì—ì„œ ì°¾ê¸°
            "guide": "íŒ€ì›ê³¼ì˜ ì˜ê²¬ ëŒ€ë¦½ ìƒí™©ì—ì„œ ë³¸ì¸ì˜ ì£¼ì¥ì„ ê´€ì² ì‹œí‚¤ê¸° ìœ„í•´ ì–´ë–¤ ê°ê´€ì  ê·¼ê±°ë¥¼ ì‚¬ìš©í–ˆëŠ”ì§€ ëŒ€í™” ê³¼ì •ì„ ë¬¼ì–´ë³¼ ê²ƒ."
        },
               {

            "stage": "5. ì±…ì„ê° ë° ê°€ì¹˜ê´€ í‰ê°€",

            "search_query": "ì§ì—… ìœ¤ë¦¬ ëª©í‘œ ê°€ì¹˜ê´€",  

            "filter_category": "narrative",     # RAGê°€ 'ê°€ì¹˜ê´€' ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ì˜´

            "guide": "ì´ìƒì ì¸ ëª©í‘œ(ì™„ë²½í•¨)ì™€ í˜„ì‹¤ì ì¸ ì œì•½(íš¨ìœ¨ì„±, ì˜¤íƒ ë“±) ì‚¬ì´ì—ì„œ ì–´ë–»ê²Œ ê· í˜•ì„ ë§ì¶œ ê²ƒì¸ì§€ ë¬¼ì–´ë³¼ ê²ƒ."

        },
        {

            "stage": "6. ë³€í™” ìˆ˜ìš©ë ¥ ë° ì„±ì¥ì˜ì§€ í‰ê°€",

            "search_query": "ì„±ì¥ ê³„íš ìê¸°ê³„ë°œ ë¯¸ë˜",

            "filter_category": "narrative",        # RAGê°€ 'ë¯¸ë˜ ê³„íš' ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ì˜´

            "guide": "í˜„ì¬ ê¸°ìˆ  íŠ¸ë Œë“œ ë³€í™”ì— ë§ì¶° êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ í•™ìŠµì„ í•˜ê³  ìˆìœ¼ë©°, ì´ë¥¼ ì‹¤ë¬´ì— ì–´ë–»ê²Œ ì ìš©í•  ê³„íšì¸ì§€ ë¬¼ì–´ë³¼ ê²ƒ."

        }



        # ... í•„ìš”ì‹œ ì¶”ê°€ ...
    ]

    print(f"\nğŸš€ [AI ë©´ì ‘ê´€ ({candidate_name} ì§€ì›ì) ë©´ì ‘ ì‹œì‘]")
    print("=" * 60)

    for step in generic_interview_plan:
        print(f"\nğŸ“Œ {step['stage']}")
        
        # ------------------------------------------------------------------
        # â˜… [ìˆ˜ì •ë¨] filter_category ê°’ì„ í•¨ìˆ˜ì— ì „ë‹¬í•˜ëŠ” ë¶€ë¶„!
        # ------------------------------------------------------------------
        contexts = retrieve_context(
            step['search_query'], 
            resume_id=resume_id, 
            top_k=2,
            filter_category=step.get('filter_category') # ğŸ‘ˆ ì—¬ê¸°ê°€ í•µì‹¬ì…ë‹ˆë‹¤!
        )
        
        if contexts:
            # ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
            preview = contexts[0]['text'].replace("\n", " ")[:60]
            # (ë””ë²„ê¹…ìš©) ì–´ë–¤ ì¹´í…Œê³ ë¦¬ê°€ ê±¸ë ¸ëŠ”ì§€ í™•ì¸
            meta_info = contexts[0].get('meta', {})
            print(f"   ğŸ“„ [ê·¼ê±° ë°ì´í„°({meta_info.get('category', 'N/A')})]: {preview}...")
        else:
            print("   âŒ (ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í•¨)")
            continue

        # ì§ˆë¬¸ ìƒì„±
        question = generate_human_like_question(
            llm=llm,
            name=candidate_name,
            stage=step['stage'],
            guide=step['guide'],
            context_list=contexts
        )
        
        print(f"\nğŸ¤ [AI ë©´ì ‘ê´€ì˜ ì§ˆë¬¸]")
        print("-" * 60)
        print(question)
        print("-" * 60)
        
        time.sleep(2)

    # 4. ì¢…ë£Œ
    print("\nâœ… ë©´ì ‘ ì¢…ë£Œ. ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...")
    del llm
    gc.collect()

if __name__ == "__main__":
    main()