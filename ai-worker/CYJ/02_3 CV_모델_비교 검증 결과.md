# 🎓 AI 면접관(AI Interviewer) Vision 분석 시스템 기술 리포트

**작성일**: 2026년 2월 11일
**작성자**: Project Big20 AI Team 조윤재
**버전**: V5.2 (Pure MediaPipe Edition - Detailed Decision Log)

---

## 1. 🚀 프로젝트 개요 (Overview)

본 프로젝트는 면접자의 **비언어적 요소(Non-verbal communication)**를 실시간으로 분석하여, 면접 태도, 시선 처리, 자신감, 정서 불안 등을 정량적인 데이터로 평가하는 **온프레미스(On-Premise) AI 비전 시스템**입니다.

기존의 단순 녹화 방식이나 무거운 딥러닝 모델(DeepFace 등)의 한계를 극복하기 위해, **구글 미디어파이프(MediaPipe)** 단일 프레임워크 기반으로 시스템을 경량화 및 최적화하였습니다. 이를 통해 GPU가 없는 일반 노트북 환경에서도 **초저지연(Low Latency) 실시간 분석**이 가능합니다.

---

## 2. 🧠 모델 및 기술 스펙 (Technical Specifications)

### 2.1. ✅ 핵심 라이브러리 및 모델 정보

| 구분                      | 상세 내용                               | 출처/제공                                                                            |
| :------------------------ | :-------------------------------------- | :----------------------------------------------------------------------------------- |
| **핵심 라이브러리** | `mediapipe` (Python Package v0.10.x)  | [Google LLC](https://pypi.org/project/mediapipe/)                                       |
| **AI 모델 파일**    | `face_landmarker.task` (Bundle Model) | [Google AI Edge](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker) |
| **모델 아키텍처**   | Face Mesh V2 (Attention Mesh 기반)      | Google Research                                                                      |
| **랜드마크 개수**   | 478개 3D 좌표 (Iris 포함)               | -                                                                                    |
| **보조 라이브러리** | `OpenCV (cv2)`, `NumPy`, `Pillow` | Open Source Community                                                                |

---

## 3. 💡 기술적 의사결정 과정 (Technical Decision Making Process)

본 프로젝트는 초기 기획 단계에서 `DeepFace`와 `Haar Cascade`를 사용하는 하이브리드 방식을 채택했으나, 엄격한 성능 테스트 과정을 거쳐 **Pure MediaPipe** 아키텍처로 최종 변경되었습니다. 그 상세한 의사결정 과정은 다음과 같습니다.

### 3.1. 1단계: 초기 가설 및 시도 (Initial Approach)

* **초기 설계**: `DeepFace`의 감정 인식(Emotion Analysis) 정확도가 가장 높을 것이라 판단하여 메인 모델로 선정했고, 눈동자 추적을 위해 가벼운 `Haar Cascade`를 보조로 사용했습니다.
* **기대 효과**: 검증된 SOTA(State-of-the-Art) 모델인 DeepFace(VGG-Face 기반)를 사용함으로써 "분석의 신뢰도"를 확보하고자 했습니다.

### 3.2. 2단계: 문제점 발견 (Problem Identification)

실제 통합 테스트(Integration Test) 과정에서 다음과 같은 치명적인 한계가 드러났습니다.

1. **지연 시간(Latency) 문제**:
   * DeepFace는 이미지 한 장을 분석하는 데 평균 **150ms ~ 200ms**가 소요되었습니다.
   * 이는 1초에 5프레임(5 FPS) 정도만 처리가 가능하다는 뜻으로, 면접자의 미세한 표정 변화를 놓치는 결과로 이어졌습니다.
2. **리소스 점유율(Resource Contention)**:
   * 백엔드에서 이미 STT(Whisper)와 LLM(Solar/EXAONE)이 무겁게 돌아가는 상황에서, Vision 모델까지 무거운 DeepFace를 돌리자 **CPU 점유율이 100%를 치며 시스템이 멈추는 현상**이 발생했습니다.
3. **2D 분석의 한계**:
   * Haar Cascade는 2D 평면상에서 눈의 위치(X, Y)만 찾을 수 있었습니다.
   * 면접자가 고개를 앞으로 숙이거나(Pitch), 비스듬히 앉는(Yaw/Roll) **3차원적인 자세 불량**을 전혀 감지하지 못했습니다.

### 3.3. 3단계: 대안 비교 및 결정 (Comparison & Decision)

이에 구글의 `MediaPipe Face Mesh V2`를 대안으로 도입하여 비교 테스트를 진행했습니다.

| 비교 항목             | DeepFace + Haar (초기안) | **MediaPipe V2 (최종안)** | 테스트 결과 해석                                       |
| :-------------------- | :----------------------- | :------------------------------ | :----------------------------------------------------- |
| **처리 속도**   | 200ms / frame (느림)     | **25ms / frame (즉시)**   | MediaPipe가 약**8배** 빠름. (실시간성 확보)      |
| **공간 분석**   | 2D (X, Y 좌표)           | **3D (X, Y, Z 좌표)**     | 고개 숙임 및 거리 측정 가능해짐.                       |
| **표정 디테일** | 7가지 단순 감정          | **52개 Blendshapes**      | "입꼬리만 살짝 올림", "눈썹 찌푸림" 등 정밀 분석 가능. |
| **리소스**      | 1.2GB 메모리 점유        | **250MB 메모리 점유**     | 백엔드 부하를 최소화하여 전체 시스템 안정성 확보.      |

> **🎯 최종 결론 (Final Verdict)**:
> *"실시간 면접 코칭 시스템에는 '무거운 정확도'보다는 **'빠른 반응 속도'**와 **'3차원 자세 분석'**이 필수적이다."* 라는 결론에 도달하여, **Pure MediaPipe** 단일 모델 체제로 확정하였습니다.

---

## 4. 🛠️ 핵심 구현 기술 (Key Technologies)

### 4.1. ⚖️ 통합 영점 조절 시스템 (Total Calibration System)

모든 사용자의 앉은키와 카메라 위치가 다르다는 점에 착안하여, **'상대 좌표 기반 영점 조절 알고리즘'**을 독자 개발했습니다.

* **작동 원리**: 사용자가 `s` 키를 누르는 순간, 현재의 **눈동자 위치, 고개 각도, 얼굴 기울기**를 '기준점(0)'으로 저장하고, 이후 발생하는 **변화량(Delta)**만을 측정합니다.
* **효과**: 카메라가 측면에 있거나 사용자가 비스듬히 앉아 있어도 정확한 '정면 응시' 판정이 가능합니다.

### 4.2. 📊 4-Factor 가중치 평가 모델

단순 감정 인식을 넘어, 면접에 특화된 4가지 핵심 역량을 정의하고 가중치를 적용하여 최종 점수를 산출합니다.

$$
\text{Total Score} = (Confidence \times 0.3) + (Focus \times 0.3) + (Posture \times 0.2) + (Stability \times 0.2)
$$

| 평가 항목                          | 비중          | 측정 원리 및 기준 (Measurement Criteria)                                                                                                                                                                                                                                                     |
| :--------------------------------- | :------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. 자신감 (Confidence)**   | **30%** | **`Average(mouthSmile) * 100`**`<br>`- **원리**: MediaPipe Blendshapes의 `mouthSmileLeft/Right` 평균값.`<br>`- **기준**: 미소 수치 **0.4 이상** 유지 시 긍정 평가.`<br>`- **의의**: 억지 웃음이 아닌, 대화 중 자연스럽게 배어나는 미소 분석.         |
| **2. 시선 집중 (Focus)**     | **30%** | **`(Center_Gaze_Frames / Total_Frames) * 100`**`<br>`- **원리**: 눈동자(Iris) 랜드마크(#468)의 영점 대비 변화량(Delta) 측정.`<br>`- **기준**: 오차 범위 **X:±0.08, Y:±0.08** 이내 유지 비율.`<br>`- **의의**: 카메라(면접관)를 회피하지 않는 집중력. |
| **3. 자세 안정 (Posture)**   | **20%** | **`(Stable_Pose_Frames / Total_Frames) * 100`**`<br>`- **원리**: 코(#1)-턱(#152) 벡터의 3D 회전각(Pitch/Roll) 계산.`<br>`- **기준**: 변화량 임계값 **0.008** 미만 유지 비율.`<br>`- **의의**: 산만함 없는 바른 자세 유지 능력.                         |
| **4. 정서 안정 (Stability)** | **20%** | **`100 - (Average(browDown) * 100)`**`<br>`- **원리**: 미간 찌푸림(`browDown`) 수치를 역산(100 - X).`<br>`- **기준**: 당황하여 미간을 찌푸리는 빈도가 낮을수록 고득점.`<br>`- **의의**: 압박 질문에도 평정심을 유지하는 능력.                              |

### 4.3. 📉 긴장 타임라인 추적 (Anxiety Timeline)

* **이벤트 트리거**: `browDown` 수치 **0.4 초과** 시.
* **쿨다운(Cooldown)**: 2초.
* **활용**: 면접 종료 후 *"3분 20초 경에 답변이 막혀 당황하셨습니다."*와 같은 **초정밀 피드백** 제공.

---

## 5. 📊 최종 산출물 (Outcomes)

### 5.1. 실시간 피드백 화면 (Live HUD)

면접 진행 중 화면에 증강현실(AR) 형태로 정보를 오버레이합니다.

> `👀 시선: 정면 응시` | `👤 자세: ✅ 안정` | `✨ 긍정 지수: 85%`

### 5.2. 종합 분석 리포트 (JSON)

면접 종료 시, 정량적 데이터가 포함된 성적표가 자동 생성됩니다.

```json
{
    "score": 88.5,
    "metrics": {
        "confidence": 28.4,
        "focus": 29.1,
        "posture": 18.0,
        "stability": 13.0
    },
    "anxiety_analysis": {
        "max_anxiety_level": 75.4,
        "tension_events_count": 3,
        "timestamps": [12.5, 45.2, 120.1]
    }
}
```

---

## 6. 🏁 결론 및 활용 계획 (Conclusion)

본 프로젝트를 통해 개발된 **AI 면접 분석 시스템(V5.2)**은 고비용의 GPU 서버 없이도 **웹캠 하나만으로 전문가 수준의 비언어적 행동 분석**이 가능함을 입증했습니다.

향후 이 모듈은 다음과 같이 활용될 예정입니다.

1. **모의 면접 트레이닝**: 취업 준비생이 스스로 영상을 보며 시선과 자세를 교정.
2. **실전 면접 보조 도구**: 면접관에게 지원자의 '긴장도'나 '진실성'을 참고할 수 있는 보조 지표 제공.
3. **데이터베이스화**: 면접 데이터를 누적하여 '합격하는 관상(태도)'에 대한 AI 모델링 고도화.

---

**[부록] 참고 자료 (References)**

1. **MediaPipe Vision Solutions**: [https://developers.google.com/mediapipe/solutions/vision](https://developers.google.com/mediapipe/solutions/vision)
2. **Ozgur Karaoglu**: *"Driver Drowsiness Detection using MediaPipe"* (Iris Tracking 알고리즘 참조)
3. **Paul Ekman**: *"Facial Action Coding System (FACS)"* (Blendshapes 해석의 이론적 배경)

---

## 7. 🔄 최근 업데이트 및 최적화 (System Updates - 2026-02-20)

### 7.1. 🔊 멀티모달(Bi-Modal) 분석 확장: 오디오 자신감 평가

단순히 표정과 자세뿐만 아니라, **목소리(Audio)**의 특성을 분석하여 면접자의 자신감을 종합적으로 평가하는 기능을 추가하였습니다.

*   **기술 스택**: `NumPy` 기반 PCM 데이터 실시간 분석 (RMS, Zero-Crossing Rate).
*   **측정 지표**:
    1.  **성량 (Volume)**: RMS(Root Mean Square) 에너지를 정규화하여 목소리 크기 측정. (기준값 0.02 이상 유효)
    2.  **발화 속도 (Speaking Density)**: 일정 임계값 이상의 신호 밀도를 계산하여 말의 빠르기 및 유창성 평가.
*   **점수 산출**: `Confidence_Score = (Volume * 0.5) + (Speed * 0.5)`
*   **기대 효과**: 우물쭈물하거나 너무 작게 말하는 습관을 감지하여 구체적인 피드백("좀 더 크게 말씀하세요") 제공 가능.

### 7.2. ⚡ 시스템 안정성 및 성능 최적화 (Stability Patch)

Vision(영상) 분석과 Audio(음성) 분석이 동시에 수행되면서 발생한 **시스템 부하(CPU Overhead)** 문제를 해결하기 위해 다음과 같은 최적화를 적용하였습니다.

#### 1. 비동기 처리 및 스레드 분리
*   **문제**: MediaPipe의 `process_frame()` 함수가 CPU 연산을 많이 점유하여, 메인 **Event Loop**를 차단(Blocking)함. 이로 인해 WebRTC 패킷 수신이 지연되어 영상 끊김 발생.
*   **해결**: `asyncio.loop.run_in_executor(None, ...)`를 사용하여 무거운 분석 작업을 별도 **스레드 풀(Thread Pool)**로 분리. 네트워크 I/O와 컴퓨팅 연산을 병렬 처리.

#### 2. STT 큐(Queue) 적체 방지 알고리즘 (Backpressure)
*   **문제**: 사용자의 발화 속도가 STT 처리 속도보다 빠를 경우, 미처리된 오디오 청크가 무한정 쌓여 **메모리 누수(OOM)** 및 1분 이상의 응답 지연 발생.
*   **해결**: `pending_stt` 카운터 도입.
    *   현재 처리 중인 작업이 설정된 임계값(`MAX_PENDING=2`)을 초과하면, 새로운 오디오 패킷을 과감히 **드랍(Drop)**하여 시스템 실시간성 유지.
    *   **"모든 것을 다 듣고 늦게 반응하는 것보다, 일부를 놓치더라도 즉시 반응하는 것이 면접 경험에 유리하다"**는 판단.

#### 3. 분석 빈도(FPS) 제한
*   **전략**: 인간의 표정 변화는 0.2초 이내에 급격히 변하지 않음. 불필요한 연산을 줄이기 위해 분석 주기를 **5 FPS (0.2초 간격)**로 제한하여, CPU 자원을 LLM(질문 생성)과 STT(음성 인식)에 양보.
