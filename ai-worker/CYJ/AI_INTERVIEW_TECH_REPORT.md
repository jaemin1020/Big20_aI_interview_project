# 🎓 AI 면접관(AI Interviewer) Vision 분석 시스템 기술 리포트

**작성일**: 2026년 2월 5일
**작성자**: Project Big20 AI Team 조윤재
**버전**: V4.5 (Final Release)

---

## 1. 🚀 프로젝트 개요 (Overview)

본 프로젝트는 면접자의 **비언어적 요소(Non-verbal communication)**를 실시간으로 분석하여, 면접 태도, 시선 처리, 자신감 등을 정량적인 데이터로 평가하는 **온프레미스(On-Premise) AI 비전 시스템**을 구축하는 것을 목표로 합니다.

기존의 단순 녹화 방식과 달리, AI가 실시간으로 면접자의 미세한 표정 변화와 자세를 추적하여 **즉각적인 피드백과 종합 리포트**를 제공합니다.

---

## 2. 🧠 모델 및 기술 스펙 (Technical Specifications)

본 시스템은 외부 API 호출 없이 로컬 환경에서 100% 구동되는 **Google MediaPipe** 솔루션을 단독 채택했습니다. 초기에는 `DeepFace`(VGG-Face 기반) 도입을 검토했으나, 실시간 면접 분석에 필수적인 **초저지연(Low Latency)**과 **3D 공간 분석 능력**을 위해 MediaPipe 단일 모델 최적화로 결정했습니다.

### ✅ 핵심 라이브러리 및 모델 정보

| 구분                      | 상세 내용                               | 출처/제공                                                                            |
| :------------------------ | :-------------------------------------- | :----------------------------------------------------------------------------------- |
| **핵심 라이브러리** | `mediapipe` (Python Package v0.10.x)  | [Google LLC](https://pypi.org/project/mediapipe/)                                       |
| **AI 모델 파일**    | `face_landmarker.task` (Bundle Model) | [Google AI Edge](https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker) |
| **모델 아키텍처**   | Face Mesh V2 (Attention Mesh 기반)      | Google Research                                                                      |
| **랜드마크 개수**   | 478개 3D 좌표 (Iris 포함)               | -                                                                                    |
| **보조 라이브러리** | `OpenCV (cv2)`, `NumPy`, `Pillow` | Open Source Community                                                                |

### ✅ 최종 아키텍처: 하이브리드 모델 (Hybrid Model)

각 모델의 장점만을 취하여 프로세스를 2단계로 분리한 **하이브리드 아키텍처**를 최종 채택했습니다.

| 단계 (Phase)                       | 사용 모델           | 주요 기능          | 기술적 근거                                              |
| :--------------------------------- | :------------------ | :----------------- | :------------------------------------------------------- |
| **Step 1. 입장 (Check-in)**  | **DeepFace**  | 본인 인증 (Verify) | 이력서 사진과 현재 얼굴의 1:1 매칭 정확도(99%+)가 최우선 |
| **Step 2. 면접 (Interview)** | **MediaPipe** | 실시간 태도 분석   | 0.03초 이내의 초고속 추론으로 대화 흐름 유지 필수        |

> **분석 결론**: DeepFace는 대리 면접을 원천 차단하는 '보안관(Security)' 역할을, MediaPipe는 면접 과정을 코칭하는 '감독관(Coach)' 역할을 수행하는 상호 보완적 관계입니다.

---

## 3. 🛠️ 핵심 구현 기술 (Key Technologies)

### 3.1. 통합 영점 조절 시스템 (Total Calibration System)

모든 사용자의 앉은키와 카메라 위치가 다르다는 점에 착안하여, **'상대 좌표 기반 영점 조절 알고리즘'**을 독자 개발했습니다.

* **작동 원리**: 사용자가 `s` 키를 누르는 순간의 3D 좌표(Pitch, Yaw, Iris X/Y)를 기준점(Zero Point)으로 잡고, 이후 발생하는 **변화량(Delta)**만을 측정하여 자세 불량 여부를 판별합니다.
* **효과**: 카메라가 측면에 있거나 사용자가 비스듬히 앉아 있어도 정확한 '정면 응시' 판정이 가능해졌습니다.

### 3.2. 4-Factor 가중치 평가 모델 (Weighted Scoring)

단순한 감정 인식을 넘어, 면접에 특화된 4가지 핵심 역량을 정의하고, 각 항목의 **원시 점수(Raw Score)**에 **가중치(Weight)**를 곱하여 최종 점수를 산출하는 투명한 평가 로직을 구축했습니다.

#### 📊 평가 산출 공식 (Scoring Formula)

최종 점수는 다음 4가지 요소의 합산으로 결정됩니다.

**`Final Score` = `(Confidence × 0.3)` + `(Focus × 0.3)` + `(Posture × 0.2)` + `(Stability × 0.2)`**

| 평가 항목                          | 산출 방법 (Calculation Method)                                                                                                                                               | 비중          |
| :--------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------ |
| **1. 자신감 (Confidence)**   | **`Average(mouthSmile + cheekSquint) * 100`**`<br>` - 답변 전체 시간 동안 밝은 표정(미소)을 유지한 평균 수치입니다. 억지 웃음보다는 자연스러운 미소를 감지합니다.  | **30%** |
| **2. 시선 집중 (Focus)**     | **`(Center_Gaze_Frames / Total_Frames) * 100`**`<br>` - 영점(Calibration Point) 기준, 시선 벡터가 허용 오차(X:±0.08, Y:±0.08) 내에 머문 시간의 비율입니다.       | **30%** |
| **3. 자세 안정 (Posture)**   | **`(Stable_Pose_Frames / Total_Frames) * 100`**`<br>` - 고개 숙임(Pitch), 비틀림(Roll) 등의 변화량이 임계값(0.008) 미만으로, 바른 자세를 유지한 시간의 비율입니다. | **20%** |
| **4. 정서 안정 (Stability)** | **`100 - (Average(browDown + mouthFrown) * 100)`**`<br>` - 미간 찌푸림이나 입꼬리 처짐 등 '부정적 신호'의 발생 빈도를 100점에서 차감한 수치입니다. (감점 방식)     | **20%** |

> **설계 의도**: 시선과 자신감은 '비언어적 설득력'의 핵심이므로 가장 높은 배점(60%)을 부여하였으며, 자세와 정서는 '기본 태도' 점수로 설정했습니다.

### 3.3. 긴장 타임라인 추적 (Anxiety Timeline)

면접 전체의 평균뿐만 아니라, **순간적인 긴장 피크(Event)**를 잡아냅니다.

* `browDown`(미간 찌푸림) 수치가 0.4를 초과하는 순간을 '긴장 이벤트'로 규정하고, 해당 시점(Time-stamp)을 리스트업하여 면접 후 **상세 피드백(Detailed Feedback)**을 돕습니다.

> **구현 주안점**: 본 시스템은 웹 서비스의 **'면접 진행(Live Session)' 페이지**에 탑재될 핵심 모듈로, 앞단의 '본인 인증(DeepFace)' 프로세스와는 독립적으로 작동하도록 설계되었습니다.

---

## 4. 🧪 테스트 및 검증 과정 (Verification)

### 4.1. 감정 인식 정밀도 검증 (`CV_EMOTION_VERIFIER.py`)

초기 모델이 미세한 슬픔이나 당황함을 잘 잡아내지 못하는 문제를 발견하여, 별도의 검증 모듈을 제작해 튜닝했습니다.

* **문제점**: 무표정일 때도 '슬픔'으로 오인되거나, 진짜 슬픈 표정을 잡아내지 못함.
* **해결책**: `mouthFrown` 값에 **1.8배 가중치**를 부여하고, 판정 문턱값(Threshold)을 **0.12**로 미세 조정하는 '밸런스 튜닝'을 통해 가만히 있을 때는 '평온', 표정을 지을 때만 '감정'이 뜨도록 최적화했습니다.

### 4.2. UI/UX 필드 테스트

* **시인성 개선**: 면접자가 화면 속 자신의 얼굴을 보며 피드백을 받을 수 있도록, 글자 크기와 색상(긍정-초록, 부정-빨강)을 직관적으로 설계했습니다.
* **영점 편의성**: 키보드 단축키(`s`, `q`)를 도입하여 마우스 조작 없이 즉각적인 제어가 가능하도록 구현했습니다.

---

## 5. 📊 최종 결과물 및 산출물 (Outcomes)

### 5.1. 실시간 분석 대시보드

면접 진행 중 화면 좌측에 실시간으로 다음 정보를 제공합니다.

> `👀 시선: 정면 응시` | `👤 자세: ✅ 안정` | `✨ 긍정 지수: 85%`

### 5.2. 종합 분석 리포트 (JSON & Terminal)

면접 종료 시, 정량적 데이터가 포함된 성적표가 자동 생성됩니다.

```json
{
    "score": 61.6,
    "metrics": {
        "confidence": 27.4,
        "anxiety": 20.5,
        "focus": 92.7,
        "steadiness": 48.4
    },
    "comment": "답변 시 시선이 분산되는 경향이 있으니 카메라를 더 응시해 주세요."
}
```

---

## 6. 🏁 결론 및 활용 계획 (Conclusion)

본 프로젝트를 통해 개발된 **AI 면접 분석 시스템(V4.5)**은 고비용의 전용 장비 없이 웹캠 하나만으로도 **전문가 수준의 비언어적 행동 분석**이 가능함을 입증했습니다.

향후 이 모듈은 다음과 같이 활용될 예정입니다.

1. **모의 면접 트레이닝**: 취업 준비생이 스스로 영상을 보며 시선과 자세를 교정.
2. **실전 면접 보조 도구**: 면접관에게 지원자의 '긴장도'나 '진실성'을 참고할 수 있는 보조 지표 제공.
3. **데이터베이스화**: 면접 데이터를 누적하여 '합격하는 관상(태도)'에 대한 AI 모델링 고도화.

---

## 7. 🔮 향후 확장 로드맵 (Future Roadmap)

본 프로젝트는 확장성이 뛰어난 **모듈형 아키텍처**로 설계되어 있어, 다음과 같은 멀티모달(Multimodal) 확장 및 플랫폼 이식이 용이합니다.

### 7.1. 🎙️ 음성 분석 통합 (Audio Integration)

현재의 비주얼(Visual) 분석에 **음성(Audio)** 분석을 결합하여 평가의 정확도를 획기적으로 높일 수 있습니다.

* **통합 모델**: `OpenAI Whisper` (STT) + `PyAudioAnalysis` (음성 특징 추출)
* **분석 시나리오**:
  1. **내용 분석**: 답변 내용의 키워드 적합성 및 논리력 평가.
  2. **비언어적 음성**: 목소리 톤(Tone)의 떨림, 말의 빠르기, '음/어' 등의 군더더기(Filler words) 감지.
  3. **Cross-Check**: "표정은 웃고 있지만(Positive) 목소리는 떨리는(Anxious)" 복합적인 심리 상태 포착 가능.

### 7.2. 🌐 웹 플랫폼 이식 (Web Scalability)

본 파이썬 프로토타입은 **웹 환경(Web Browser)**으로 즉시 이식할 수 있는 호환성을 가집니다.

* **기술 스택**: `React` / `Next.js` + `MediaPipe JS` (TensorFlow.js)
* **이점**:
  - 사용자가 별도의 프로그램 설치 없이 **크롬 브라우저** 접속만으로 면접 가능.
  - MediaPipe는 클라이언트(브라우저)에서 구동되므로, **서버 비용(GPU)이 거의 발생하지 않음** (Edge Computing).
  - 수백 명의 동시 접속자가 있어도 서버 부하 없이 서비스 운영 가능.

> *"이 시스템은 단순한 감시자가 아니라, 면접자의 숨겨진 자신감을 찾아주는 **페이스 메이커(Pacemaker)** 역할을 수행할 것입니다."*
