"""
EXAONE-3.5-7.8B-Instruct μμ LLM μ—”μ§„ λ¨λ“ (GGUF λ²„μ „)
ν”„λ΅¬ν”„νΈλ‚ λΉ„μ¦λ‹μ¤ λ΅μ§ μ—†μ΄, λ¨λΈ λ΅λ”© λ° ν…μ¤νΈ μƒμ„± κΈ°λ¥λ§ μ κ³µν•©λ‹λ‹¤.
"""
import os
import logging
# from llama_cpp import Llama (Moved inside ExaoneLLM.__init__)

logger = logging.getLogger("EXAONE-ENGINE")

# λ¨λΈ κ²½λ΅ (μ»¨ν…μ΄λ„ λ‚΄λ¶€ κ²½λ΅)
MODEL_PATH = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

from typing import Any, List, Optional, ClassVar
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
# from llama_cpp import Llama (Moved inside ExaoneLLM.__init__)

class ExaoneLLM(LLM):
    """
    EXAONE-3.5-7.8B-Instruct (GGUF) μ‹±κΈ€ν†¤ LLM μ—”μ§„
    LangChain LLM μΈν„°νμ΄μ¤λ¥Ό μƒμ†λ°›μ•„ LCEL νΈν™μ„±μ„ μ κ³µν•©λ‹λ‹¤.
    """
    _instance: ClassVar[Optional["ExaoneLLM"]] = None
    llm: ClassVar[Any] = None
    _initialized: ClassVar[bool] = False
    
    def __new__(cls, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if hasattr(self, "_initialized") and self._initialized:
            return
        
        # CPU ν™κ²½μ—μ„λ„ GGUFλ” μ‹¤ν–‰ κ°€λ¥ν•λ―€λ΅ λ΅λ”© ν—μ©
        use_gpu = os.getenv("USE_GPU", "true").lower() == "true"
        gpu_layers = int(os.getenv("N_GPU_LAYERS", "-1"))
        
        if not use_gpu:
            logger.info("β„ΉοΈ CPU mode detected. Loading EXAONE on CPU (this may be slow).")
            gpu_layers = 0 # GPU μ‚¬μ© μ•ν•¨ κ°•μ  μ„¤μ •
            
        logger.info(f"π€ Loading EXAONE Engine from: {MODEL_PATH}")
        
        if not os.path.exists(MODEL_PATH):
            local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
            if os.path.exists(local_path):
                target_path = local_path
            else:
                 raise FileNotFoundError(f"λ¨λΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {MODEL_PATH} (Local fallback also failed: {local_path})")
        else:
            target_path = MODEL_PATH

        try:
            # π¨ CPU ν™κ²½μ—μ„ CUDA λΉλ“λ llama-cpp λ΅λ”© μ‹ λ°μƒν•λ” ν¬λμ‹ λ°©μ§€λ¥Ό μ„ν•΄ μ§€μ—° μ„ν¬νΈ
            from llama_cpp import Llama
            
            # ν΄λμ¤ λ³€μλ΅ llm κ°μ²΄ κ΄€λ¦¬ (μ‹±κΈ€ν†¤)
            ExaoneLLM.llm = Llama(
                model_path=target_path,
                n_gpu_layers=gpu_layers,
                n_ctx=4096,
                n_batch=512,
                verbose=False
            )
            logger.info(f"β… EXAONE Engine Loaded (n_gpu_layers: {gpu_layers})")
        except Exception as e:
            logger.error(f"β μ—”μ§„ λ΅λ“ μ‹¤ν¨: {e}")
            raise e
        
        ExaoneLLM._initialized = True

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """LLM μ‹¤ν–‰ ν•µμ‹¬ λ©”μ„λ“ (LangChain ν‘μ¤€)"""
        if ExaoneLLM.llm is None:
            logger.error("β EXAONE λ¨λΈμ΄ λ΅λ“λμ§€ μ•μ•μµλ‹λ‹¤. (CPU λ¨λ“μ΄κ±°λ‚ λ΅λ”© μ‹¤ν¨)")
            raise RuntimeError("EXAONE engine is not initialized. Check if this is a GPU worker.")

        try:
            # stop μ‹ν€€μ¤ κΈ°λ³Έκ°’ μ„¤μ •
            stop_sequences = ["[|endofturn|]", "[|user|]"] if stop is None else stop
            
            output = ExaoneLLM.llm(
                prompt,
                max_tokens=kwargs.get("max_tokens", 512),
                stop=stop_sequences,
                temperature=kwargs.get("temperature", 0.7),
                echo=False
            )
            return output['choices'][0]['text'].strip()
        except Exception as e:
            logger.error(f"μƒμ„± λ„μ¤‘ μ¤λ¥ λ°μƒ: {e}")
            return ""

    @property
    def _llm_type(self) -> str:
        return "exaone_gguf"

    def _create_prompt(self, system_msg: str, user_msg: str) -> str:
        """EXAONE 3.5 μ „μ© Chat Template ν¬λ§·ν… (ν•μ„ νΈν™μ„± μ μ§€)"""
        return f"[|system|]{system_msg}[|endofturn|]\n[|user|]{user_msg}[|endofturn|]\n[|assistant|]"


def get_exaone_llm() -> ExaoneLLM:
    """μ—”μ§„ μ‹±κΈ€ν†¤ μΈμ¤ν„΄μ¤ λ°ν™"""
    return ExaoneLLM()