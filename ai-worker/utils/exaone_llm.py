"""
EXAONE-3.5-7.8B-Instruct í†µí•© LLM ëª¨ë“ˆ (GGUF ë²„ì „)
ì§ˆë¬¸ ìƒì„± ë° ë‹µë³€ í‰ê°€ë¥¼ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ì²˜ë¦¬
Llama.cpp ì—”ì§„ ì‚¬ìš©ìœ¼ë¡œ CPU/GPU íš¨ìœ¨ì  ì‹¤í–‰
"""
import os
import logging
import json
import re
from typing import Optional, Dict, List
from llama_cpp import Llama

logger = logging.getLogger("EXAONE-LLM")

# ëª¨ë¸ ê²½ë¡œ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
MODEL_PATH = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

class ExaoneLLM:
    """
    EXAONE-3.5-7.8B-Instruct (GGUF) ì‹±ê¸€í†¤ LLM
    
    Attributes:
        llm (Llama): Llama.cpp ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        _initialized (bool): ì´ˆê¸°í™” ì—¬ë¶€
    
    ìƒì„±ì: ejm
    ìƒì„±ì¼ì: 2026-02-04
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        logger.info(f"ğŸš€ Loading EXAONE GGUF Model from: {MODEL_PATH}")
        
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")

        # Llama.cpp ëª¨ë¸ ë¡œë“œ
        try:
            self.llm = Llama(
                model_path=MODEL_PATH,
                n_gpu_layers=-1,      # ê°€ëŠ¥í•œ ëª¨ë“  ë ˆì´ì–´ë¥¼ GPUë¡œ ì˜¤í”„ë¡œë“œ
                n_ctx=4096,           # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°
                n_batch=512,          # ë°°ì¹˜ í¬ê¸°
                verbose=False          # ë¡œë”© ë¡œê·¸ ì¶œë ¥
            )
            logger.info("âœ… EXAONE GGUF Model Initialized")
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e
        
        self._initialized = True
    
    def _create_prompt(self, system_msg: str, user_msg: str) -> str:
        """EXAONE 3.5 í”„ë¡¬í”„íŠ¸ í¬ë§· ì ìš©"""
        return f"[|system|]{system_msg}[|endofturn|]\n[|user|]{user_msg}[|endofturn|]\n[|assistant|]"

    def generate_questions(
        self,
        position: str,
        context: str = "",
        examples: List[str] = None,
        count: int = 5
    ) -> List[str]:
        """ë©´ì ‘ ì§ˆë¬¸ ìƒì„±
        
        Args:
            position (str): ì§ë¬´ í¬ì§€ì…˜
            context (str, optional): ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸. Defaults to "".
            examples (List[str], optional): ì˜ˆì‹œ ì§ˆë¬¸. Defaults to None.
            count (int, optional): ìƒì„±í•  ì§ˆë¬¸ ìˆ˜. Defaults to 5.
            
        Returns:
            List[str]: ìƒì„±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        
        ìƒì„±ì: ejm
        ìƒì„±ì¼ì: 2026-02-04
        """
        # Few-shot ì˜ˆì‹œ
        if examples:
            few_shot = "\n".join([f"- {q}" for q in examples[:3]])
        else:
            few_shot = "- Reactì˜ Virtual DOMì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n- HTTPì™€ HTTPSì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\n- ë³¸ì¸ì´ ê²½í—˜í•œ ê°€ì¥ í° ê¸°ìˆ ì  ë¬¸ì œëŠ” ë¬´ì—‡ì´ì—ˆë‚˜ìš”?"
        
        context_str = f"\n\nì¶”ê°€ ì»¨í…ìŠ¤íŠ¸:\n{context}" if context else ""
        
        system_msg = "ë‹¹ì‹ ì€ í•œêµ­ ê¸°ì—…ì˜ ë©´ì ‘ê´€ì´ì ì±„ìš© ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ì¤‘í•˜ê³  í•µì‹¬ì ì¸ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."
        user_msg = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {position} ì§ë¬´ ë©´ì ‘ ì§ˆë¬¸ {count}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

{context_str}

ê¸°ì¡´ ì§ˆë¬¸ ì˜ˆì‹œ:
{few_shot}

[ìš”êµ¬ì‚¬í•­]
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
2. ë²ˆí˜¸ë‚˜ ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì—†ì´ ì§ˆë¬¸ ë‚´ìš©ë§Œ í•œ ì¤„ì”© ì‘ì„±í•˜ì„¸ìš”.
3. ê¸°ìˆ ì ì¸ ê¹Šì´ê°€ ìˆëŠ” ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.
4. ì´ {count}ê°œì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

ìƒì„±ëœ ì§ˆë¬¸:"""

        prompt = self._create_prompt(system_msg, user_msg)
        
        try:
            output = self.llm(
                prompt,
                max_tokens=1024,
                stop=["[|endofturn|]", "[|user|]", "ìƒì„±ëœ ì§ˆë¬¸:"],
                temperature=0.7,
                top_p=0.9,
                echo=False
            )
            
            response_text = output['choices'][0]['text']
            
            # í›„ì²˜ë¦¬: ì¤„ë³„ ë¶„ë¦¬ ë° ì •ì œ
            questions = []
            for line in response_text.split('\n'):
                line = line.strip()
                if not line: continue
                
                # ë²ˆí˜¸ ì œê±° (1. ì§ˆë¬¸ -> ì§ˆë¬¸)
                line = re.sub(r'^\d+[\.\)]\s*', '', line)
                line = line.strip('"\'')
                
                if len(line) > 10 and '?' in line: # ìµœì†Œ ê¸¸ì´ ë° ì§ˆë¬¸ í˜•íƒœ í™•ì¸
                    questions.append(line)
            
            # ë¶€ì¡±í•˜ë©´ fallback
            if len(questions) < count:
                logger.warning(f"ìƒì„±ëœ ì§ˆë¬¸ì´ ë¶€ì¡±í•¨ ({len(questions)}/{count}). Fallback ì¶”ê°€.")
                questions.extend(self._get_fallback_questions(position, count - len(questions)))
            
            return questions[:count]
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return self._get_fallback_questions(position, count)

    def generate_human_like_question(
        self,
        name: str,
        stage: str,
        guide: str,
        context_list: List[Dict]
    ) -> str:
        """ë² í…Œë‘ ë©´ì ‘ê´€ ìŠ¤íƒ€ì¼ì˜ ë‹¨ì¼ ì§ˆë¬¸ ìƒì„± (RAG ê¸°ë°˜)
        
        Args:
            name (str): ì§€ì›ì ì´ë¦„
            stage (str): ë©´ì ‘ ë‹¨ê³„ (ì˜ˆ: ì§ë¬´ ì§€ì‹ í‰ê°€)
            guide (str): í‰ê°€ ì˜ë„/ê°€ì´ë“œ
            context_list (List[Dict]): RAG ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [{'text': ...}, ...]
            
        Returns:
            str: ìƒì„±ëœ ì§ˆë¬¸
        """
        if not context_list:
            return f"{name}ë‹˜, í•´ë‹¹ ì§ë¬´ì— ì§€ì›í•˜ê²Œ ëœ ê³„ê¸°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?"

        # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ë³€í™˜
        texts = [item['text'] for item in context_list] if isinstance(context_list[0], dict) else context_list
        context_text = "\n".join([f"- {txt}" for txt in texts])

        prompt_template = """[|system|]
ë„ˆëŠ” 15ë…„ ì°¨ ë² í…Œë‘ 'ë³´ì•ˆ ì§ë¬´ ë©´ì ‘ê´€'ì´ë‹¤. 
ì§€ê¸ˆì€ **ë©´ì ‘ì´ í•œì°½ ì§„í–‰ ì¤‘ì¸ ìƒí™©**ì´ë‹¤. (ìê¸°ì†Œê°œëŠ” ì´ë¯¸ ëë‚¬ë‹¤.)
ì œê³µëœ [ì´ë ¥ì„œ ë‚´ìš©]ì„ ê·¼ê±°ë¡œ, í•´ë‹¹ ë‹¨ê³„({stage})ì— ë§ëŠ” **ë‚ ì¹´ë¡œìš´ ì§ˆë¬¸ 1ê°œ**ë§Œ ë˜ì ¸ë¼.

[ì‘ì„± ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­] 
1. **"ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤" ì ˆëŒ€ ê¸ˆì§€.** (ì´ë¯¸ í–ˆë‹¤ê³  ê°€ì •)
2. **"(ì ì‹œ ì¹¨ë¬µ)", "ë‹µë³€ ê°ì‚¬í•©ë‹ˆë‹¤"** ê°™ì€ ê°€ìƒì˜ ì§€ë¬¸ì´ë‚˜ ëŒ€ë³¸ì„ ì“°ì§€ ë§ˆë¼.
3. ì§ˆë¬¸ ì•ë’¤ì— ì‚¬ì¡±ì„ ë¶™ì´ì§€ ë§ê³  **ì§ˆë¬¸ë§Œ ê¹”ë”í•˜ê²Œ** ì¶œë ¥í•˜ë¼.

[ì§ˆë¬¸ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ]
1. ì‹œì‘ì€ ë°˜ë“œì‹œ **"{name}ë‹˜,"** ìœ¼ë¡œ ë¶€ë¥´ë©° ì‹œì‘í•  ê²ƒ.
2. **"ì´ë ¥ì„œë¥¼ ë³´ë‹ˆ...", "ìì†Œì„œë¥¼ ì½ì–´ë³´ë‹ˆ ~ë¼ê³  í•˜ì…¨ëŠ”ë°..."** ì²˜ëŸ¼ ê·¼ê±°ë¥¼ ëª…í™•íˆ ëŒˆ ê²ƒ.
3. ë§íˆ¬ëŠ” ì •ì¤‘í•˜ë©´ì„œë„ ì˜ˆë¦¬í•œ ë©´ì ‘ê´€ í†¤(..í•˜ì…¨ëŠ”ë°, ..ì„¤ëª…í•´ ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?)ì„ ìœ ì§€í•  ê²ƒ.
[|endofturn|]
[|user|]
# í‰ê°€ ë‹¨ê³„: {stage}
# í‰ê°€ ì˜ë„: {guide}
# ì§€ì›ì ì´ë ¥ì„œ ê·¼ê±°:
{context}

# ìš”ì²­:
ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ {name} ì§€ì›ìì—ê²Œ **ë‹¨ë„ì§ì…ì ìœ¼ë¡œ** ì§ˆë¬¸ì„ ë˜ì ¸ì¤˜.
[|endofturn|]
[|assistant|]"""

        prompt = prompt_template.format(
            name=name,
            stage=stage,
            guide=guide,
            context=context_text
        )

        try:
            output = self.llm(
                prompt,
                max_tokens=256,
                stop=["[|endofturn|]", "[|user|]"],
                temperature=0.7,
                top_p=0.9,
                echo=False
            )
            
            response_text = output['choices'][0]['text'].strip()
            # í˜¹ì‹œ ëª¨ë¥¼ ë”°ì˜´í‘œ ì œê±°
            response_text = response_text.strip('"\'')
            return response_text
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"{name}ë‹˜, ì¤€ë¹„í•˜ì‹  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì‹­ì‹œì˜¤."

    def evaluate_answer(
        self,
        question_text: str,
        answer_text: str,
        rubric: Optional[Dict] = None
    ) -> Dict:
        """ë‹µë³€ í‰ê°€
        
        Args:
            question_text (str): í‰ê°€í•  ì§ˆë¬¸ í…ìŠ¤íŠ¸
            answer_text (str): í‰ê°€í•  ë‹µë³€ í…ìŠ¤íŠ¸
            rubric (Optional[Dict], optional): í‰ê°€ ê¸°ì¤€. Defaults to None.
            
        Returns:
            Dict: í‰ê°€ ê²°ê³¼
        
        ìƒì„±ì: ejm
        ìƒì„±ì¼ì: 2026-02-04
        """
        if not answer_text or not answer_text.strip():
            return {"technical_score": 0, "communication_score": 0, "feedback": "ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤."}

        system_msg = "ë‹¹ì‹ ì€ ê³µì •í•˜ê³  ì—„ê²©í•œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”."
        user_msg = f"""ë‹¤ìŒ ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”.

ì§ˆë¬¸: {question_text}
ë‹µë³€: {answer_text}

í‰ê°€ ê¸°ì¤€:
1. ê¸°ìˆ ì  ì •í™•ì„± (1-5ì )
2. ì˜ì‚¬ì†Œí†µ ëŠ¥ë ¥ (1-5ì )
3. êµ¬ì²´ì ì¸ í”¼ë“œë°± (í•œêµ­ì–´)

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "technical_score": 3,
    "communication_score": 3,
    "feedback": "í”¼ë“œë°± ë‚´ìš©"
}}"""

        prompt = self._create_prompt(system_msg, user_msg)
        
        try:
            output = self.llm(
                prompt,
                max_tokens=512,
                stop=["[|endofturn|]"],
                temperature=0.1, # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ìŒ
                echo=False
            )
            
            response_text = output['choices'][0]['text']
            
            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                    return {
                        "technical_score": int(result.get("technical_score", 3)),
                        "communication_score": int(result.get("communication_score", 3)),
                        "feedback": result.get("feedback", "í‰ê°€ ì™„ë£Œ")
                    }
                except:
                    pass
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨. Raw response: {response_text}")
            return {"technical_score": 3, "communication_score": 3, "feedback": "í‰ê°€ ê²°ê³¼ë¥¼ ì‚°ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (í˜•ì‹ ì˜¤ë¥˜)"}

        except Exception as e:
            logger.error(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"technical_score": 3, "communication_score": 3, "feedback": "í‰ê°€ ì¤‘ ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë°œìƒ"}

    def _get_fallback_questions(self, position: str, count: int) -> List[str]:
        """ê¸°ë³¸ ì§ˆë¬¸ ìƒì„±
        
        Args:
            position (str): ì§ë¬´ í¬ì§€ì…˜
            count (int): ìƒì„±í•  ì§ˆë¬¸ ìˆ˜
            
        Returns:
            List[str]: ìƒì„±ëœ ê¸°ë³¸ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        
        ìƒì„±ì: ejm
        ìƒì„±ì¼ì: 2026-02-07
        """
        base_qs = [
            f"{position} ì§ë¬´ì— ì§€ì›í•˜ê²Œ ëœ êµ¬ì²´ì ì¸ ë™ê¸°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ë³¸ì¸ì˜ ê°€ì¥ í° ê°•ì ê³¼ ì•½ì ì€ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ë‚˜ìš”?",
            "ì…ì‚¬ í›„ 3ë…„, 5ë…„, 10ë…„ í›„ì˜ ì»¤ë¦¬ì–´ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë™ë£Œì™€ ì˜ê²¬ ì¶©ëŒì´ ë°œìƒí–ˆì„ ë•Œ ì–´ë–»ê²Œ ëŒ€ì²˜í•˜ì‹œë‚˜ìš”?",
            "ìµœê·¼ ê´€ì‹¬ ìˆê²Œ ë³´ê³  ìˆëŠ” ê¸°ìˆ  íŠ¸ë Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        return base_qs[:count]

def get_exaone_llm() -> ExaoneLLM:
    """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Returns:
        ExaoneLLM: ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
    
    ìƒì„±ì: ejm
    ìƒì„±ì¼ì: 2026-02-07
    """
    return ExaoneLLM()

# Warmup
try:
    if os.path.exists(MODEL_PATH):
        logger.info("ğŸ”¥ GGUF Model Warmup...")
        _ = get_exaone_llm()
except Exception as e:
    logger.warning(f"Warmup skipped: {e}")
