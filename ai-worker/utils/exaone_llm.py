"""
EXAONE-3.5-7.8B-Instruct μμ LLM μ—”μ§„ λ¨λ“ (GGUF λ²„μ „)
ν”„λ΅¬ν”„νΈλ‚ λΉ„μ¦λ‹μ¤ λ΅μ§ μ—†μ΄, λ¨λΈ λ΅λ”© λ° ν…μ¤νΈ μƒμ„± κΈ°λ¥λ§ μ κ³µν•©λ‹λ‹¤.
"""
import os
import logging
from llama_cpp import Llama

logger = logging.getLogger("EXAONE-ENGINE")

# λ¨λΈ κ²½λ΅ (μ»¨ν…μ΄λ„ λ‚΄λ¶€ κ²½λ΅)
MODEL_PATH = "/app/models/EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"

from typing import Any, List, Optional
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from llama_cpp import Llama

class ExaoneLLM(LLM):
    """
    EXAONE-3.5-7.8B-Instruct (GGUF) μ‹±κΈ€ν†¤ LLM μ—”μ§„
    LangChain LLM μΈν„°νμ΄μ¤λ¥Ό μƒμ†λ°›μ•„ LCEL νΈν™μ„±μ„ μ κ³µν•©λ‹λ‹¤.
    """
    _instance = None
    llm: Any = None
    _initialized: bool = False
    
    def __new__(cls, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if hasattr(self, "_initialized") and self._initialized:
            return
            
        logger.info(f"π€ Loading EXAONE Engine from: {MODEL_PATH}")
        
        if not os.path.exists(MODEL_PATH):
            local_path = r"C:\big20\Big20_aI_interview_project\ai-worker\models\EXAONE-3.5-7.8B-Instruct-Q4_K_M.gguf"
            target_path = local_path if os.path.exists(local_path) else MODEL_PATH
            if not os.path.exists(target_path):
                 raise FileNotFoundError(f"λ¨λΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {target_path}")
        else:
            target_path = MODEL_PATH

        try:
            gpu_layers = int(os.getenv("N_GPU_LAYERS", "-1"))
            # ν΄λμ¤ λ³€μλ΅ llm κ°μ²΄ κ΄€λ¦¬ (μ‹±κΈ€ν†¤)
            ExaoneLLM.llm = Llama(
                model_path=target_path,
                n_gpu_layers=gpu_layers,
                n_ctx=4096,
                n_batch=512,
                verbose=False
            )
            logger.info(f"β… EXAONE Engine Loaded (n_gpu_layers: {gpu_layers})")
        except Exception as e:
            logger.error(f"β μ—”μ§„ λ΅λ“ μ‹¤ν¨: {e}")
            raise e
        
        self._initialized = True

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """LLM μ‹¤ν–‰ ν•µμ‹¬ λ©”μ„λ“ (LangChain ν‘μ¤€)"""
        try:
            # stop μ‹ν€€μ¤ κΈ°λ³Έκ°’ μ„¤μ •
            stop_sequences = ["[|endofturn|]", "[|user|]"] if stop is None else stop
            
            output = ExaoneLLM.llm(
                prompt,
                max_tokens=kwargs.get("max_tokens", 512),
                stop=stop_sequences,
                temperature=kwargs.get("temperature", 0.7),
                echo=False
            )
            return output['choices'][0]['text'].strip()
        except Exception as e:
            logger.error(f"μƒμ„± λ„μ¤‘ μ¤λ¥ λ°μƒ: {e}")
            return ""

    @property
    def _llm_type(self) -> str:
        return "exaone_gguf"

    def _create_prompt(self, system_msg: str, user_msg: str) -> str:
        """EXAONE 3.5 μ „μ© Chat Template ν¬λ§·ν… (ν•μ„ νΈν™μ„± μ μ§€)"""
        return f"[|system|]{system_msg}[|endofturn|]\n[|user|]{user_msg}[|endofturn|]\n[|assistant|]"

    def invoke(self, prompt: str, **kwargs: Any) -> str:
        """κΈ°μ΅΄ invoke μΈν„°νμ΄μ¤ μ μ§€ (ν•μ„ νΈν™μ©)"""
        return self._call(prompt, **kwargs)

def get_exaone_llm() -> ExaoneLLM:
    """μ—”μ§„ μ‹±κΈ€ν†¤ μΈμ¤ν„΄μ¤ λ°ν™"""
    return ExaoneLLM()
