"""
ë²¡í„° ì„ë² ë”© ìƒì„± ìœ í‹¸ë¦¬í‹°
Question ë° AnswerBankì˜ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
"""
from sentence_transformers import SentenceTransformer
from typing import List
import os
import logging

logger = logging.getLogger("VectorUtils")

# í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (KURE-v1)
# ì§ˆë¬¸/ë‹µë³€/ë¬¸ì„œ ë“± ìš©ë„ì— ë”°ë¼ prefix í•„ìš” ("query: ", "passage: ")
MODEL_NAME = "nlpai-lab/KURE-v1"

class EmbeddingGenerator:
    """ì‹±ê¸€í†¤ íŒ¨í„´ì˜ ì„ë² ë”© ìƒì„±ê¸°"""
    _instance = None
    _model = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._model is None:
            cache_dir = "/app/models/embeddings" if os.path.exists("/app/models") else "./models/embeddings"
            os.makedirs(cache_dir, exist_ok=True)
            
            logger.info(f"Loading embedding model: {MODEL_NAME}")
            logger.info(f"ğŸ“‚ Cache path: {cache_dir}")
            
            # trust_remote_code=True ê¶Œì¥ (KURE ëª¨ë¸)
            self._model = SentenceTransformer(
                MODEL_NAME, 
                trust_remote_code=True,
                cache_folder=cache_dir
            )
            logger.info("âœ… Embedding model loaded")
    
    def encode(self, text: str, is_query: bool = True) -> List[float]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ë³€í™˜í•  í…ìŠ¤íŠ¸
            is_query: ì¿¼ë¦¬(ì§ˆë¬¸/ê²€ìƒ‰ì–´) ì—¬ë¶€. Trueë©´ "query: ", Falseë©´ "passage: " ì ‘ë‘ì–´ ì‚¬ìš©
        
        Returns:
            ë²¡í„° (ë¦¬ìŠ¤íŠ¸)
        """
        if not text or len(text.strip()) == 0:
            logger.warning("Empty text provided for encoding")
            # KURE-v1 output dimension typically 1024.
            return [0.0] * 1024
        
        prefix = "query: " if is_query else "passage: "
        # í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ì— ì ‘ë‘ì–´ ì¶”ê°€
        full_text = prefix + text
        
        embedding = self._model.encode(full_text, convert_to_tensor=False)
        return embedding.tolist()
    
    def encode_passage(self, text: str) -> List[float]:
        """ë¬¸ì„œ(Passage) ì„ë² ë”© ìƒì„± ("passage: " ì ‘ë‘ì–´ ì‚¬ìš©)"""
        return self.encode(text, is_query=False)

    def encode_query(self, text: str) -> List[float]:
        """ì§ˆë¬¸(Query) ì„ë² ë”© ìƒì„± ("query: " ì ‘ë‘ì–´ ì‚¬ìš©)"""
        return self.encode(text, is_query=True)
    
    def encode_batch(self, texts: List[str], is_query: bool = True) -> List[List[float]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ë²¡í„°í™” (ë°°ì¹˜ ì²˜ë¦¬)
        """
        if not texts:
            return []
        
        prefix = "query: " if is_query else "passage: "
        prefixed_texts = [prefix + t for t in texts]
        
        embeddings = self._model.encode(prefixed_texts, convert_to_tensor=False, show_progress_bar=True)
        return [emb.tolist() for emb in embeddings]


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_generator = None

def get_embedding_generator() -> EmbeddingGenerator:
    """ì„ë² ë”© ìƒì„±ê¸° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _generator
    if _generator is None:
        _generator = EmbeddingGenerator()
    return _generator


def generate_question_embedding(question_text: str) -> List[float]:
    """ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (Query ëª¨ë“œ)"""
    generator = get_embedding_generator()
    return generator.encode_query(question_text)


def generate_answer_embedding(answer_text: str) -> List[float]:
    """ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (Passage ëª¨ë“œ)"""
    generator = get_embedding_generator()
    # ë‹µë³€ì€ ê²€ìƒ‰ ëŒ€ìƒì´ë¯€ë¡œ Passageë¡œ ì·¨ê¸‰
    return generator.encode_passage(answer_text)
