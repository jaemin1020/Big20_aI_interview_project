import cv2  # OpenCV: ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì¹´ë©”ë¼ ì œì–´ë¥¼ ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import mediapipe as mp  # MediaPipe: êµ¬ê¸€ì˜ AI í”„ë ˆì„ì›Œí¬ (ì–¼êµ´, ì† ì¸ì‹ ë“±)
from mediapipe.tasks import python  # MediaPipeì˜ íŒŒì´ì¬ íƒœìŠ¤í¬ ëª¨ë“ˆ
from mediapipe.tasks.python import vision  # ë¹„ì „(Vision) ê´€ë ¨ íƒœìŠ¤í¬ (FaceLandmarker ë“±)
import numpy as np  # NumPy: ìˆ˜ì¹˜ ê³„ì‚° ë° í–‰ë ¬ ì—°ì‚°ìš©
import time  # ì‹œê°„ ì¸¡ì • (FPS ê³„ì‚° ë° íƒ€ì„ë¼ì¸ ê¸°ë¡ìš©)
import os  # ìš´ì˜ì²´ì œ ê²½ë¡œ ë° íŒŒì¼ ì œì–´ìš©
import json  # ê²°ê³¼ ë¦¬í¬íŠ¸ ì €ì¥ì„ ìœ„í•œ JSON ë¼ì´ë¸ŒëŸ¬ë¦¬
from PIL import Image, ImageDraw, ImageFont  # Pillow: í•œê¸€ í…ìŠ¤íŠ¸ ë Œë”ë§ì„ ìœ„í•œ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

# ==========================================
# ğŸ“ AI Interviewer Vision System
# Version: V4.5 (Final Release)
# Date: 2026-02-05
# Description: ì‹¤ì‹œê°„ ì–¼êµ´ ëœë“œë§ˆí¬ ë¶„ì„ ê¸°ë°˜ AI ë©´ì ‘ ì½”ì¹­ ì‹œìŠ¤í…œ
# ==========================================

# ==========================================
# [Step 1] ì‚¬ìš©ì ì„¤ì • ë° ìƒìˆ˜ ì •ì˜ (Configuration)
# ==========================================
# 1-1. ì‹œì„  ë° ìì„¸ íŒì • ì„ê³„ê°’ (ë¯¼ê°ë„ ì„¤ì •)
GAZE_TOLERANCE_X = 0.08   # ì‹œì„ ì´ ì¢Œìš°ë¡œ ì´ë§Œí¼ ë²—ì–´ë‚˜ë©´ 'ì´íƒˆ'ë¡œ ê°„ì£¼
GAZE_TOLERANCE_Y = 0.08   # ì‹œì„ ì´ ìƒí•˜ë¡œ ì´ë§Œí¼ ë²—ì–´ë‚˜ë©´ 'ì´íƒˆ'ë¡œ ê°„ì£¼
HEAD_SENSITIVITY = 0.008  # ê³ ê°œ ìˆ™ì„/ë“¤ë¦¼ ë³€í™”ëŸ‰ ë¯¼ê°ë„ (ì‘ì„ìˆ˜ë¡ ì˜ˆë¯¼í•¨)

# 1-2. ìµœì¢… ë¦¬í¬íŠ¸ ì ìˆ˜ ë°°ì  (ì´í•© 1.0)
WEIGHT_CONFIDENCE = 0.3   # ìì‹ ê°(ë¯¸ì†Œ) ë¹„ì¤‘: 30%
WEIGHT_FOCUS      = 0.3   # ì‹œì„  ì§‘ì¤‘ë„ ë¹„ì¤‘: 30%
WEIGHT_POSTURE    = 0.2   # ìì„¸ ì•ˆì •ì„± ë¹„ì¤‘: 20%
WEIGHT_EMOTION    = 0.2   # ì •ì„œ ì•ˆì •ì„± ë¹„ì¤‘: 20%

# 1-3. ì˜ì (Calibration) ì´ˆê¸°ê°’ ì„¤ì •
# ì‚¬ìš©ìê°€ 's'í‚¤ë¥¼ ëˆ„ë¥´ë©´ í˜„ì¬ ìì‹ ì˜ ìœ„ì¹˜ë¡œ ì´ ê°’ë“¤ì´ ê°±ì‹ ë©ë‹ˆë‹¤.
calibrated_gaze_x = 0.43   # ê¸°ì¤€ ëˆˆë™ì X ì¢Œí‘œ
calibrated_gaze_y = 0.36   # ê¸°ì¤€ ëˆˆë™ì Y ì¢Œí‘œ
calibrated_pitch = 0.05    # ê¸°ì¤€ ê³ ê°œ ê°ë„ (Pitch)
calibrated_eye_diff = 0.0  # ê¸°ì¤€ ëˆˆ ë†’ì´ ì°¨ì´ (ê¸°ìš¸ê¸°)
calibrated_tilt_diff = 0.0 # ê¸°ì¤€ ì–¼êµ´ ê¸°ìš¸ê¸°

# 1-4. ì„¸ì…˜ ë°ì´í„° ì €ì¥ì†Œ (Session Storage)
# ë©´ì ‘ ì§„í–‰ ë™ì•ˆ ë°œìƒí•˜ëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ëˆ„ì í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.
session_data = {
    "start_time": 0,           # ë©´ì ‘ ì‹œì‘ ì‹œê°„
    "total_frames": 0,         # ì²˜ë¦¬ëœ ì´ í”„ë ˆì„ ìˆ˜
    "smile_scores": [],        # ë§¤ í”„ë ˆì„ì˜ ë¯¸ì†Œ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
    "anxiety_scores": [],      # ë§¤ í”„ë ˆì„ì˜ ë¶ˆì•ˆ(ë¯¸ê°„) ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
    "gaze_center_frames": 0,   # ì‹œì„ ì´ ì •ë©´ì´ì—ˆë˜ í”„ë ˆì„ ìˆ˜
    "posture_stable_frames": 0,# ìì„¸ê°€ ì•ˆì •ì ì´ì—ˆë˜ í”„ë ˆì„ ìˆ˜
    "max_anxiety": 0.0,        # ë©´ì ‘ ì¤‘ ê¸°ë¡ëœ ìµœëŒ€ ê¸´ì¥ ìˆ˜ì¹˜
    "tension_events": []       # ê¸´ì¥ì´ ë°œìƒí•œ ì‹œì (ì´ˆ) ë¦¬ìŠ¤íŠ¸
}

# 1-5. AI ëª¨ë¸ ê²½ë¡œ ì§€ì •
# ë‹¤ìš´ë¡œë“œ ë°›ì€ MediaPipe Face Landmarker ëª¨ë¸ íŒŒì¼ì˜ ìœ„ì¹˜
model_path = 'face_landmarker.task'

# ==========================================
# [Step 2] ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ (Helper Functions)
# ==========================================

def get_korean_font(size):
    """
    í•œê¸€ í°íŠ¸ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    Windowsì˜ ê²½ìš° 'malgun.ttf'ë¥¼ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try: 
        return ImageFont.truetype("malgun.ttf", size)
    except: 
        return ImageFont.load_default()

# í°íŠ¸ ê°ì²´ ë¯¸ë¦¬ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
font_main = get_korean_font(24)  # ë©”ì¸ í…ìŠ¤íŠ¸ìš©
font_sub = get_korean_font(18)   # ì„œë¸Œ ì •ë³´ìš©
font_debug = get_korean_font(14) # ë””ë²„ê·¸ ì •ë³´ìš©

def draw_korean_text(img, text, position, color, font=font_main):
    """
    OpenCV ì´ë¯¸ì§€ ìœ„ì— í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë¦¬ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    OpenCVëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í•œê¸€ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Pillowë¡œ ë³€í™˜í•˜ì—¬ ê·¸ë¦½ë‹ˆë‹¤.
    """
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) # CV2(BGR) -> PIL(RGB) ë³€í™˜
    draw = ImageDraw.Draw(img_pil)
    draw.text(position, text, font=font, fill=color) # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR) # PIL(RGB) -> CV2(BGR) ì¬ë³€í™˜

# ==========================================
# [Step 3] ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜ (Report Generator)
# ==========================================
def generate_report():
    """
    ë©´ì ‘ ì¢…ë£Œ ì‹œ í˜¸ì¶œë˜ë©°, ëˆ„ì ëœ session_dataë¥¼ ë¶„ì„í•˜ì—¬ 
    í„°ë¯¸ë„ì— ì¶œë ¥í•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*50)
    print("ğŸ“ AI ë©´ì ‘ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸")
    print("="*50)
    
    total = session_data["total_frames"]
    if total == 0: return # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ

    # 3-1. í†µê³„ ë°ì´í„° ê³„ì‚° (Averages & Ratios)
    duration = time.time() - session_data["start_time"]
    avg_smile = (sum(session_data["smile_scores"]) / total) * 100
    avg_anxiety = (sum(session_data["anxiety_scores"]) / total) * 100
    gaze_ratio = (session_data["gaze_center_frames"] / total) * 100
    posture_ratio = (session_data["posture_stable_frames"] / total) * 100

    # 3-2. í‰ê°€ ê¸°ì¤€ ì¶œë ¥
    print("ğŸ“‹ í‰ê°€ ì‚°ì¶œ ê¸°ì¤€ (Scoring Criteria):")
    print(f"   - ìì‹ ê°({WEIGHT_CONFIDENCE*100:.0f}%): ë‹µë³€ ì¤‘ ë°ì€ í‘œì •(ë¯¸ì†Œ)ì„ ìœ ì§€í•œ í‰ê·  ìˆ˜ì¹˜")
    print(f"   - ì‹œì„ ì§‘ì¤‘({WEIGHT_FOCUS*100:.0f}%): ì˜ì  ê¸°ì¤€ ì¹´ë©”ë¼ë¥¼ ì •ë©´ìœ¼ë¡œ ì‘ì‹œí•œ ì‹œê°„ ë¹„ìœ¨")
    print(f"   - ìì„¸ì•ˆì •({WEIGHT_POSTURE*100:.0f}%): ì˜ì  ê¸°ì¤€ ë°”ë¥¸ ìì„¸(ê³ ê°œ ìˆ™ì„/ë¹„í‹€ë¦¼ ë°©ì§€) ìœ ì§€ ë¹„ìœ¨")
    print(f"   - ì •ì„œì•ˆì •({WEIGHT_EMOTION*100:.0f}%): ë¯¸ê°„ ì°Œí‘¸ë¦¼ ë“± ë¶ˆì•ˆ ì§€í‘œê°€ ë‚®ê²Œ ìœ ì§€ëœ ì •ë„")
    print("-" * 50)

    # 3-3. ê°€ì¤‘ì¹˜ ì ìš© ë° ìµœì¢… ì ìˆ˜ ì‚°ì¶œ
    score_conf = avg_smile * WEIGHT_CONFIDENCE
    score_focus = gaze_ratio * WEIGHT_FOCUS
    score_posture = posture_ratio * WEIGHT_POSTURE
    score_emotion = (100 - avg_anxiety) * WEIGHT_EMOTION
    overall_score = score_conf + score_focus + score_posture + score_emotion
    
    # 3-4. ê²°ê³¼ ì¶œë ¥
    print(f"â±ï¸ ì´ ë©´ì ‘ ì‹œê°„: {int(duration // 60)}ë¶„ {int(duration % 60)}ì´ˆ")
    print("-" * 50)
    print("ğŸ§® ìƒì„¸ ì±„ì  ë‚´ì—­ (Score Breakdown):")
    print(f"   1. ìì‹ ê°: {avg_smile:5.1f}ì  x {WEIGHT_CONFIDENCE:3.1f} = {score_conf:4.1f}ì ")
    print(f"   2. ì‹œì„ ì§‘ì¤‘: {gaze_ratio:5.1f}ì  x {WEIGHT_FOCUS:3.1f} = {score_focus:4.1f}ì ")
    print(f"   3. ìì„¸ì•ˆì •: {posture_ratio:5.1f}ì  x {WEIGHT_POSTURE:3.1f} = {score_posture:4.1f}ì ")
    print(f"   4. ì •ì„œì•ˆì •: {100-avg_anxiety:5.1f}ì  x {WEIGHT_EMOTION:3.1f} = {score_emotion:4.1f}ì ")
    print(f"   -------------------------------------------")
    print(f"   âˆ‘ ìµœì¢… í•©ê³„: {score_conf:.1f} + {score_focus:.1f} + {score_posture:.1f} + {score_emotion:.1f} = {overall_score:.1f}ì ")
    
    print("-" * 50)
    print(f"ğŸ”¥ ê¸´ì¥ ì§‘ì¤‘ ë¶„ì„:")
    print(f"   - ìµœê³  ê¸´ì¥ ìˆ˜ì¹˜: {session_data['max_anxiety']*100:.1f}%")
    print(f"   - ê¸´ì¥ ë°œìƒ íšŸìˆ˜: {len(session_data['tension_events'])}íšŒ")
    if session_data['tension_events']:
        intervals = ", ".join([f"{t:.1f}ì´ˆ" for t in session_data['tension_events'][:5]])
        print(f"   - ì£¼ìš” ê¸´ì¥ ì‹œì : {intervals}{' ...' if len(session_data['tension_events']) > 5 else ''}")
    
    print("-" * 50)
    print(f"ğŸ† ìµœì¢… ì¢…í•© í‰ì : {overall_score:.1f} / 100")
    print("="*50 + "\n")
    
    # 3-5. JSON íŒŒì¼ ì €ì¥
    report_json = {
        "score": round(overall_score, 1),
        "metrics": {"confidence": round(avg_smile, 1), "focus": round(gaze_ratio, 1), "posture": round(posture_ratio, 1), "anxiety": round(avg_anxiety, 1)},
        "tension_events": session_data['tension_events']
    }
    with open("ai-worker/cv_poc/interview_result.json", "w", encoding="utf-8") as f:
        json.dump(report_json, f, ensure_ascii=False, indent=4)
    print("ğŸ’¾ ê²°ê³¼ íŒŒì¼ì´ 'ai-worker/cv_poc/interview_result.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n")

# ==========================================
# [Step 4] ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (Main Loop)
# ==========================================
def run_live_vision():
    # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš© ì„ ì–¸ (ì˜ì  ì¡°ì ˆ ê°’)
    global calibrated_gaze_x, calibrated_gaze_y, calibrated_pitch, calibrated_eye_diff, calibrated_tilt_diff
    
    session_data["start_time"] = time.time()
    
    # 4-1. MediaPipe FaceLandmarker ëª¨ë¸ ë¡œë“œ ë° ì„¤ì •
    detector = vision.FaceLandmarker.create_from_options(vision.FaceLandmarkerOptions(
        base_options=python.BaseOptions(model_asset_path=model_path), # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        output_face_blendshapes=True, # í‘œì • ë¶„ì„ì„ ìœ„í•œ Blendshapes í™œì„±í™”
        running_mode=vision.RunningMode.VIDEO # ë¹„ë””ì˜¤ ëª¨ë“œ (ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬)
    ))
    
    # 4-2. ì›¹ìº  ì—°ê²°
    cap = cv2.VideoCapture(0)
    cv2.namedWindow('AI Interviewer Vision System', cv2.WINDOW_NORMAL)
    cv2.resizeWindow('AI Interviewer Vision System', 1280, 720)

    # 4-3. í”„ë ˆì„ ë£¨í”„ ì‹œì‘
    while cap.isOpened():
        success, frame = cap.read()
        if not success: break # ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë¦¬ì‚¬ì´ì¦ˆ ë° ì¢Œìš° ë°˜ì „)
        frame = cv2.resize(frame, (1280, 720))
        frame = cv2.flip(frame, 1) # ê±°ìš¸ ëª¨ë“œ
        h, w, _ = frame.shape
        
        # MediaPipeìš© ì´ë¯¸ì§€ ë³€í™˜ (BGR -> RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        
        # [í•µì‹¬] AI ì¶”ë¡  ì‹¤í–‰ (ì–¼êµ´ ê°ì§€)
        result = detector.detect_for_video(mp_image, int(time.time() * 1000))

        if result.face_landmarks:
            landmarks = result.face_landmarks[0]
            left_iris = landmarks[468] # ì™¼ìª½ ëˆˆë™ì ëœë“œë§ˆí¬ ID: 468
            
            # [Step 5] ì‹œì„ (Gaze) ë¶„ì„
            diff_x = left_iris.x - calibrated_gaze_x # ì˜ì  ëŒ€ë¹„ X ë³€í™”ëŸ‰
            diff_y = left_iris.y - calibrated_gaze_y # ì˜ì  ëŒ€ë¹„ Y ë³€í™”ëŸ‰
            
            gaze_label = "ì •ë©´ ì‘ì‹œ"
            gaze_color = (0, 255, 255) # ë…¸ë€ìƒ‰
            if diff_x < -GAZE_TOLERANCE_X: gaze_label = "ì™¼ìª½ ì£¼ì‹œ"
            elif diff_x > GAZE_TOLERANCE_X: gaze_label = "ì˜¤ë¥¸ìª½ ì£¼ì‹œ"
            elif diff_y < -GAZE_TOLERANCE_Y: gaze_label = "ìœ„ìª½ ì£¼ì‹œ"
            elif diff_y > GAZE_TOLERANCE_Y: gaze_label = "ì•„ë˜ìª½ ì£¼ì‹œ"
            
            # [Step 6] ìì„¸(Posture) ë¶„ì„
            eye_diff = abs(landmarks[33].z - landmarks[263].z) # ëˆˆ ê¹Šì´ ì°¨ì´ (ëª¸ ë¹„í‹€ë¦¼)
            tilt_diff = abs(landmarks[33].y - landmarks[263].y)# ëˆˆ ë†’ì´ ì°¨ì´ (ê°¸ìš°ëš±)
            nose_tip = landmarks[1]; chin = landmarks[152]
            pitch_val = chin.z - nose_tip.z # ê³ ê°œ ë„ë•ì„ (Pitch) ê³„ì‚°
            
            # ì˜ì  ëŒ€ë¹„ ì˜¤ì°¨ê°€ í—ˆìš© ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
            is_posture_stable = abs(eye_diff - calibrated_eye_diff) < 0.04 and abs(tilt_diff - calibrated_tilt_diff) < 0.03
            is_head_straight = abs(pitch_val - calibrated_pitch) < HEAD_SENSITIVITY
            
            alignment_label = "âœ… ì•ˆì •"
            align_color = (255, 255, 0)
            if not is_posture_stable: 
                alignment_label = "âš ï¸ [ìì„¸ ë¶ˆê· í˜•]"; align_color = (100, 100, 255)
            elif not is_head_straight: 
                alignment_label = "ğŸš« [ê³ ê°œ ê°ë„ ì´íƒˆ]"; align_color = (50, 50, 255)

            # [Step 7] ê°ì •(Emotion) ë¶„ì„
            blendshapes = {b.category_name: b.score for b in result.face_blendshapes[0]}
            smile = (blendshapes.get('mouthSmileLeft', 0) + blendshapes.get('mouthSmileRight', 0)) / 2
            brow_down = (blendshapes.get('browDownLeft', 0) + blendshapes.get('browDownRight', 0)) / 2
            
            emotion_label = "í‰ì˜¨"
            emotion_color = (255, 255, 255)
            if brow_down > 0.35: emotion_label, emotion_color = "âŒ ê¸´ì¥ë„ ë†’ìŒ", (255, 50, 50)
            elif smile > 0.4: emotion_label, emotion_color = "âœ… ìì‹ ê° ìˆìŒ", (0, 255, 0)

            # [Step 8] ë°ì´í„° ëˆ„ì  ë° ì´ë²¤íŠ¸ ê¸°ë¡
            session_data["total_frames"] += 1
            session_data["smile_scores"].append(smile)
            session_data["anxiety_scores"].append(brow_down)
            
            # ê¸´ì¥ ìµœëŒ€ì¹˜ ê°±ì‹ 
            if brow_down > session_data["max_anxiety"]: session_data["max_anxiety"] = brow_down
            
            # ê¸´ì¥ ì´ë²¤íŠ¸ ë°œìƒ ì‹œì  ê¸°ë¡ (2ì´ˆ ì¿¨ë‹¤ìš´ ì ìš©)
            current_sec = time.time() - session_data["start_time"]
            if brow_down > 0.4:
                if not session_data["tension_events"] or (current_sec - session_data["tension_events"][-1] > 2):
                    session_data["tension_events"].append(current_sec)
                    
            if gaze_label == "ì •ë©´ ì‘ì‹œ": session_data["gaze_center_frames"] += 1
            if alignment_label == "âœ… ì•ˆì •": session_data["posture_stable_frames"] += 1

            # [Step 9] UI ë Œë”ë§ (í™”ë©´ ê·¸ë¦¬ê¸°)
            overlay = frame.copy()
            cv2.rectangle(overlay, (0, 0), (450, 310), (0, 0, 0), -1) # ë°˜íˆ¬ëª… ë°°ê²½ ë°•ìŠ¤
            cv2.addWeighted(overlay, 0.5, frame, 0.5, 0, frame)

            frame = draw_korean_text(frame, f"ğŸ‘€ ì‹œì„ : {gaze_label}", (20, 20), gaze_color)
            frame = draw_korean_text(frame, f"ğŸ‘¤ ìì„¸: {alignment_label}", (20, 60), align_color)
            frame = draw_korean_text(frame, f"ğŸ“Š ê°ì •: {emotion_label}", (20, 100), emotion_color)
            frame = draw_korean_text(frame, f"âœ¨ ê¸ì • ì§€ìˆ˜: {int(smile*100)}%", (20, 150), (0, 255, 100))
            frame = draw_korean_text(frame, f"ğŸ“‰ ê¸´ì¥ ì§€ìˆ˜: {int(brow_down*100)}%", (20, 190), (255, 80, 80))
            
            # ë””ë²„ê¹…ìš© ì¢Œí‘œ ì •ë³´ ì¶œë ¥
            debug_info = f"ğŸ“ ëˆˆë™ì X:{left_iris.x:.3f} Y:{left_iris.y:.3f} | ğŸ“ ê°ë„:{pitch_val:.4f}"
            frame = draw_korean_text(frame, debug_info, (20, 235), (180, 255, 255), font=font_sub)
            frame = draw_korean_text(frame, f"ğŸ¯ ì„¤ì •ëœ ì˜ì : ì‹œì„ ({calibrated_gaze_x:.2f},{calibrated_gaze_y:.2f}) / ê°ë„({calibrated_pitch:.3f})", (20, 270), (150, 150, 150), font=font_debug)

        # í•˜ë‹¨ ì•ˆë‚´ ë©”ì‹œì§€
        footer = frame.copy()
        cv2.rectangle(footer, (0, h-60), (w, h), (0, 0, 0), -1)
        cv2.addWeighted(footer, 0.6, frame, 0.4, 0, frame)
        frame = draw_korean_text(frame, "ì •ë©´ ì‘ì‹œ í›„ 's' : ì˜ì ì¡°ì ˆ  |  'q' : ì¢…ë£Œ ë° ë¦¬í¬íŠ¸ í™•ì¸", (w//2 - 280, h - 45), (255, 255, 255))

        # ìµœì¢… í™”ë©´ ì¶œë ¥
        cv2.imshow('AI Interviewer Vision System', frame)
        
        # í‚¤ë³´ë“œ ì…ë ¥ ëŒ€ê¸° (1ms)
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'): break # ì¢…ë£Œ (ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ ì´ë™)
        elif key == ord('s') and result.face_landmarks: # ì˜ì  ì¡°ì ˆ
            calibrated_gaze_x = left_iris.x; calibrated_gaze_y = left_iris.y
            calibrated_pitch = pitch_val; calibrated_eye_diff = eye_diff; calibrated_tilt_diff = tilt_diff
            print("âœ… ì˜ì  ì¡°ì ˆ ì™„ë£Œ! í˜„ì¬ ìì„¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì¡ì•˜ìŠµë‹ˆë‹¤.")

    # [Step 10] ìì› í•´ì œ ë° ë¦¬í¬íŠ¸ í˜¸ì¶œ
    detector.close()
    cap.release()
    cv2.destroyAllWindows()
    generate_report() # ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œì‘ì 
if __name__ == "__main__":
    run_live_vision()
