# Big20 AI 면접 프로젝트 시스템 진행 보고서

> **작성일**: 2026-02-27
> **문서 버전**: 1.1
> **작성자**: 엄재민
> **프로젝트 기간**: 2026-01-20 ~ 2026-03-11

## 1. 프로젝트 개요

### 1.1 프로젝트 목적

Big20 AI 면접 프로젝트(A4 프로젝트)는 최신 AI 기술(LLM, Vision AI, STT)과 실시간 웹 통신 기술(WebRTC)을 결합하여, 실제 면접과 유사한 환경을 제공하는 온프레미스 기반 통합 아키텍처를 구축하는 것을 목적으로 합니다. 대규모 클라우드 자원에 의존하지 않고 로컬 서버 및 통제된 네트워크 내에서 독립적으로 구동 가능한 B2B/B2C 면접 솔루션을 목표로 하며, 면접자의 답변 내용과 비언어적 태도, 이력서 문맥을 종합적으로 분석하여 객관적이고 심도 있는 피드백을 제공합니다.

### 1.2 시스템 아키텍처 부서의 역할 및 관리 범위

시스템 아키텍처는 단순한 컨테이너 배포를 넘어, 무거운 AI 추론 작업을 지연 없이 처리하고 서비스가 365일 안정적으로 동작하기 위한 전체 구조와 기술적 경계를 책임집니다.

- **인프라 아키텍처 (Infrastructure)**: 온프레미스 서버 환경 구성, GPU 구동 환경 최적화(NVIDIA Container Toolkit), GPU/CPU 워커 역할 물리적 분리, Docker Compose 기반 오케스트레이션 전략, VNet(가상 네트워크) 및 포트 할당 정책.
- **애플리케이션 설계 (Application)**: FastAPI 기반의 비동기 백엔드 API 레이어, Celery와 Redis 기반의 메시지 큐 시스템, RAG(Retrieval-Augmented Generation) 파이프라인 흐름 정의.
- **데이터 구조 (Data)**: PostgreSQL 및 pgvector 확장 기능(Extension)을 기반으로 한 1024차원 벡터 검색 구조, ERD 상세 설계, 세션 트랜잭션과 Redis 임시 저장 전략의 분리.
- **보안 및 운영 전략 (Security & Ops)**: 시스템 모니터링 체계, JWT 기반의 Stateless Auth 및 Role-Based Access Control(RBAC), 데이터베이스 외부 노출 차단 정책 설정 등.

---

## 2. 초기 아키텍처 설계안

프로젝트 기획 초기에는 빠른 프로토타이핑을 위해 단일 인스턴스 중심의 모놀리식 구조와 제한적인 상태 관리를 가정했습니다.

- **초기 인프라 구조**: 단일 Docker Compose 환경 내에서 Backend(FastAPI), Frontend(React), DB(PostgreSQL), Media-Server를 모두 구동하는 구조.
- **AI 처리 구조**: 단일 AI Worker를 할당하여 비동기 구분 없이 대형 언어모델 예측(LLM), 음성 인식(STT), 비전 프레임 분석(Vision) 등 이질적인 무거운 작업들을 순차적으로 처리하도록 계획.
- **상태 관리 체계**: 면접 세션의 진행 상태(질문 순서, 현재 발화 등)를 메모리(Variable) 수준에서 단기적으로 관리하고, DB는 최종 결과 통보 및 단순 로그 저장용으로만 활용할 계획이었습니다.
- **동기 LLM 호출**: Frontend에서 질문을 요청할 때마다 백엔드가 동기(Synchronous) 방식으로 블로킹된 상태에서 API 언어 모델을 호출, 사용자에게 답변을 내려주기까지 장시간 대기가 발생할 것으로 예상되는 구조였습니다.

**문제점 도출**: 이러한 초기 구조는 개발의 용이성을 제공했지만, 본격적인 스트레스 테스트와 복합적인 AI 추론 환경 하에서 심각한 병목을 일으켰습니다. 특히 GPU 연산과 CPU 연산이 한정된 자형 안에서 경합을 벌이면서 STT 변환 주기가 길어지거나, LLM의 텍스트 생성이 멈추는 확장성 한계를 뚜렷이 드러냈습니다.

---

## 3. 설계 진행 과정

시스템 구축이 본격화되면서 발견된 병목 현상과 성능 이슈에 적극적으로 대응하기 위해 아키텍처를 대폭 진화시켰습니다.

### 3.1 인프라 진행 과정 (Resource Isolation & Network)

- **자원 격리 적용 (GPU vs CPU)**: AI 연산의 특징적 차이를 분석하여, VRAM 집약적인 작업(EXAONE-3.5 LLM 프롬프트 추론, TTS 변환)과 코어 연산 집약적인 작업(Whisper/Browser 기반 STT 전처리, PDF 이력서 파싱, Face 랜드마크 추출)을 수행하는 Celery Worker 컨테이너를 물리적으로 2개(`gpu_worker`, `cpu_worker`) 분리했습니다. 이를 통해 서로 간의 리소스 경합(Lock)을 방지했습니다.
- **WebRTC 네트워크 최적화**: 화상 면접에 필수적인 WebRTC 영상 스트리밍을 안정적으로 연결하기 위해 NAT 우회 로직을 구성했습니다. Media-Server 컴포넌트 내에 Host IP 마스킹 기법을 적용하고, 데이터 채널 소켓용 광역 포트(`50000-50050/udp`)를 명시적으로 열어 내부망 컨테이너 네트워크 충돌을 완전히 제거했습니다.

### 3.2 애플리케이션 구조 변화 (Async Message Broker)

- **비동기 큐 전면 도입**: 동기식 AI 호출의 치명적인 타임아웃 문제를 해결하기 위해 백엔드를 Event-Driven 방식으로 개편했습니다. Redis 기반의 메모리형 메세지 브로커와 Celery Task Queue를 병합 적용하여, 수신된 API 요청을 Queue에 밀어넣고 클라이언트는 즉각 응답을 받은 뒤 필요 시점에 결괏값만 폴링/웹소켓으로 받아가도록 비동기 파이프라인을 확립했습니다.
- **상태 관리 고도화**: 휘발성 메모리에 의존하던 면접 세션을 변경하여, Redis로는 단기 연결상태(Connection Caching)만 유지하고, 질문 번호 추적, 진행률 등 지속적인 상태 트랜잭션은 모두 PostgreSQL 로 격리하여 데이터 영속성 상실(Data Loss)을 차단했습니다.

### 3.3 데이터 구조 변화 (PostgreSQL & Vector Search)

- **RAG/Vector 검색 통합**: LLM이 이력서와 기업 인재상에 맞춰 정교하게 질문하려면 벡터 검색이 필수였습니다. 덩치가 큰 외부 Vector DB(Milvus 등)를 별도 설치하는 대신, 단일 DB 전략으로 아키텍처를 일원화하고자 기존 PostgreSQL 18 인스턴스에 `pgvector` 확장을 직접 이식했습니다. 이로써 정형 RDBMS 데이터와 1024차원의 AI 임베딩 벡터 데이터를 Join 쿼리하여 속도 지연 없이 통합 서치할 수 있게 되었습니다.
- **인덱스 전략 수정**: 데이터베이스 관계 무결성을 지키기 위해 엔티티 간 Foreign Key 연결을 강화했습니다. 실시간 기록되는 사용자 대화(`Transcripts`)와 AI의 특정 `Question` ID를 상호 매핑하는 형태로 ERD 구조를 튜닝, 추후 종합 채점 파이프라인에서 완벽하게 문맥을 연결할 수 있도록 개선했습니다.

### 3.4 협업 및 형상 관리 환경 구축

- **Git 기반 형상 관리 확립**: 초기 단계에서의 무작위한 파일 복사 시스템 한계를 극복하기 위해, Git 원격 저장소를 전면 도입했습니다. 커밋 이력(History Tracking)과 브랜치(Branching) 전략을 통해 프론트엔드와 백엔드 간 API 동기화 문제를 제거했습니다.
- **.gitignore 상세 정책 적용**: 보안에 민감한 API Key 관련 환경 변수(`.env`), 크기가 5GB에 달하는 막대한 GGUF/PT 모델 가중치 파일들, 그리고 운영 체제별 충돌을 유발하는 파이썬/노드 종속성 캐시(`__pycache__`, `node_modules`) 등을 강력히 배제하여 저장소를 깃허브 등 원격 서버에 올리기 적합하도록 슬림화했습니다.
- **멀티 컴포넌트 모노레포 관리**: `frontend` (React), `backend-core` (FastAPI), `ai-worker` (DL 환경), `media-server` (aiortc) 등 각기 다른 언어와 실행 환경을 가진 복잡한 MSA 모듈들을 하나의 최상위 레포지토리 하위에 단일 트리로 분산 배치하여 코드 관리를 일원화했습니다.

---

## 4. 아키텍처 변경 이력

주요 구조적 전환 내역은 다음과 같습니다.

| 변경 항목               | 변경 전                      | 변경 후                              | 변경 이유 및 영향도                                                                                                     |
| ----------------------- | ---------------------------- | ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------- |
| **Worker 구성**   | 단일 AI Worker               | GPU / CPU 분리 Worker                | **이유**: 컨테이너 간 리소스 경합 방지 `<br>`**영향**: GPU 메모리 분산 및 처리 응답성 극대화              |
| **LLM 호출 방식** | 동기식(Synchronous) API 호출 | Celery 비동기 처리                   | **이유**: 요청 대기 시간으로 인한 Time-out 방지 `<br>`**영향**: 응답 안정성 및 시스템 로드 감소 효과 확보 |
| **STT 처리**      | 서버 내 로컬 Whisper 혼용    | Client-Side Audio / Celery 원격 처리 | **이유**: 실시간성 보강 및 서버 부하 이전 `<br>`**영향**: 발화 즉시 텍스트화 달성 및 2초 이내 변환 완성   |
| **DB 운영**       | 일반 RDBMS + 별도 Vector DB  | PostgreSQL + pgvector 통합           | **이유**: 아키텍처 복잡도 축소 방안 `<br>`**영향**: 트랜잭션 관리와 RAG 파이프라인의 통합으로 정합성 향상 |

---

## 5. 기술 이슈 및 리스크 대응

과정 중 직면했던 주요 엔지니어링 병목과 해결 기법을 요약합니다.

- **PostgreSQL 볼륨 마운트 에러**: PostgreSQL 18 버전 업그레이드 후, 기존 `pg17`에서 사용되던 `/var/lib/postgresql/data` 디렉토리 경로가 `/var/lib/postgresql`로 스펙이 변경되며 컨테이너 구동 실패 발생. `docker-compose.yml` 볼륨 설정을 새 버전에 맞춰 변경하여 해결함.
- **Celery Task 모듈 참조 오류 (Namespace Isolation)**: 백엔드 컨테이너와 AI 워커 컨테이너가 물리적으로 쪼개지면서 Task Class 코드를 직접 `Import`하여 호출하는 데 실패함. Task의 고유 이름을 직접 String으로 넘겨 호출하는 `celery_app.send_task()` 패턴으로 완전히 전환해 양측 의존성을 디커플링함.
- **Docker 내부 파일 볼륨 공유 부재**: 백엔드 사용자가 업로드한 PDF(이력서) 파일이 다른 컨테이너인 AI Worker에서 파싱 스크립트에 접근하지 못하는 파일 시스템 격리 현상 대응. 백엔드의 `uploads` 디렉토리를 공용 Docker 마운트 볼륨으로 바인딩 매핑 처리하여 읽기/쓰기 환경을 공유함.
- **비전 AI 모델의 연산 병목 (Overload)**: 카메라 스트리밍에서 프레임을 무제한으로 분석하다보니 Media-Server CPU Load가 급증함. 이에 OpenCV와 MediaPipe 측에 5FPS(2초당 1장) 간격의 강제 샘플링 타이머 레이트를 적용하여 프레임 추출을 제한함으로써 서버 안정성을 비약적으로 확보함.
- **AI 질문 생성 지연 무한 루프**: LLM 엔진에서 메모리 오류 등으로 생성이 지연될 때 프론트가 2분 넘게 멈춰있는 현상 발생. 3회 이상 Retry가 넘어가면 예외처리(Exception) 블록에서 **기본 폴백(Fallback) 답변**('[시스템 질문] ... 가장 뛰어난 점은?')을 `db_session`에 강제 트리거하도록 추가하여 대화 세션의 동맥경화를 예방함.

---

## 6. 성능 검증 결과

실제 온프레미스 인스턴스 구축 결과 달성한 주요 성능 스펙입니다.

1. **실시간성(WebRTC)**: P2P 환경 기준 미디어 응답 레이턴시 500ms 미만 유지 (UDP 50000 포트 트래버스 최적화).
2. **LLM 처리 속도(EXAONE 3.5)**: 꼬리질문 및 답변 종합 평가 요청 처리 시 Task 큐 딜레이 포함 API 평균 질문 생성 10초 이내 완료 달성.
3. **음성인식(STT) 속도**: 사용자 발화 종료(Silence) 감지 후 10초 이내 Text Transcript 변환 완료.
4. **시각 분석(Vision)**: 200ms 미만의 Latency로 얼굴 랜드마크 분석 및 감정(Dominant Emotion) 스코어 산출 달성.

---

## 7. 산출물 목록

진행 과정에서 다음과 같은 산출물을 도출하여 관리하고 있습니다.

1. `시스템_아키텍처_디자인(SAD).docx` (전반 시스템 개요 정의)
2. `SYSTEM_SPECIFICATION.md` (시스템 명세서)
3. `TROUBLESHOOTING.md` (기술 오류 및 해결 가이드)
4. `docker-compose.yml` (통합 인프라 배포 파일 최종 버전)
5. `QUALITY_INSPECTION_REPORT.md` (품질 검사 및 보안 개선 리포트)

---

## 8. 최종 확정 아키텍처

거듭된 리팩토링을 통해 확정된 최종 컴포넌트 간 통합 배치도입니다.

- **최종 인프라 구조**: Docker Compose 기반 가상 멀티 컨테이너 네트워크(DNS).
  - Web Server: React Client (Node.js)
  - Core API API: FastAPI (Python)
  - RTC Streaming: Media-Server (aiortc 기반 P2P Relay)
  - ML Operations: AI Worker (CPU와 GPU로 이원화된 Task 큐 처리반)
  - Data Layer: PostgreSQL, Redis 브로커.
- **최종 AI 모델 구동 현황**:
  - LLM: `EXAONE-3.5-7.8B-Instruct (GGUF)` 단일 모델로 사용자 답변 꼬리질문 생성, 이력서 스캔, 문장 요약, 최종 종합 평가 점수 부여 역할 완전 통합수행.
  - Vision/Face: `DeepFace`, `MediaPipe` 기반의 표정 분석. 
- **데이터 구조 통합 완료**: `pgvector` 플러그인 설정이 완료된 RDBMS 모델 생성기. 사용자 메타데이터, 파싱된 이력서의 1024차원 벡터 수, 그리고 사전에 정의된 회사 인재상 객체들이 Foreign Key를 매개로 견고하게 연계 됨.

최종 확정된 이 구조는 On-premise 환경 기준 수십 명 규모의 동시 면접 시나리오를 효과적으로 방어하며, 안정적으로 실서비스 투입 가능한 검증 단계에 도달했습니다.

---

## 9. 운영 준비 상태

본 시스템은 실제 운영 투입을 위한 기반이 갖춰져 있습니다.

- **권한 철저 분리**: JWT Stateless Token 및 API EndPoint 마다 `Candidate / Recruiter` 별 RBAC (Role-Based Access Control) 권한 제어 적용 완료.
- **민감정보 보호**: 외부 통신망(DB, Broker) 외부 공개 차단 설정 및 환경변수 주입을 통해 컨테이너 레벨 보안 정책수립.
- **장애 모니터링 체계 준비**: Docker log 및 Celery Task 추적 로그 확인 환경을 제공하며, 장애 발생 시 원인을 빠르게 트래킹할 수 있는 기초 환경(Troubleshooting Guide) 구성.

---

## 10. 향후 고도화 계획

- **모델 경량화 전략(Quantization)**: VRAM 비용 절감 및 속도 개선을 위해 EXAONE의 GGUF 기반 양자화 추론기 고도화 및 최적화 추진.
- **분산 스케일아웃 확장(Scale-out)**: 동시 접속 증가 대응을 위한 Celery 워커 클러스터 물리 확충 방안 모색 및 대규모 Kafka 브로커 전환 검토.
- **서비스 분리 및 고가용성 확보(Docker Swarm)**: 현재의 단일 노드 Compose 환경에서 벗어나 Frontend, Backend, DB 노드를 물리적으로 분리하고, Docker Swarm 기반의 오케스트레이션을 구축하여 부하 분산 및 고가용성(HA) 환경 확보 예정.
- **LLM 품질 파이프라인**: LangSmith 등 Observability 도구 연동을 통한 면접 응답 품질(QnA) 트레이싱 대시보드 구축 진행.
