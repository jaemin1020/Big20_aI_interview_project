# 2026-02-11 AI Interview 시스템 오류 수정 내역

## 1. AI-Worker 문법 오류 수정 (SyntaxError)

- **대상 파일**: `ai-worker/tasks/stt.py`
- **오류 현황**: Git 머지 충돌 기호(`<<<<<<<`, `=======`, `>>>>>>>`)가 코드 내에 남아서 Celery 워커가 실행되지 않고 즉시 종료되는 문제 발생.
- **수정 내용**: 충돌 기호를 제거하고 `stt_model` 전역 변수를 사용하는 올바른 코드로 정리 완료.

## 2. RAG 파이프라인 데이터 저장 로직 연결

- **대상 파일**: `ai-worker/tasks/resume_embedding.py`
- **오류 현황**: 이력서 파싱 및 청킹은 수행되나, 생성된 청크 벡터(Embedding)들이 실제 벡터 DB 테이블(`resume_embeddings`)에 저장되지 않고 누락되는 현상 확인. (RAG 검색 불가 원인)
- **수정 내용**: `pgvector_store.py`의 `store_embeddings` 함수를 연결하여, 모든 이력서 청크 데이터가 PostgreSQL 벡터 테이블에 정상적으로 적재되도록 로직 수정.

## 3. DB 초기화 스크립트 오류 수정

- **대상 파일**: `infra/postgres/init.sql`
- **오류 현황**: `evaluation_reports` 테이블이 아직 생성되지 않은 상태에서 트리거를 생성하려 시도하여 DB 컨테이너가 에러(Code 3)와 함께 종료됨.
- **수정 내용**: 테이블 존재 여부를 체크하여 안전하게 트리거를 생성하거나, 백엔드에서 생성하도록 로직 분리 검토 (현재는 스크립트 실패를 방지하도록 주석 처리 또는 조건문 추가).

---

**상태**: 수정 및 연결 작업 완료 (정상 작동 확인 대기)

## 4. LLM 아키텍처 구조 분리 이유 (기술적 배경)
현재 `utils/exaone_llm.py`(엔진)와 `tasks/question_generator.py`(작업자)가 분리된 핵심 이유는 다음과 같습니다.

1. **GPU 메모리(VRAM) 효율성**: EXAONE 같은 거대 모델은 로딩 시 수 GB의 메모리를 사용합니다. `exaone_llm.py`에서 **싱글톤(Singleton)** 패턴을 사용함으로써, 시스템 전체에서 모델을 딱 한 번만 메모리에 올리고 여러 작업(질문 생성, 답변 평가)이 이 모델을 공유하여 메모리 폭발을 방지합니다.
2. **다목적 재사용성**: 현재 모델은 질문 생성뿐만 아니라 답변 평가(`evaluator.py`)에서도 동일하게 사용됩니다. 공통 유틸리티로 분리함으로써 코드 중복을 막고 통합 관리가 가능해집니다.
3. **관심사 분리 (Separation of Concerns)**:
    - `question_generator.py`: 면접의 **비즈니스 로직**(순서, 이력서 데이터 추출, 시나리오 단계)에 집중합니다.
    - `exaone_llm.py`: **AI 엔진 설정**(모델 경로, GPU 레이어 설정, 토큰 생성 파라미터)에 집중합니다.

## 5. 프롬프트 제어권 단일화 및 엔진 정예화 (Authority Consolidation & Engine Purification)
- **문제**: `question_generator.py`와 `exaone_llm.py` 양쪽의 코드 중복으로 인해 프롬프트 수정이 제대로 반영되지 않던 구조적 결함 해결.
- **수정 상황**:
    - **`utils/exaone_llm.py`**: 모든 하드코딩된 프롬프트와 면접 로직을 삭제하고, 오직 모델 로딩과 텍스트 출력 기능만 남긴 **'순수 엔진'**으로 변경.
    - **`tasks/question_generator.py`** & **`tasks/evaluator.py`**: 각 작업에서 사용하는 페르소나와 평가지표(프롬프트)를 각 파일 상단에서 직접 정의하고 엔진의 `invoke()` 메서드만 호출하도록 통합 완료.

## 6. LLM 엔진 잔존 코드의 기술적 분석 (Why some codes remain)
모델 출력 외에 `exaone_llm.py`에 최소한의 설정 코드가 남아있는 핵심 이유는 다음과 같습니다.

1. **하드웨어 제어 (GPU/CPU 자원 할당)**:
    - 모델이 질문 생성(GPU)과 답변 분석(CPU) 중 어떤 자원을 사용할지 결정하는 `n_gpu_layers` 설정이 필수적입니다. 이 코드가 없으면 시스템 성능이 10배 이상 저하되거나 메모리 부족으로 실시간 면접이 불가능해집니다.
2. **언어 모델용 프로토콜 (Chat Template)**:
    - EXAONE 3.5 전용 특수 기호(`[|system|]`, `[|endofturn|]`)를 입혀주는 과정이 필요합니다. 이 "포맷팅" 과정이 빠지면 모델이 지시 사항을 무시하고 횡설수설하거나 프롬프트 문구를 그대로 복사하는 오류가 발생합니다.
3. **싱글톤 아키텍처 (메모리 중복 방지)**:
    - 수십 GB에 달하는 모델 파일을 매 요청마다 새로 로드하는 것을 방지합니다. 메모리에 한 번만 모델을 올리고 모든 태스크가 공유하게 함으로써 응답 속도를 비약적으로 높입니다.

## 7. 질문 생성 로직 이원화 문제 분석 (Batch vs Real-time)
- **현상**: 인터뷰 생성 직후 로그에 질문 5개가 리스트(`[...]`) 형태로 한꺼번에 출력되는 현상 확인.
- **원인 분석**:
    1. **이원화된 로직**: 현재 시스템에 '실시간 생성'(`generate_next_question`) 로직 외에, 인터뷰 생성(`POST /interviews`) 시점에 5개의 질문을 미리 뽑아두는 '일괄 생성'(`generate_questions`) 태스크가 남아있음.
    2. **로그의 정체**: 사용자님이 확인하신 로그는 `ai-worker/tasks/question_generator.py`의 `generate_questions_task`가 실행된 결과임.
- **문제점**: 
    - **중복 작업**: 실시간 면접 모드에서는 상황을 봐가며 1개씩 생성하는 것이 핵심인데, 처음에 5개를 미리 뽑는 것은 CPU/GPU 자원 낭비임.
    - **연결 오류**: 앞선 '엔진 정예화' 작업으로 엔진에서 일괄 생성 함수를 지웠기 때문에, 이 구형 로직을 그대로 두면 조만간 에러가 발생할 예정. (현시점에서는 도커 캐시나 이전 버전의 영향으로 작동했을 가능성 큼)
- **해결 방향**: 구형 일괄 생성 로직을 제거하고, 면접 흐름에 따른 **'완전 실시간(1개씩 생성)'** 방식으로 통합 필요.

---
**작업자**: Antigravity (AI Assistant)
**상태**: 질문 생성 이원화 문제 분석 완료 및 기록 완료
