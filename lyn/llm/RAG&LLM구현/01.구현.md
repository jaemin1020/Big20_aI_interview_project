
---

### 1. 지식 적재 파이프라인 (The ETL Pipeline)

이력서나 기업 데이터가 들어왔을 때 사람이 일일이 변환하는 게 아니라, 시스템이 알아서 **[추출 → 가공 → 저장]**하는 컨베이어 벨트를 만드는 과정입니다.

* **구현 로직:**
  1. **Document Loader:** 사용자가 업로드한 PDF/HWP 이력서에서 텍스트만 뽑아냅니다.
  2. **Recursive Character Text Splitter:** 이력서를 단순히 글자 수로 자르는 게 아니라, **문단(Paragraph)이나 마침표 단위**로 의미가 끊기지 않게 자릅니다. (문맥 보존이 핵심!)
  3. **Batch Embedding:** 수천 개의 문장을 하나씩 보내면 느리기 때문에, 묶음(Batch)으로 임베딩 모델(OpenAI 등)에 보내 벡터값으로 바꾼 뒤 VectorDB에 쏟아 넣습니다.
* **전문가의 팁:** 이 과정에서 **'고유 ID'**를 잘 부여해야 합니다. 나중에 지원자가 이력서를 수정하면 해당 ID의 벡터만 찾아 업데이트해야 하니까요.

### 2. Top-K & Threshold 설정 (The Filter)

이 부분이 면접의 **'정확도'와 '창의성'**을 결정하는 밸런스 게임입니다.

* **Top-K (얼마나 많이?)** : 벡터 DB에서 "유사한 문장 상위 K개를 가져와"라고 정하는 것입니다.
* **K가 너무 작으면(예: 1):** 정보가 부족해 AI가 뻔한 질문만 합니다.
* **K가 너무 크면(예: 10):** 관련 없는 정보까지 섞여서 질문이 산으로 갑니다. 보통 면접용으로는 **K=3~5**가 적당합니다.
* **Threshold (얼마나 비슷해야?):** 유사도 점수가 예를 들어 **0.7(70%)** 미만이면 "관련 없는 정보"로 간주하고 무시하는 문턱값입니다.
* **구현 예시:** 지원자가 "저는 파이썬을 잘합니다"라고 했는데, DB에서 찾은 'Java' 관련 데이터 유사도가 0.6이라면, Threshold 0.7 설정 시 이 데이터는 질문 생성에 사용되지 않습니다.

### 3. 출력 파싱 (Output Parsing)

LLM은 기본적으로 '수다쟁이'입니다. "질문은 이거고 점수는 80점이야"라고 말하라고 해도 "네, 알겠습니다. 제가 분석한 결과 질문은..."이라며 군더더기를 붙이죠. 이를 **프로그램이 읽을 수 있는 데이터(JSON)**로 강제하는 과정입니다.

* **구현 방법 (Pydantic Output Parser):** LLM에게 답변 형식을 아예 **스키마(Schema)**로 박아버립니다.
  **JSON**

  ```
  {
    "question": "프로젝트 A에서 사용한 기술은 무엇인가요?",
    "intent": "기술 역량 검증",
    "score": 85,
    "feedback": "구체적인 기술 스택 언급이 좋으나 성과 수치가 부족함"
  }
  ```
* **왜 하는가?** : 이렇게 파싱을 해야 웹 화면에서 질문은 **굵은 글씨**로, 점수는 **게이지 바**로 예쁘게 시각화할 수 있습니다. 파싱이 안 되면 화면에 AI의 수다 텍스트가 통째로 출력되어 서비스 퀄리티가 떨어집니다.

---

### 🛠️ 실제 구현 시의 핵심 흐름 (Data Flow)

1. **사용자 답변 입력** (예: "저는 협업을 중요하게 생각합니다.")
2. **Vector Search:** 프로그램이 이 답변을 들고 DB에서 `Top-K=3`, `Threshold=0.75` 조건으로 **[기업 인재상]**과 **[모범 답안]**을 검색합니다.
3. **Prompt Assembly:** * `시스템 프롬프트(면접관 페르소나)` +
   * `검색된 모범 답안 3개` +
   * `사용자의 현재 답변`을 하나로 합쳐 LLM에게 보냅니다.
4. **LLM 추론:** "음, 모범 답안에 비해 갈등 해결 과정이 부족하네? 꼬리질문을 던져야지."
5. **Parsing:** LLM이 내뱉은 JSON 데이터를 프로그램이 쪼개서 화면에 질문을 띄웁니다.

---

### 💡 20년 차 전문가가 주는 마지막 '디테일'

이 구현 단계에서 가장 중요한 것은 **'답변 속도(Latency)'**입니다.
RAG는 [임베딩 -> 검색 -> 생성] 과정을 거치기 때문에 느려질 수 있어요. 그래서 **Stream(스트리밍)** 방식을 사용해 AI가 답변을 생성하는 대로 한 글자씩 화면에 바로 보여주는 구현이 필수적입니다. 그래야 사용자가 "AI가 생각 중이구나"라고 느끼며 기다릴 수 있거든요.

**혹시 이 과정 중에서 어떤 부분이 가장 구현하기 막막해 보이시나요?**

* "Python으로 **VectorDB에 데이터를 넣는 실제 코드 예시**가 필요하신가요?"
* "LLM이 자꾸 형식을 어기는데, **JSON으로 완벽하게 답변하게 만드는 프롬프트 기법**이 궁금하신가요?"
