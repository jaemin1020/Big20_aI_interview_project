# [산출물 2] Final_Project_RAG_설계서

## 1. 문서 개요

### 1.1 문서 목적

본 문서는 멀티모달 생성형 AI 면접 시스템에서 활용되는 RAG(Retrieval-Augmented Generation) 구조의 상세 설계 기준을 정의합니다. 지원자의 이력서 데이터를 정밀하게 파싱하고, 이를 벡터화하여 실시간 면접 질문 생성 및 답변 평가에 활용하는 전 과정을 명확히 함으로써 시스템의 신뢰성과 성능을 확보하는 것을 목적으로 합니다.

### 1.2 적용 범위

* **데이터 전처리**: PDF 이력서 파싱 및 의미 단위 청킹 (`tasks/parse_resume.py`, `tasks/chunking.py`)
* **지식 저장소**: 벡터 임베딩 생성 및 PostgreSQL pgvector 저장 (`tasks/embedding.py`, `tasks/pgvector_store.py`)
* **지식 검색 & 생성**: 상황별 컨텍스트 추출 및 프롬프트 주입 (`tasks/rag_retrieval.py`, `tasks/question_generator.py`)
* **정성 평가**: 답변 품질 분석 및 루브릭 기반 채점 (`tasks/evaluator.py`)

---

## 2. RAG 적용 배경 및 목적

### 2.1 RAG 도입 필요성

* **할루시네이션(환각) 방지**: LLM이 지원자의 경력을 임의로 지어내지 않도록 실제 이력서 데이터를 검색 근거로 강제합니다.
* **초개인화 면접**: 지원자가 기술한 구체적인 프로젝트 기여도, 사용 기술, 성과 수치를 질문에 직접 인용하여 변별력을 강화합니다.
* **동적 지식 주입**: 면접 단계(자기소개 -> 기술 -> 인성)에 따라 필요한 정보를 실시간으로 교체하여 지연 시간을 최소화합니다.

### 2.2 RAG 설계 목표

* **검색 정확도(Precision)**: 1,000자 이상의 긴 문서에서 질문 의도와 가장 밀접한 상위 3개 청크를 추출합니다.
* **실시간성**: 임베딩 생성부터 검색 결과 반영까지의 전 과정을 5초 이내로 처리합니다.
* **일관성**: 동일한 이력서에 대해 표준화된 평가 지표(Rubric)를 바탕으로 객관적인 점수를 산출합니다.

---

## 3. 문서 파싱 및 구조화 (`tasks/parse_resume.py`)

멀티모달 데이터를 처리하기 위해 단순 텍스트 추출이 아닌 **'구조 기반 분석'** 전략을 취합니다.

### 3.1 PDF 구조 해석 (Table & Text Hybrid)

* **도구**: `pdfplumber` 라이브러리를 통해 레이아웃을 파싱합니다.
* **표(Table) 파싱**: 인적 사항, 학력, 자격증 등 정형화된 데이터는 셀 단위로 분해하여 키-값 쌍으로 매핑합니다.
* **텍스트(Text) 파싱**: 자기소개서 등 비정형 텍스트는 정규표현식(Regex)을 사용하여 질문과 답변 쌍을 분리합니다.

### 3.2 섹션 자동 분류 로직

추출된 데이터는 다음 7개의 논리적 섹션으로 자동 분류됩니다.

1. **Header**: 이름, 목표 직무(`target_role`), 목표 기업
2. **Education**: 학교, 전공, 학점, 졸업 상태
3. **Activities**: 대외활동, 동아리, 인턴십 경험
4. **Awards**: 수상 내역, 주관 기관
5. **Projects**: 프로젝트명, 활동 기간, 기술 스택, 상세 설명
6. **Certifications**: 자격증 명칭, 취득일
7. **Self-Intro**: 특정 질문에 대한 지원자의 서술형 답변

---

## 4. Chunk 전략 및 메타데이터 설계 (`tasks/chunking.py`)

검색 효율을 극대화하기 위해 **'의미 단위 분할'**과 **'속성 정보 유지'**를 동시에 수행합니다.

### 4.1 Chunking 가이드라인

* **알고리즘**: `RecursiveCharacterTextSplitter` (계층적 분할)를 사용합니다.
* **설정값**: `chunk_size=200`, `chunk_overlap=70` (문맥 유지를 위한 35% 중첩 허용)으로 설정합니다.
* **구분자(Separators)**: `["\n\n", "\n", ".", " "]` 순으로 우선 적용하여 문장 중간의 단절을 방지합니다.

### 4.2 메타데이터(Metadata) 구성

각 청크는 검색 결과 필터링을 위해 다음 정보를 포함한 JSONB를 유지합니다.

* `source`: "resume"
* `category`: 청크의 성격 (예: "project", "education", "narrative", "award")
* `subtype`: 하위 분류 (예: "question", "answer")
* `ref_id`: 원본 레코드와의 연결 고리

### 4.3 [상세 분석] 데이터 청킹 로직 (`tasks/chunking.py`)

이 모듈은 구조화된 이력서 데이터를 검색에 최적화된 작은 단위로 분해하며, 각 카테고리별로 다음과 같은 세부 전략을 수행합니다.

#### **1)RecursiveCharacterTextSplitter 기반의 지능형 분할**

* **중첩 허용(Overlap)**: `chunk_overlap=70`을 설정하여 문맥 정보가 청크 사이에서 단절되지 않도록 보장합니다.
* **분할 우선순위**: `\n\n` (단락) -> `\n` (줄바꿈) -> `.` (문장) ->  (단어) 순의 구분자를 사용하여 의미적 완결성을 최대한 유지합니다.

#### **2) 카테고리별 컨텍스트 생성 규칙**

단순히 텍스트를 자르는 것이 아니라, 검색 결과가 어떤 정보인지를 명확히 하기 위해 접두사(Prefix)를 붙여 구조화합니다.

* **프로필**: `[프로필] 이름: {name}, 지원직무: {role}` 형식으로 정보를 통합하여 색인합니다.
* **경험 정보**: `[학력]`, `[활동]`, `[프로젝트]` 등의 명시적 키워드를 청크 서두에 배치하여 LLM이 데이터의 성격을 즉각 파악하게 합니다.
* **자기소개서(Narrative)**:
  * **질문 독립화**: 지원서의 질문 자체도 `[자소서 질문N]` 형식의 독립 청크로 생성하여, 면접관이 지원자의 자기소개서 문항을 참조하기 쉽게 합니다.
  * **답변 맥락 유지**: 답변이 길어질 경우 이를 분할하되, 메타데이터에 `question_ref` (원본 질문 요약)를 포함시켜 검색된 답변 조각이 어떤 질문에 대한 것인지 추적 가능하게 설계했습니다.

#### **3) 검색 최적화를 위한 텍스트 정규화**

* 청킹 과정에서 불필요한 공백과 특수문자를 제거하며, 각 청크가 독립된 지식 단위(Knowledge Unit)로서 의미를 가질 수 있도록 텍스트 구조를 정제합니다.

---

## 5. 임베딩 및 벡터 저장소 (`tasks/embedding.py`, `pgvector_store.py`)

### 5.1 한국어 특화 임베딩 모델

* **모델명**: `nlpai-lab/KURE-v1` (Korean Usage-specific Retrieval Embedder)를 사용합니다.
* **필요성**: 한국어 문맥 이해도가 높고, 자격증 명칭 등 기술 용어 임베딩에 최적화되어 있습니다.
* **최적화**: `normalize_embeddings=True`를 설정하여 코사인 유사도 연산 성능을 향상시킵니다.

### 5.2 Vector DB 설계 (PostgreSQL + pgvector)

* **테이블명**: `resume_embeddings`
* **컬럼 구성**:
  * `resume_id`: 지원자 식별자 (Index)
  * `chunk_type`: 섹션 구분
  * `chunk_text`: 실제 데이터 텍스트
  * `metadata`: JSONB 형식의 상세 속성
  * `embedding`: `Vector(1024)` (768~1024차원 고정)

---

## 6. Retrieval 엔진 설계 (`tasks/rag_retrieval.py`)

검색어 위주의 단순 키워드 매칭이 아닌 **'하이브리드 벡터 검색'** 방식을 적용합니다.

### 6.1 검색 메커니즘

1. **Semantic Search**: 입력받은 쿼리를 벡터로 변환하여 유클리드 거리가 아닌 **'코사인 유사도'**(`<=>` 연산자) 기반의 검색을 수행합니다.
2. **Category Filtering**: `metadata->>'category' = 'project'`와 같이 현재 질문 단계에 맞는 데이터만 필터링하여 노이즈를 제거합니다.
3. **Result Aggregation**: 상위 **3개(Top-K=3)**의 청크를 결합하여 LLM용 컨텍스트를 생성합니다.

### 6.2 검색 쿼리 예시

* **기술 역량 단계**: "지원자의 웹 개발 프로젝트 수행 능력" (filter: `project`)
* **서류 기반 단계**: "지원서의 1번 문항 답변 내용" (filter: `narrative`)

### 6.3 [상세 분석] Retrieval 엔진 내부 로직 (`tasks/rag_retrieval.py`)

이 모듈은 지원자의 질문 내용(Query)과 벡터 데이터베이스 간의 가교 역할을 수행하며, 다음의 핵심 과정을 거칩니다.

#### **1) 임베딩 모델의 지연 로딩(Lazy Loading) 및 싱글톤 관리**

* **리소스 효율화**: 무거운 임베딩 모델(`KURE-v1`)을 서버 시작 시 무조건 로드하지 않고, 첫 검색 요청이 발생할 때 `get_embedder()`를 통해 메모리에 적재합니다.
* **장치 자동 감지**: `torch.cuda.is_available()`을 통해 GPU를 우선 할당하며, GPU가 없을 경우 CPU로 자동 폴백(Fallback)하여 시스템 안정성을 확보합니다.
* **모델 정규화**: `encode_kwargs={'normalize_embeddings': True}`를 설정하여 모델 출력 벡터의 길이를 1로 맞춤으로써, 복잡한 공식 없이 단순 내적(Dot Product)만으로 코사인 유사도를 계산할 수 있게 최적화합니다.

#### **2) 동적 SQL 기반의 하이브리드 필터링**

단순 벡터 검색의 한계를 극복하기 위해 **메타데이터 기반의 하이브리드 쿼리**를 생성합니다.

```sql
SELECT chunk_text, metadata, (embedding <=> :qv) as distance
FROM resume_embeddings
WHERE resume_id = :rid
AND metadata->>'category' = :filter_category -- 특정 섹션(프로젝트 등)만 콕 집어 검색
ORDER BY distance ASC LIMIT :k
```

* **범위 제한**: `resume_id`를 통해 다른 지원자의 데이터가 섞이는 것을 원천 차단합니다.
* **의미적 집중**: `metadata->>'category'` 필터를 사용하여, 기술 면접 시에는 '프로젝트' 섹션만 검색하고 인성 면접 시에는 '자기소개' 섹션만 검색하여 검색의 순도(Purity)를 높입니다.

#### **3) 벡터 거리 연산 (`<=>` Operator)**

* **코사인 거리**: `pgvector`의 `<=>` 연산자를 활용합니다. 이는 벡터 간의 각도를 계산하여 거리가 0에 가까울수록(유사도가 1에 가까울수록) 가장 연관성이 높다고 판단합니다.
* **Top-K 추출**: 연산 결과 정렬(ASC) 후 상위 3개를 추출하여 LLM 프롬프트에 제공하며, 각 결과에는 원문(`text`)과 속성(`meta`)을 함께 포함시켜 생성 단계에서 근거를 명확히 할 수 있게 합니다.

---

## 7. 질문 생성 아키텍처 및 시스템 연동 (`tasks/question_generator.py`)

이 섹션은 AI가 단순히 텍스트를 만드는 것을 넘어, 면접의 전체 흐름(Scenario)과 백엔드 서비스가 어떻게 상호작용하는지를 상세히 기술합니다.

### 7.1 시나리오 기반의 단계 제어 (`config/interview_scenario.py` 의존성)

본 시스템은 정해진 시퀀스에 따라 면접을 진행하며, 이는 `INTERVIEW_STAGES` 설정을 통해 제어됩니다.

* **15단계 면접 설계**: 자기소개(Intro)부터 최종 발언(Final Statement)까지 총 15개의 세부 단계가 정의되어 있습니다.
* **단계별 가이드(Guide) 주입**: 각 단계마다 "기술적 원리를 물어볼 것", "갈등 조율 과정을 확인할 것" 등 구체적인 면접관 지침이 포함되어 있어 질문의 목적성을 유지합니다.
* **동적 쿼리 템플릿**: 각 단계는 `query_template`을 보유하여, RAG 검색 시 지원자의 직무와 결합된 최적의 검색어를 생성합니다.

### 7.2 질문 생성 전략 (Generation Strategy)

`question_generator.py`는 시나리오의 `type`에 따라 세 가지 생성 모드를 지원합니다.

1. **Template 모드**: '자기소개'와 같은 공통 질문은 사전에 정의된 문형에 이름과 직무만 치환하여 즉시 반환합니다. (백엔드 초기 기동 시 사용)
2. **AI 모드 (RAG 기반)**: 이력서에서 가장 연관성 높은 3개 청크를 추출하여, 이를 근거로 새로운 질문을 창조합니다.
3. **Follow-up 모드 (꼬리질문)**: RAG 검색 대신 **지원자의 직전 답변**을 컨텍스트로 활용합니다. 이전 답변의 모순점이나 추가 설명이 필요한 부분을 파고들어 깊이 있는 검증을 수행합니다.

### 7.3 [상세 분석] 백엔드 및 DB 실시간 연동 아키텍처

질문 생성은 백엔드 서비스와 Celery 비동기 태스크를 통해 유기적으로 연결됩니다.

#### **1) 백엔드 초기화 (`routes/interviews.py` 연동)**

* 사용자가 면접 시작 버튼을 누르면, 백엔드는 시나리오의 `get_initial_stages()`를 호출합니다.
* 대기 시간 없이 즉시 '자기소개'와 '지원동기'라는 2개의 템플릿 질문을 생성하여 클라이언트에 응답하고 면접을 즉시 시작합니다.

#### **2) 실시간 질문 전이 (`generate_next_question_task`)**

* 지원자의 답변이 완료되고 STT 프로세싱이 끝나면, 백엔드는 차순위 질문 생성을 위해 Celery 태스크를 호출합니다.
* **상태 추적**: `Transcript` 테이블에서 해당 인터뷰의 최신 대화 기록을 조회하여 현재 어느 단계까지 진행되었는지 파악하고, `get_next_stage()`를 통해 다음 목표 단계를 결정합니다.
* **결과 저장**: 생성된 질문은 `save_generated_question` 함수를 통해 DB의 `Questions` 테이블과 `Transcripts` 테이블에 동시에 기록되어 영속성을 확보합니다.

#### **3) 프롬프트 인스트럭션 구조 (Instruction Hierarchy)**

1. **Persona (페르소나)**: "당신은 15년 차 베테랑 보안 면접관입니다."
2. **Context (문맥 주입)**: RAG 검색 결과 또는 직전 답변을 `{context}` 자리에 주입합니다.
3. **Instruction (제약 조건)**: "한 문장으로 딱딱하게 물어보십시오", "사족을 붙이지 마십시오" 등의 명령을 통해 출력 품질을 강제합니다.

---

## 8. 답변 평가 및 루브릭 산출 (`tasks/evaluator.py`)

이 모듈은 면접 진행 중 발생하는 지원자의 답변을 실시간으로 분석하고, 면접 종료 후 전체 결과를 종합하여 심층 리포트를 생성하는 역할을 수행합니다.

### 8.1 실시간 답변 분석 프로세스 (`analyze_answer`)

지원자의 답변이 수신되면 백엔드는 분석 태스크를 실행하며, 이는 다음과 같은 세부 과정을 거칩니다.

* **즉시 응답성 확보 (Async Trigger)**: 답변 분석 완료를 기다리지 않고, 태스크 시작과 동시에 차순위 질문 생성을 위한 `generate_next_question_task`를 비동기 호출합니다. 이를 통해 지원자는 AI의 평가 시간 동안 대기할 필요 없이 다음 질문을 즉시 받게 됩니다.
* **컴퓨팅 자원 최적화**: 실행 환경의 GPU 활용 가능 여부(`N_GPU_LAYERS`)를 감지합니다. 분석 전담 GPU가 없는 CPU 워커 환경일 경우, 무거운 LLM 연산을 생략하고 기본 점수를 부여하여 처리 속도를 극대화하고 큐(Queue) 정체를 방지합니다. 정밀 평가는 GPU 워커에서 집중적으로 수행됩니다.
* **다차원 평가 루브릭**: 질문과 답변을 대조하여 **기술적 정확도(Technical Score)**와 **의사소통 능력(Communication Score)**을 산출합니다.
* **감정 분석 연동**: 산출된 점수를 기반으로 감정 점수(Sentiment Score)를 도출하여 `Transcript` 테이블에 실시간으로 반영합니다. 이는 면접관 리포트에서 답변의 자신감이나 톤을 시각화하는 지표로 활용됩니다.

### 8.2 최종 평가 리포트 생성 프로세스 (`generate_final_report`)

면접 세션이 종료되면 모든 대화 기록을 통합하여 지원자에 대한 종합적인 인사이트를 도출합니다.

* **전체 대화 맥락(Context) 분석**: 개별 답변의 합산이 아닌, 면접 전체의 대화 흐름을 LLM에 주입하여 맥락을 고려한 종합 요약을 수행합니다.
* **3대 핵심 평가 지표**:
  1. **Technical Score**: 직무 전문성 및 전문 용어 사용의 적절성을 평가합니다. (0~100점)
  2. **Communication Score**: 논리적 전능성 및 의사소통의 명확성을 평가합니다. (0~100점)
  3. **Cultural Fit Score**: 지원자의 태도와 가치관이 조직 문화와 부합하는지 분석합니다. (0~100점)
* **객관적 근거 기반 피드백**: LLM을 통해 면접 답변에서 도출된 **강점(Strengths)**과 **보안점(Weaknesses)**을 JSON 형식으로 정밀하게 추출합니다.
* **데이터 영속화 및 자동 채점**: 생성된 리포트를 `EvaluationReport` 테이블에 저장하고, 인터뷰 레코드의 전체 총점(`overall_score`)을 자동으로 업데이트하여 채점을 완결합니다.

### 8.3 RAG 기반의 진위 검증 (Evidence Validation)

* 평가 단계에서 LLM은 RAG로 저장된 이력서 원문을 다시 검색하여 답변의 사실 여부를 검증합니다.
* 지원자가 언급한 프로젝트 성취가 이력서 데이터와 일치하는지 교차 검증(`Cross-Validation`)함으로써 평가의 객관성과 신뢰도를 확보합니다.

---

## 9. 실패 및 예외 처리 전략 (Exception Management)

시스템의 안정성을 위해 각 단계별 발생 가능한 예외 상황과 이에 대한 구체적인 폴백(Fallback) 메커니즘을 설계하였습니다.

### 9.1 파싱 및 데이터 소실 대응 (`tasks/parse_resume.py`)

* **상황**: PDF 파일의 암호화, 손상 또는 레이아웃 복잡성으로 인해 `pdfplumber`가 표 구조를 읽지 못하는 경우.
* **대응**:
  - 예외 발생 시 즉시 **'텍스트 모드'**로 전환합니다.
  - 표 구조가 없더라도 파일 전체의 텍스트 덩어리를 추출하여 `full_text_buffer`에 담고, 이를 기반으로 자소서 질문/답변 쌍이라도 추출할 수 있도록 보수적으로 처리합니다.

### 9.2 지식 검색 공백 대응 (`tasks/rag_retrieval.py` & `tasks/question_generator.py`)

* **상황**: 이력서의 특정 섹션 정보가 너무 빈약하여 유사도 기준을 충족하는 청크가 검색되지 않는 경우 (`retrieve_context` 결과가 빈 리스트인 경우).
* 대응 :
  - 검색 결과가 없을 경우 프롬프트에 **"이력서 근거 부족"**이라는 상태값을 명시적으로 전달합니다.
  - 모델은 이 상태값을 확인하고, 이력서 인용 대신 해당 면접 단계(`stage`)의 일반적인 평가 가이드(`guide`)에 충실한 질문을 생성하도록 유도됩니다.

### 9.3 AI 모델 응답 불능 대응 (`tasks/question_generator.py`)

* **상황**: LLM 엔진이 토큰 제한이나 연산 오류로 인해 빈 문자열을 반환하거나, 면접관 페르소나를 벗어난 응답을 하는 경우.
* 대응 :
  - 생성된 `content`가 비어있을 경우, 사전에 정의된 **'범용 폴백 질문'**(`{candidate_name}님, 준비하신 내용을 토대로 해당 역량에 대해 더 말씀해주실 수 있나요?`)을 즉시 할당하여 면접 흐름이 끊기지 않도록 방어합니다.

### 9.4 최종 리포트 생성 오류 대응 (`tasks/evaluator.py`)

* **상황**: 면접 요약 및 점수 산출 과정에서 LLM 엔진 부하로 인해 JSON 응답이 파싱되지 않는 경우.
* 대응 :
  - 시스템 지연 상황을 알리는 기본 요약 문구와 함께, 안정적인 통계값(평균점수 75점 등)을 임시로 부여합니다.
  - "분석 시스템 지연으로 요약이 지체되었습니다"라는 명확한 사유를 리포트에 남겨 운영자가 사후 조치할 수 있게 합니다.

### 9.5 시스템 인프라 장애 대응 (`backend-core/routes/interviews.py`)

* **상황**: Celery 브로커 통신 장애나 AI 워커 컨테이너 다운으로 인해 비동기 태스크가 응답하지 않는 경우.
* **대응**:
  - 백엔드는 타임아웃 설정을 통해 무한 대기를 방지하며, 실시간 면접 모드에서는 템플릿 질문을 우선 제공하여 사용자에게 시스템 장애가 노출되는 것을 최소화합니다.

---

## 10. 결론 및 기대 성과

본 RAG 설계를 통해 본 시스템은 **'이력서를 속속들이 알고 있는 AI 면접관'**을 구현합니다. 지원자의 모든 경력을 데이터베이스화하고, 면접관의 페르소나와 결합함으로써 기존 AI 면접이 가졌던 '질문의 단조로움'과 '신뢰성 부족' 문제를 완전히 해결합니다.
