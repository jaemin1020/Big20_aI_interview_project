import cv2
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import numpy as np
import time
import logging
import os

logger = logging.getLogger("Vision-Analyzer")

# [ì„¤ì •] PoCì™€ ë™ì¼í•œ ë¶„ì„ ë¯¼ê°ë„ (ìƒìˆ˜)
GAZE_TOLERANCE_X = 0.08  # ì‹œì„  ì¢Œìš° í—ˆìš©ì¹˜
GAZE_TOLERANCE_Y = 0.08  # ì‹œì„  ìƒí•˜ í—ˆìš©ì¹˜
HEAD_SENSITIVITY = 0.008 # ê³ ê°œ ë„ë•ì„ ë¯¼ê°ë„

class VisionAnalyzer:
    """
    ë¯¸ë””ì–´ ì„œë²„ìš© MediaPipe ë¹„ì „ ë¶„ì„ê¸°
    (í™”ë©´ ê·¸ë¦¬ê¸° ê¸°ëŠ¥ ì œê±°, ìˆœìˆ˜ ë°ì´í„° ë¶„ì„ìš©)
    """
    def __init__(self):
        # ëª¨ë¸ ê²½ë¡œ (Docker í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        # í˜„ì¬ ê²½ë¡œ: /app (media-server root)
        # ëª¨ë¸ì€ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ ë˜ì–´ ìˆì–´ì•¼ í•¨
        self.model_path = 'model_repository/face_landmarker.task'
        
        self.detector = None
        self.is_ready = False

        # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(self.model_path):
            logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.model_path}")
            # ì»¨í…Œì´ë„ˆ ë‚´ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„ (curl)
            try:
                logger.info("-> Google ì„œë²„ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
                os.makedirs("model_repository", exist_ok=True)
                os.system(f"curl -L -o {self.model_path} https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task")
            except Exception as e:
                logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                return
            
        try:
            base_options = python.BaseOptions(model_asset_path=self.model_path)
            options = vision.FaceLandmarkerOptions(
                base_options=base_options,
                output_face_blendshapes=True, # í‘œì • ë¶„ì„ í™œì„±í™”
                running_mode=vision.RunningMode.VIDEO, # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ
                num_faces=1 # ë©´ì ‘ì 1ëª…ë§Œ ë¶„ì„
            )
            # ëª¨ë¸ ë¡œë“œ
            self.detector = vision.FaceLandmarker.create_from_options(options)
            logger.info("âœ… MediaPipe FaceLandmarker ë¡œë“œ ì™„ë£Œ")
            self.is_ready = True
        except Exception as e:
            logger.error(f"âŒ Vision ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_ready = False

        # ì˜ì  (Calibration) ê¸°ë³¸ê°’ (PoCì™€ ë™ì¼)
        self.calibrated_gaze_x = 0.43
        self.calibrated_gaze_y = 0.36
        self.calibrated_pitch = 0.05
        
    def process_frame(self, frame_bgr, timestamp_ms):
        """
        ë¹„ë””ì˜¤ í”„ë ˆì„(BGR)ì„ ë°›ì•„ ë¶„ì„ ê²°ê³¼(Dict)ë¥¼ ë°˜í™˜
        """
        if not self.is_ready:
            return None

        # OpenCV(BGR) -> MediaPipe(RGB) ë³€í™˜
        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)
        
        try:
            # AI ì¶”ë¡  ì‹¤í–‰
            result = self.detector.detect_for_video(mp_image, int(timestamp_ms))
            
            if not result.face_landmarks:
                return {"status": "not_detected"}
                
            landmarks = result.face_landmarks[0]
            blendshapes = result.face_blendshapes[0]
            
            # 1. ì‹œì„  ë¶„ì„ (Gaze)
            left_iris = landmarks[468] # ì™¼ìª½ ëˆˆë™ì
            diff_x = left_iris.x - self.calibrated_gaze_x
            diff_y = left_iris.y - self.calibrated_gaze_y
            
            gaze_status = "center"
            if diff_x < -GAZE_TOLERANCE_X: gaze_status = "left"
            elif diff_x > GAZE_TOLERANCE_X: gaze_status = "right"
            elif diff_y < -GAZE_TOLERANCE_Y: gaze_status = "up"
            elif diff_y > GAZE_TOLERANCE_Y: gaze_status = "down"
            
            # 2. ìì„¸ ë¶„ì„ (Head Pose)
            nose_tip = landmarks[1]
            chin = landmarks[152]
            pitch_val = chin.z - nose_tip.z # ê³ ê°œ ë„ë•ì„
            head_status = "stable" if abs(pitch_val - self.calibrated_pitch) < HEAD_SENSITIVITY else "unstable"
            
            # 3. ê°ì • ë¶„ì„ (Blendshapes)
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            bs_map = {b.category_name: b.score for b in blendshapes}
            
            # ì£¼ìš” ì§€í‘œ ì¶”ì¶œ
            smile_score = (bs_map.get('mouthSmileLeft', 0) + bs_map.get('mouthSmileRight', 0)) / 2
            brow_down_score = (bs_map.get('browDownLeft', 0) + bs_map.get('browDownRight', 0)) / 2
            
            emotion_label = "neutral"
            if smile_score > 0.4: emotion_label = "happy"
            if brow_down_score > 0.4: emotion_label = "anxious" # ê¸´ì¥/ì°Œí‘¸ë¦¼
            
            # [Explicit Log for User Verification]
            if int(timestamp_ms) % 1000 < 100: # Log roughly once per second
                logger.info(f"ğŸ“Š [Vision Score] Emotion: {emotion_label} | Smile: {smile_score:.2f} | Anxiety: {brow_down_score:.2f} | Gaze: {gaze_status}")
            
            return {
                "status": "detected",
                "gaze": gaze_status,
                "head": head_status,
                "emotion": emotion_label,
                "scores": {
                    "smile": round(smile_score, 3),
                    "anxiety": round(brow_down_score, 3),
                    "gaze_x": round(diff_x, 3),
                    "gaze_y": round(diff_y, 3)
                }
            }
            
        except Exception as e:
            logger.error(f"ë¹„ì „ ë¶„ì„ ì¤‘ ì—ëŸ¬: {e}")
            return None

    def close(self):
        if self.detector:
            self.detector.close()
