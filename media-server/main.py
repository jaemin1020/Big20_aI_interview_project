import asyncio
import json
import logging
import os
import base64
import time
import cv2
from typing import Dict, Set
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack, RTCConfiguration, RTCIceServer
from aiortc.contrib.media import MediaRelay
from celery import Celery
import av
from vision_analyzer import VisionAnalyzer  # [NEW] MediaPipe Vision Analyzer
import io  # [NEW] ì˜¤ë””ì˜¤ ë²„í¼ë§ìš©

# 1. ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger("Media-Server")

app = FastAPI()

# CORS ì„¤ì •
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://127.0.0.1:3000",
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "http://127.0.0.1:5173"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

relay = MediaRelay()

# 2. Celery ì„¤ì •
redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
celery_app = Celery("ai_worker", broker=redis_url, backend=redis_url)

# 3. ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„ WebSocket ë° PeerConnection ì €ì¥)
active_websockets: Dict[str, WebSocket] = {}
active_pcs: Dict[str, RTCPeerConnection] = {} # [ì¶”ê°€] ì„¸ì…˜ë³„ PeerConnection ì €ì¥

class VideoAnalysisTrack(MediaStreamTrack):
    """ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ai-workerì— ê°ì • ë¶„ì„ì„ ìš”ì²­í•˜ëŠ” íŠ¸ë™"""
    kind = "video"

    def __init__(self, track, session_id):
        super().__init__()
        self.track = track
        self.session_id = session_id
        
        # [ë°ì´í„° ëˆ„ì ìš©] POC ì„¸ì…˜ ë°ì´í„° êµ¬ì¡° ì´ì‹
        self.analyzer = VisionAnalyzer()
        self.session_started_at = time.time()
        self.total_frames = 0
        
        # ì§ˆë¬¸ë³„ ë°ì´í„° (ì „ì²´ í•©ì‚°ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬)
        self.questions_history = [] 
        self.current_q_index = 0
        self.current_q_data = self._get_empty_q_data()
        
        # ì‹¤ì‹œê°„ ë¡œê·¸ ì¿¨íƒ€ì„
        self.last_log_time = 0
        self.last_tracking_time = 0
        
        logger.info(f"[{session_id}] VideoAnalysisTrack initialized with MediaPipe (CV-V2-TASK Logic)")

    def _get_empty_q_data(self):
        """ìƒˆ ì§ˆë¬¸ì„ ìœ„í•œ ë¹ˆ ë°ì´í„° êµ¬ì¡° ìƒì„±"""
        return {
            "smile_scores": [],
            "anxiety_scores": [],
            "gaze_center_frames": 0,
            "posture_stable_frames": 0,
            "total_frames": 0,
            "start_time": time.time()
        }

    def switch_question(self, new_index):
        """ì§ˆë¬¸ì´ ë°”ë€” ë•Œ í˜¸ì¶œ (from WebSocket)"""
        if self.current_q_data["total_frames"] > 0:
            # ì´ì „ ì§ˆë¬¸ ê²°ê³¼ ìš”ì•½ ë¡œê·¸ ì¶œë ¥
            self._log_question_summary()
            self.questions_history.append(self.current_q_data)
        
        self.current_q_index = new_index
        self.current_q_data = self._get_empty_q_data()
        logger.info(f"[{self.session_id}] â¡ï¸ Switched to Question {new_index}")

    def _log_question_summary(self):
        """ì§ˆë¬¸ë³„ ì¤‘ê°„ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥"""
        q = self.current_q_data
        total = q["total_frames"]
        if total == 0: return
        
        avg_smile = (sum(q["smile_scores"]) / total) * 100
        avg_anxiety = (sum(q["anxiety_scores"]) / total) * 100
        gaze_ratio = (q["gaze_center_frames"] / total) * 100
        posture_ratio = (q["posture_stable_frames"] / total) * 100
        
        logger.info(f"\n[{self.session_id}] ğŸ“ Question {self.current_q_index} ì¤‘ê°„ ê²°ê³¼:")
        logger.info(f"   - ë¯¸ì†Œ(ìì‹ ê°): {avg_smile:.1f}% | ê¸´ì¥ë„: {avg_anxiety:.1f}%")
        logger.info(f"   - ì‹œì„  ì§‘ì¤‘: {gaze_ratio:.1f}% | ìì„¸ ì•ˆì •: {posture_ratio:.1f}%")

    def generate_final_report(self):
        """ë©´ì ‘ ì¢…ë£Œ ì‹œ ì „ì²´ í•©ì‚° ë¦¬í¬íŠ¸ ë¡œê·¸ ì¶œë ¥ (POC í˜•ì‹)"""
        if self.current_q_data["total_frames"] > 0:
            self.questions_history.append(self.current_q_data)
            
        if not self.questions_history:
            return

        total_frames = sum(q["total_frames"] for q in self.questions_history)
        if total_frames == 0: return

        all_smiles = []
        all_anxiety = []
        total_gaze_center = 0
        total_posture_stable = 0
        
        for q in self.questions_history:
            all_smiles.extend(q["smile_scores"])
            all_anxiety.extend(q["anxiety_scores"])
            total_gaze_center += q["gaze_center_frames"]
            total_posture_stable += q["posture_stable_frames"]

        avg_smile = (sum(all_smiles) / total_frames) * 100
        avg_anxiety = (sum(all_anxiety) / total_frames) * 100
        gaze_ratio = (total_gaze_center / total_frames) * 100
        posture_ratio = (total_posture_stable / total_frames) * 100
        
        score_conf = avg_smile * 0.3
        score_focus = gaze_ratio * 0.3
        score_posture = posture_ratio * 0.2
        score_emotion = (100 - avg_anxiety) * 0.2
        overall_score = score_conf + score_focus + score_posture + score_emotion

        print("\n" + "="*50)
        print(f"ğŸ“ AI ë©´ì ‘ ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ [{self.session_id}]")
        print("="*50)
        print(f"â±ï¸ ì´ ì§ˆë¬¸ ìˆ˜: {len(self.questions_history)}ê°œ")
        print(f"â±ï¸ ë¶„ì„ ì‹œê°„: {int(time.time() - self.session_started_at)}ì´ˆ")
        print("-" * 50)
        print("ğŸ§® ìƒì„¸ ì±„ì  ë‚´ì—­ (Score Breakdown):")
        print(f"   1. ìì‹ ê°(ë¯¸ì†Œ) : {avg_smile:5.1f}ì  x 0.3 = {score_conf:4.1f}ì ")
        print(f"   2. ì‹œì„ ì§‘ì¤‘     : {gaze_ratio:5.1f}ì  x 0.3 = {score_focus:4.1f}ì ")
        print(f"   3. ìì„¸ì•ˆì •     : {posture_ratio:5.1f}ì  x 0.2 = {score_posture:4.1f}ì ")
        print(f"   4. ì •ì„œì•ˆì •     : {100-avg_anxiety:5.1f}ì  x 0.2 = {score_emotion:4.1f}ì ")
        print(f"   -------------------------------------------")
        print(f"   âˆ‘ ìµœì¢… í•©ê³„: {overall_score:.1f}ì ")
        print("="*50 + "\n")

    async def process_vision(self, frame, timestamp_ms):
        try:
            img = frame.to_ndarray(format="bgr24")
            result = self.analyzer.process_frame(img, timestamp_ms)
            
            if result and result.get("status") == "detected":
                self.total_frames += 1
                q = self.current_q_data
                q["total_frames"] += 1
                q["smile_scores"].append(result["scores"]["smile"])
                q["anxiety_scores"].append(result["scores"]["anxiety"])
                if result["flags"]["is_center"]: q["gaze_center_frames"] += 1
                if result["flags"]["is_stable"]: q["posture_stable_frames"] += 1

                current_time = time.time()
                if current_time - self.last_log_time > 1.5:
                    self.last_log_time = current_time
                    labels = result["labels"]
                    logger.info(f"[{self.session_id}] Q{self.current_q_index} | ğŸ‘€ ì‹œì„ : {labels['gaze']} | ğŸ‘¤ ìì„¸: {labels['posture']} | ğŸ˜Š ë¯¸ì†Œ: {int(result['scores']['smile']*100)}%")

                ws = active_websockets.get(self.session_id)
                if ws:
                    await send_to_websocket(ws, {
                        "type": "vision_analysis",
                        "data": result,
                        "timestamp": current_time
                    })
        except Exception as e:
            logger.error(f"Vision analysis failed: {e}")

    async def recv(self):
        try:
            frame = await self.track.recv()
            current_time = time.time()
            if current_time - self.last_tracking_time > 0.1:
                self.last_tracking_time = current_time
                asyncio.create_task(self.process_vision(frame, int(current_time * 1000)))
            return frame
        except Exception:
            self.generate_final_report()
            raise

async def start_remote_stt(track, session_id):
    """
    AI-Workerì—ê²Œ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì „ì†¡í•˜ì—¬ STT ì²˜ë¦¬ (Remote STT)
    """
    logger.info(f"[{session_id}] Remote STT Task Loop Started")
    
    audio_buffer = []
    # 2ì´ˆ ë¶„ëŸ‰ ëª¨ì•„ì„œ ì „ì†¡ (ë¹ˆë²ˆí•œ Task ìƒì„± ë°©ì§€)
    # 16kHz, 16bit(2bytes), Mono -> 2ì´ˆ = 16000 * 2 * 2 = 64000 bytes
    BUFFER_SIZE = 64000 
    
    try:
        while True:
            frame = await track.recv()
            
            # 1. ë¦¬ìƒ˜í”Œë§ (WebRTC 48k -> Whisper 16k)
            resampler = av.AudioResampler(format='s16', layout='mono', rate=16000)
            resampled_frames = resampler.resample(frame)
            
            for f in resampled_frames:
                # av.AudioFrame.to_ndarray() -> numpy array
                # tobytes()ë¡œ raw bytes ì¶”ì¶œ
                data = f.to_ndarray().tobytes()
                audio_buffer.append(data)
                
            # 2. ë²„í¼ í¬ê¸° í™•ì¸
            current_size = sum(len(b) for b in audio_buffer)
            
            if current_size >= BUFFER_SIZE:
                # ì²­í¬ ë³‘í•©
                full_audio = b"".join(audio_buffer)
                audio_buffer = [] # ì´ˆê¸°í™”
                
                # Base64 ì¸ì½”ë”©
                b64_audio = base64.b64encode(full_audio).decode('utf-8')
                
                # 3. AI-Workerë¡œ Task ì „ì†¡
                # CeleryëŠ” ë¹„ë™ê¸°ì´ë¯€ë¡œ ì—¬ê¸°ì„œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  Taskë§Œ íì— ë„£ìŒ
                # í•„ìš” ì‹œ ê²°ê³¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³„ë„ ë©”ì»¤ë‹ˆì¦˜ í•„ìš” (ì˜ˆ: Taskê°€ ê²°ê³¼ DBì— ì“°ê³  Polling ë“±)
                task = celery_app.send_task(
                    "tasks.stt.recognize",
                    args=[b64_audio]
                )
                logger.debug(f"[{session_id}] Sent STT chunk to AI-Worker. Task ID: {task.id}")
                
                # (Optional) ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ë¡œ ê¸°ë‹¤ë¦¬ëŠ” ë¡œì§ì„ ì¶”ê°€í•˜ë ¤ë©´ asyncio.to_thread ë“± ì‚¬ìš©
                # í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ Celery RTTëŠ” ì§€ì—°ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ.
                
    except Exception as e:
        logger.error(f"[{session_id}] Remote STT Fail: {e}")
    finally:
        logger.info(f"[{session_id}] Remote STT Stopped")

async def send_to_websocket(ws: WebSocket, data: dict):
    """WebSocketìœ¼ë¡œ ë°ì´í„° ì „ì†¡"""
    try:
        await ws.send_json(data)
    except Exception as e:
        logger.error(f"WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")

# ============== WebSocket ì—”ë“œí¬ì¸íŠ¸ ==============
# [ì¶”ê°€ ë‚´ì—­: 2026-02-11]
# STT ì¤‘ê³„ í•¨ìˆ˜ (Remote STT)
# WebRTC ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ -> WAV íŒŒì¼ ë³€í™˜ -> AI Workerë¡œ ì „ì†¡
async def start_remote_stt(track, session_id):
    logger.info(f"[{session_id}] ğŸ™ï¸ ì›ê²© STT ì‹œì‘ (Remote STT Started)")
    
    # 3ì´ˆ ë‹¨ìœ„ë¡œ ì˜¤ë””ì˜¤ë¥¼ ëª¨ì•„ì„œ ì „ì†¡ (VAD ì—†ì´ ì‹œê°„ ê¸°ë°˜ ë¶„í• )
    CHUNK_DURATION_MS = 3000 
    accumulated_frames = []
    accumulated_time = 0
    
    try:
        while True:
            # 1. ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹ 
            frame = await track.recv()
            accumulated_frames.append(frame)
            
            # í”„ë ˆì„ ì‹œê°„ ëˆ„ì  (packet.duration ì‚¬ìš©í•˜ê±°ë‚˜ ê°œìˆ˜ë¡œ ì¶”ì •)
            # ë³´í†µ Opus í”„ë ˆì„ì€ 20ms or 60ms
            # ì—¬ê¸°ì„œëŠ” í”„ë ˆì„ ê°œìˆ˜ë¡œ ëŒ€ëµì ì¸ ì‹œê°„ ê³„ì‚° (50ê°œ = ì•½ 1ì´ˆ ê°€ì •)
            # ì •í™•ì„±ì„ ìœ„í•´ av.AudioFrame.time ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¨ìˆœí™”
            if len(accumulated_frames) >= 150: # ì•½ 3ì´ˆ (20ms * 150 = 3000ms)
                
                # 2. WAV ë³€í™˜ (In-Memory)
                # av ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Output Container ì‚¬ìš©
                output_buffer = io.BytesIO()
                output_container = av.open(output_buffer, mode='w', format='wav')
                output_stream = output_container.add_stream('pcm_s16le', rate=16000, layout='mono')
                
                for f in accumulated_frames:
                    # ë¦¬ìƒ˜í”Œë§ ë° íŒ¨í‚· ì‘ì„±
                    for packet in output_stream.encode(f):
                        output_container.mux(packet)
                        
                # 3. ë§ˆë¬´ë¦¬ (Flush)
                for packet in output_stream.encode(None):
                    output_container.mux(packet)
                output_container.close()
                
                # 4. Base64 ì¸ì½”ë”©
                wav_bytes = output_buffer.getvalue()
                audio_b64 = base64.b64encode(wav_bytes).decode('utf-8')
                
                # 5. Celery Task ë°°ë‹¬ (AI Workerì—ê²Œ)
                # ê²°ê³¼ê°’ì€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 'ë³´ëƒˆë‹¤'ëŠ” ì‚¬ì‹¤ë§Œ ì¤‘ìš”
                celery_app.send_task(
                    "tasks.stt.recognize",
                    args=[audio_b64],
                    queue="gpu_queue" # GPU ì›Œì»¤ ì „ìš© í ì‚¬ìš©
                )
                
                logger.info(f"[{session_id}] ğŸ“¤ ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡ ì™„ë£Œ ({len(wav_bytes)} bytes)")
                
                # ë²„í¼ ì´ˆê¸°í™”
                accumulated_frames = []

    except Exception as e:
        logger.info(f"[{session_id}] STT ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ: {e}")
    finally:
        logger.info(f"[{session_id}] STT ë¦¬ì†ŒìŠ¤ ì •ë¦¬")


@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    active_websockets[session_id] = websocket
    logger.info(f"[{session_id}] âœ… WebSocket ì—°ê²° ì„±ê³µ")
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸°
            data = await websocket.receive_text()
            try:
                msg = json.loads(data)
                # [ì¶”ê°€] ì§ˆë¬¸ ì „í™˜ ì‹ í˜¸ ì²˜ë¦¬
                if msg.get("type") == "next_question":
                    new_idx = msg.get("index", 0)
                    # í•´ë‹¹ ì„¸ì…˜ì˜ ë¹„ë””ì˜¤ íŠ¸ë™ì„ ì°¾ì•„ì„œ switch_question í˜¸ì¶œ
                    pc = active_pcs.get(session_id)
                    if pc:
                        for sender in pc.getSenders():
                            if isinstance(sender.track, VideoAnalysisTrack):
                                sender.track.switch_question(new_idx)
                                break
            except json.JSONDecodeError:
                pass
            
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] âŒ WebSocket ì—°ê²° ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"[{session_id}] WebSocket ì—ëŸ¬: {e}")
    finally:
        if session_id in active_websockets:
            del active_websockets[session_id]
        if session_id in active_pcs:
            # PCëŠ” ë³„ë„ë¡œ ë‹«íˆì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ìœ„í•´ ìœ ì§€í•˜ê±°ë‚˜ ì¢…ë£Œ ì²˜ë¦¬ ê³ ë¯¼
            # ì—¬ê¸°ì„œëŠ” WebSocket ì¢…ë£Œ ì‹œ PCë„ ì •ë¦¬í•˜ë„ë¡ êµ¬í˜„
            pc = active_pcs.pop(session_id, None)
            if pc:
                await pc.close()
            logger.info(f"[{session_id}] ì„¸ì…˜ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ============== WebRTC ì—”ë“œí¬ì¸íŠ¸ ==============
@app.post("/offer")
async def offer(request: Request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
    session_id = params.get("session_id", "unknown")

    # STUN ì„œë²„ ì„¤ì •ì€ ìœ ì§€ (ë¹„ë””ì˜¤ ì—°ê²° ì•ˆì •ì„±ì„ ìœ„í•´)
    pc = RTCPeerConnection(
        configuration=RTCConfiguration(
            iceServers=[RTCIceServer(urls="stun:stun.l.google.com:19302")]
        )
    )
    active_pcs[session_id] = pc # [ì¶”ê°€] ì„¸ì…˜ë³„ PC ì €ì¥
    @pc.on("track")
    def on_track(track):
        logger.info(f"[{session_id}] Received track: {track.kind}")

        if track.kind == "audio":
            # [ë³€ê²½ ë‚´ì—­: 2026-02-11]
            # 1. ì´ì „ ì½”ë“œì˜ `start_stt_with_local_whisper` í•¨ìˆ˜ëŠ” ì •ì˜ë˜ì§€ ì•Šì•„ ì„œë²„ í¬ë˜ì‹œë¥¼ ìœ ë°œí–ˆìŠµë‹ˆë‹¤.
            # 2. ë¯¸ë””ì–´ ì„œë²„ì—ì„œ ëª¨ë¸ì„ ì§ì ‘ ëŒë¦¬ë©´ ë¹„ë””ì˜¤ ì¤‘ê³„ê°€ ë ‰ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
            #    ë¬´ê±°ìš´ STT ì‘ì—…ì€ ì „ìš© GPU ì›Œì»¤(AI-Worker)ì—ê²Œ ìœ„ì„(Delegate)í•©ë‹ˆë‹¤.
            asyncio.ensure_future(start_remote_stt(track, session_id))
            logger.info(f"[{session_id}] Audio track processing started (Remote STT via AI-Worker)")
            
        elif track.kind == "video":
            # ë¹„ë””ì˜¤ íŠ¸ë™: ê°ì • ë¶„ì„ ì²˜ë¦¬
            pc.addTrack(VideoAnalysisTrack(relay.subscribe(track), session_id))
            logger.info(f"[{session_id}] Video analysis track added")

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

async def consume_audio(track):
    """ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì†Œë¹„í•˜ì—¬ ë²„í¼ê°€ ì°¨ì§€ ì•Šë„ë¡ í•¨"""
    try:
        while True:
            await track.recv()
    except Exception:
        # íŠ¸ë™ì´ ì¢…ë£Œë˜ë©´ ì˜ˆì™¸ ë°œìƒ (ì •ìƒì ì¸ ì¢…ë£Œ)
        pass

@app.get("/")
async def root():
    return {
        "service": "AI Interview Media Server",
        "status": "running",
        "mode": "Video Analysis + Remote STT (via AI-Worker)"
    }

# [ë³µêµ¬: 2026-02-12]
# EnvTestPage.jsx í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸
from fastapi import UploadFile, File, HTTPException

@app.post("/stt/recognize")
async def stt_recognize(file: UploadFile = File(...)):
    """
    STT í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸ (EnvTestPage.jsxì—ì„œ í˜¸ì¶œ)
    """
    try:
        audio_bytes = await file.read()
        audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')
        
        task = celery_app.send_task(
            "tasks.stt.recognize",
            args=[audio_b64],
            queue="gpu_queue"
        )
        # í…ŒìŠ¤íŠ¸ìš©ì´ë¯€ë¡œ ê²°ê³¼ ëŒ€ê¸°
        result = task.get(timeout=30)
        return result
    except Exception as e:
        logger.error(f"STT Test Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")