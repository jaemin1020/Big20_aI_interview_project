import asyncio
import json
import logging
import os
import base64
import time
import cv2
from typing import Dict, Set
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack, RTCConfiguration, RTCIceServer
from aiortc.contrib.media import MediaRelay
from celery import Celery
import av
import numpy as np
from faster_whisper import WhisperModel

# 1. ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger("Media-Server")

app = FastAPI()

# CORS ì„¤ì •
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://127.0.0.1:3000",
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "http://127.0.0.1:5173"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

relay = MediaRelay()

# 2. Celery ì„¤ì •
redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
celery_app = Celery("ai_worker", broker=redis_url, backend=redis_url)

# 3. WebSocket ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„ WebSocket ì €ì¥)
active_websockets: Dict[str, WebSocket] = {}

# 4. Deepgram ì„¤ì • (STTê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
DEEPGRAM_API_KEY = os.getenv("DEEPGRAM_API_KEY")
USE_DEEPGRAM = bool(DEEPGRAM_API_KEY)

if USE_DEEPGRAM:
    try:
        from deepgram import DeepgramClient
        from deepgram.core.events import EventType
        logger.info("âœ… Deepgram SDK v5+ loaded successfully")
    except ImportError as e:
        logger.error(f"âŒ deepgram-sdk import failed: {e}")
        USE_DEEPGRAM = False
    except Exception as e:
        logger.warning(f"âš ï¸ Error loading Deepgram SDK: {e}. STT will be disabled.")
        USE_DEEPGRAM = False
else:
    logger.warning("âš ï¸ DEEPGRAM_API_KEY not set. STT will be disabled.")

# 4.1 Local Whisper ì„¤ì •
WHISPER_MODEL = None
LOCAL_MODEL_SIZE = "large-v3-turbo" # or small, medium, etc.

def load_local_whisper():
    global WHISPER_MODEL
    try:
        if WHISPER_MODEL is None:
            logger.info(f"â³ Loading Local Whisper Model ({LOCAL_MODEL_SIZE})...")
            # Run on GPU with FP16
            WHISPER_MODEL = WhisperModel(LOCAL_MODEL_SIZE, device="cuda", compute_type="float16")
            logger.info("âœ… Local Whisper Model Loaded")
    except Exception as e:
        logger.error(f"âŒ Failed to load Local Whisper: {e}")

import threading

class VideoAnalysisTrack(MediaStreamTrack):
    """ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ai-workerì— ê°ì • ë¶„ì„ì„ ìš”ì²­í•˜ëŠ” íŠ¸ë™"""
    kind = "video"

    def __init__(self, track, session_id):
        super().__init__()
        self.track = track
        self.session_id = session_id
        self.last_frame_time = 0

        # Haar Cascade Load
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')



    async def process_eye_tracking(self, frame):
        """WebRTC í”„ë ˆì„ì—ì„œ ëˆˆ/ì–¼êµ´ ì¶”ì  í›„ WebSocket ì „ì†¡"""
        try:
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            img = frame.to_ndarray(format="bgr24")
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)
            
            tracking_data = []
            
            for (x, y, w, h) in faces:
                roi_gray = gray[y:y+h, x:x+w]
                eyes = self.eye_cascade.detectMultiScale(roi_gray)
                
                eyes_coords = []
                for (ex, ey, ew, eh) in eyes:
                    eyes_coords.append({
                        "x": int(x + ex),
                        "y": int(y + ey),
                        "w": int(ew),
                        "h": int(eh)
                    })
                
                tracking_data.append({
                    "face": {"x": int(x), "y": int(y), "w": int(w), "h": int(h)},
                    "eyes": eyes_coords
                })

            # Status determination
            status = "not_detected"
            if len(tracking_data) > 0:
                face = tracking_data[0] # Assuming first face
                num_eyes = len(face["eyes"])
                
                if num_eyes >= 2:
                    status = "focused"
                elif num_eyes == 1:
                    status = "partially_detected"
                else:
                    status = "eyes_not_visible"
            
            # Log status (throttled)
            current_time = time.time()
            if current_time - getattr(self, 'last_log_time', 0) > 2.0: # Log every 2 seconds
                self.last_log_time = current_time
                logger.info(f"[{self.session_id}] Eye Tracking Status: {status} (Faces: {len(faces)})")

            # WebSocketìœ¼ë¡œ ì „ì†¡
            ws = active_websockets.get(self.session_id)
            if ws:
                await send_to_websocket(ws, {
                    "type": "eye_tracking",
                    "data": tracking_data,
                    "status": status  # Send status to frontend as well
                })

        except Exception as e:
            logger.error(f"Eye tracking frame failed: {e}")

    async def recv(self):
        frame = await self.track.recv()
        current_time = time.time()

        # 1. ëˆˆ ì¶”ì  (ì‹¤ì‹œê°„ì„± ì¤‘ìš” - 0.1ì´ˆë§ˆë‹¤ ìˆ˜í–‰)
        # ëª¨ë“  í”„ë ˆì„ì„ í•˜ë©´ ë¶€í•˜ê°€ í´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°„ê²© ì¡°ì ˆ
        if current_time - getattr(self, 'last_tracking_time', 0) > 0.1:
            self.last_tracking_time = current_time
            # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ì—¬ ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ì§€ì—° ë°©ì§€
            asyncio.create_task(self.process_eye_tracking(frame))

        # 2. ê°ì • ë¶„ì„ (ë¬´ê±°ìš´ ì‘ì—… - 2ì´ˆë§ˆë‹¤ ìˆ˜í–‰)
        if current_time - self.last_frame_time > 2.0:
            self.last_frame_time = current_time
            
            # í”„ë ˆì„ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            img = frame.to_ndarray(format="bgr24")
            _, buffer = cv2.imencode('.jpg', img)
            base64_img = base64.b64encode(buffer).decode('utf-8')

            # ai-workerì— ë¹„ë™ê¸° ê°ì • ë¶„ì„ íƒœìŠ¤í¬ ì „ë‹¬
            celery_app.send_task(
                "tasks.vision.analyze_emotion",
                args=[self.session_id, base64_img]
            )
            # ëˆˆ ì¶”ì  Taskë„ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ì €ì¥ (ì„ íƒì )
            # celery_app.send_task("tasks.vision.track_eyes", args=[self.session_id, base64_img])

        return frame


async def start_stt_with_deepgram(audio_track: MediaStreamTrack, session_id: str):
    """Deepgram ì‹¤ì‹œê°„ STT ì‹¤í–‰ (SDK v5 Sync Pattern with Threading)"""
    if not USE_DEEPGRAM:
        logger.warning(f"[{session_id}] Deepgram ë¹„í™œì„±í™” ìƒíƒœ. STT ê±´ë„ˆëœ€.")
        return
    
    try:
        # Deepgram í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (v5 ë°©ì‹)
        deepgram = DeepgramClient()
        
        # ì—°ê²° ì˜µì…˜
        options = {
            "model": "nova-2",
            "language": "ko",
            "smart_format": True,
            "encoding": "linear16",
            "channels": 1,
            "sample_rate": 16000,
            # VAD ë° ë°œí™” ê°ì§€ ì˜µì…˜ ì¶”ê°€
            "interim_results": True,      # ì¤‘ê°„ ê²°ê³¼ ìˆ˜ì‹  (ë¹ ë¥¸ í”¼ë“œë°±)
            "vad_events": True,           # ë°œí™” ì‹œì‘(SpeechStarted) ê°ì§€ í™œì„±í™”
            "utterance_end_ms": "3000",   # 1ì´ˆ ì¹¨ë¬µ ì‹œ ë°œí™” ì¢…ë£Œë¡œ ê°„ì£¼
            # "endpointing": 300            # (ì„ íƒ) ë” ë¹ ë¥¸ ë¬¸ì¥ ì¢…ê²° ì²˜ë¦¬
        }

        # Deepgram ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì˜¤ë””ì˜¤ ë³€í™˜ (16kHz, Mono, s16le)
        resampler = av.AudioResampler(format='s16', layout='mono', rate=16000)

        # Thread-safe WebSocket sending helper
        loop = asyncio.get_running_loop()

        # [ì¤‘ìš”] Deepgram ì—°ê²° íƒ€ì„ì•„ì›ƒ ë°©ì§€: ì²« ì˜¤ë””ì˜¤ í”„ë ˆì„ì´ ë„ì°©í•  ë•Œê¹Œì§€ ëŒ€ê¸°
        try:
            logger.info(f"[{session_id}] Waiting for first audio frame...")
            first_frame = await audio_track.recv()
            logger.info(f"[{session_id}] First audio frame received. Connecting to Deepgram...")
        except Exception as e:
            logger.warning(f"[{session_id}] Failed to receive first frame: {e}")
            return
        
        # Deepgram v5 Sync Connect Pattern
        with deepgram.listen.v1.connect(**options) as connection:
            logger.info(f"[{session_id}] Deepgram V5 Connection Established")

            def on_message(message, **kwargs):
                """Callback for receiving transcripts & events"""
                try:
                    # 1. ë©”ì‹œì§€ íƒ€ì… í™•ì¸ (SpeechStarted ë“±)
                    msg_type = getattr(message, "type", "Result")
                    
                    if msg_type == "SpeechStarted":
                        logger.info(f"[{session_id}] ğŸ—£ï¸ Speech Started detected")
                        # í”„ë¡ íŠ¸ì—”ë“œì— ë°œí™” ì‹œì‘ ì•Œë¦¼ (ë§í•˜ê¸° ì‹œì‘í–ˆìŒì„ UIì— í‘œì‹œ ê°€ëŠ¥)
                        event_data = {
                            "session_id": session_id,
                            "type": "speech_started",
                            "timestamp": time.time()
                        }
                        if session_id in active_websockets:
                            ws = active_websockets[session_id]
                            asyncio.run_coroutine_threadsafe(send_to_websocket(ws, event_data), loop)
                        return

                    # 2. ì¼ë°˜ Transcript ì²˜ë¦¬
                    if hasattr(message, 'channel') and hasattr(message.channel, 'alternatives'):
                        alt = message.channel.alternatives[0]
                        sentence = alt.transcript
                        
                        if len(sentence) == 0:
                            return
                        
                        # ìµœì¢… ê²°ê³¼(final)ë§Œ ë¡œê·¸ ë˜ëŠ” ì²˜ë¦¬í•  ìˆ˜ë„ ìˆê³ , interimë„ ë³´ë‚¼ ìˆ˜ ìˆìŒ
                        is_final = message.is_final if hasattr(message, 'is_final') else False
                        
                        # ë¡œê·¸ì—ëŠ” Finalë§Œ, í”„ë¡ íŠ¸ì—”ë“œì—ëŠ” ë‘˜ ë‹¤ ì „ì†¡í•˜ì—¬ ì‹¤ì‹œê°„ì„±ì„ ë†’ì„
                        if is_final:
                            logger.info(f"[{session_id}] STT (Final): {sentence}")
                        
                        stt_data = {
                            "session_id": session_id,
                            "text": sentence,
                            "type": "stt_result",
                            "is_final": is_final,
                            "timestamp": time.time()
                        }
                        
                        if session_id in active_websockets:
                            ws = active_websockets[session_id]
                            asyncio.run_coroutine_threadsafe(send_to_websocket(ws, stt_data), loop)

                except Exception as e:
                    logger.error(f"[{session_id}] on_message Error: {e}")

            def on_error(error, **kwargs):
                logger.error(f"[{session_id}] Deepgram Error: {error}")

            # Register Events
            connection.on(EventType.MESSAGE, on_message)
            connection.on(EventType.ERROR, on_error)
            
            # Start listening in a separate thread (Blocking call)
            def listening_thread_func():
                try:
                    connection.start_listening()
                except Exception as e:
                    logger.error(f"[{session_id}] Listening Thread Error: {e}")

            listen_thread = threading.Thread(target=listening_thread_func, daemon=True)
            listen_thread.start()

            
            try:
                # Main Audio Send Loop (Async)
                # 1. ì²« ë²ˆì§¸ í”„ë ˆì„ ì²˜ë¦¬ (ì´ë¯¸ ë°›ì•˜ìœ¼ë¯€ë¡œ)
                try:
                    transformed = resampler.resample(first_frame)
                    for tf in transformed:
                        connection.send_media(tf.to_ndarray().tobytes())
                except Exception as e:
                    logger.error(f"[{session_id}] Error sending first frame: {e}")

                # 2. ì´í›„ í”„ë ˆì„ ë£¨í”„
                logger.info(f"[{session_id}] Streaming audio to Deepgram...")
                frame_count = 1
                while True:
                    try:
                        frame = await audio_track.recv()
                        frame_count += 1
                        
                        # WebRTC AudioFrame(ë³´í†µ 48kHz, Stereo) -> Deepgram(16kHz, Mono) ë³€í™˜
                        # ë³€í™˜í•˜ì§€ ì•Šìœ¼ë©´ Deepgramì´ ë°ì´í„°ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•´ Timeout(1011) ë°œìƒ ê°€ëŠ¥
                        transformed_frames = resampler.resample(frame)
                        
                        for tf in transformed_frames:
                            audio_data = tf.to_ndarray().tobytes()
                            connection.send_media(audio_data)
                        
                        if frame_count % 100 == 0:
                            logger.debug(f"[{session_id}] Sent {frame_count} frames")
                            
                    except Exception as e:
                        logger.warning(f"[{session_id}] Audio Stream Ended/Error: {e}")
                        break
            finally:
                # Loop ends when track closes
                logger.info(f"[{session_id}] Audio track closed. Finishing Deepgram session...")
                # Context manager exit will automatically call finish(), but explicit call ensures thread unblocks
                connection.finish()
            
            # Wait for listening thread to exit
            listen_thread.join(timeout=2.0)
            if listen_thread.is_alive():
                logger.warning(f"[{session_id}] Deepgram listening thread did not exit cleanly")
            else:
                logger.info(f"[{session_id}] Deepgram listening thread finished")

    except Exception as e:
        logger.error(f"[{session_id}] Deepgram Init Failed: {e}")

async def start_stt_with_local_whisper(audio_track: MediaStreamTrack, session_id: str):
    """Local Faster-Whisper ì‹¤ì‹œê°„ STT ì‹¤í–‰ (Buffering approach)"""
    
    logger.info(f"[{session_id}] â­ start_stt_with_local_whisper CALLED - Function entered successfully")
    
    # ëª¨ë¸ ë¡œë”© (ìµœì´ˆ 1íšŒ)
    if WHISPER_MODEL is None:
        load_local_whisper()
    
    if WHISPER_MODEL is None:
        logger.error(f"[{session_id}] Local Whisper Model not available.")
        return

    try:
        logger.info(f"[{session_id}] Starting Local Whisper STT Stream...")
        loop = asyncio.get_running_loop()
        
        # WhisperëŠ” 16kHz, Mono, Float32 ì…ë ¥ì„ ê¸°ëŒ€í•¨
        resampler = av.AudioResampler(format='flt', layout='mono', rate=16000)
        
        buffer = [] # Float32 samples accumulation
        BUFFER_DURATION_SEC = 1.0 # [ìˆ˜ì •] 2ì´ˆ â†’ 1ì´ˆë¡œ ë‹¨ì¶• (ì§§ì€ ë‹µë³€ ì¸ì‹ ê°œì„ )
        SAMPLE_RATE = 16000
        CHUNK_SIZE = int(SAMPLE_RATE * BUFFER_DURATION_SEC)
        
        # ì´ì „ í…ìŠ¤íŠ¸ ì¤‘ë³µ ë°©ì§€ìš©
        last_text = ""

        frame_count = 0
        while True:
            try:
                frame = await audio_track.recv()
                frame_count += 1
                
                # Resample & Convert to Numpy
                frame_resampled = resampler.resample(frame)
                for f in frame_resampled:
                    # to_ndarray returns (1, samples) for stereo or mono depending on layout
                    # format='flt' -> float32
                    chunk = f.to_ndarray()[0] 
                    buffer.extend(chunk)
                
                # í”„ë ˆì„ ìˆ˜ì‹  ë¡œê·¸ (10í”„ë ˆì„ë§ˆë‹¤)
                if frame_count % 10 == 0:
                    logger.info(f"[{session_id}] ğŸ“Š Received {frame_count} frames, buffer size: {len(buffer)}")
                
                # ë²„í¼ê°€ ì¼ì • í¬ê¸° ì´ìƒ ìŒ“ì´ë©´ ì¶”ë¡  ì‹¤í–‰
                if len(buffer) >= CHUNK_SIZE:
                    logger.info(f"[{session_id}] ğŸ¤ Buffer full ({len(buffer)} samples), starting transcription...")
                    audio_data = np.array(buffer, dtype=np.float32)
                    buffer = [] # ë²„í¼ ì´ˆê¸°í™” (ë˜ëŠ” ì˜¤ë²„ë© êµ¬í˜„ ê°€ëŠ¥)

                    # VAD Filterë¥¼ ì¼œì„œ ë¬´ìŒ êµ¬ê°„ ì œì™¸í•˜ê³  ì¸ì‹
                    segments, info = WHISPER_MODEL.transcribe(audio_data, language="ko", vad_filter=True)
                    
                    text_segments = [s.text for s in segments]
                    current_text = " ".join(text_segments).strip()
                    
                    if not current_text:
                        logger.info(f"[{session_id}] ğŸ”‡ No active speech detected (Silence/VAD)")
                    elif current_text == last_text:
                        logger.info(f"[{session_id}] ğŸ” Duplicate text (ignored): {current_text}")
                    else:
                        logger.info(f"[{session_id}] Local STT: {current_text}")
                        last_text = current_text
                        
                        stt_data = {
                            "session_id": session_id,
                            "text": current_text,
                            "type": "stt_result",
                            "is_final": True, # ë¡œì»¬ ë°°ì¹˜ëŠ” í•­ìƒ Finalë¡œ ì·¨ê¸‰
                            "timestamp": time.time()
                        }
                        
                        if session_id in active_websockets:
                            ws = active_websockets[session_id]
                            asyncio.run_coroutine_threadsafe(send_to_websocket(ws, stt_data), loop)

            except Exception as e:
                logger.error(f"[{session_id}] Local Whisper Stream Error/End: {e}", exc_info=True)
                break
        
        # [ì¤‘ìš”] ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œ ë‚¨ì€ ë²„í¼ ì²˜ë¦¬ (ì§§ì€ ë‹µë³€ ë³´ì¡´)
        if len(buffer) > 0:
            logger.info(f"[{session_id}] ğŸ”š Processing remaining {len(buffer)} samples before stream end...")
            try:
                audio_data = np.array(buffer, dtype=np.float32)
                segments, info = WHISPER_MODEL.transcribe(audio_data, language="ko", vad_filter=True)
                text_segments = [s.text for s in segments]
                final_text = " ".join(text_segments).strip()
                
                if final_text and final_text != last_text:
                    logger.info(f"[{session_id}] Local STT (final): {final_text}")
                    stt_data = {
                        "session_id": session_id,
                        "text": final_text,
                        "type": "stt_result",
                        "is_final": True,
                        "timestamp": time.time()
                    }
                    if session_id in active_websockets:
                        ws = active_websockets[session_id]
                        asyncio.run_coroutine_threadsafe(send_to_websocket(ws, stt_data), loop)
            except Exception as e:
                logger.error(f"[{session_id}] Error processing final buffer: {e}")
                
        logger.info(f"[{session_id}] Local Whisper Stream Finished")

    except Exception as e:
        logger.error(f"[{session_id}] Local STT Init Error: {e}")



async def send_to_websocket(ws: WebSocket, data: dict):
    """WebSocketìœ¼ë¡œ ë°ì´í„° ì „ì†¡"""
    try:
        await ws.send_json(data)
    except Exception as e:
        logger.error(f"WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")

# ============== WebSocket ì—”ë“œí¬ì¸íŠ¸ ==============
@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    active_websockets[session_id] = websocket
    logger.info(f"[{session_id}] âœ… WebSocket ì—°ê²° ì„±ê³µ")
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸° (í˜„ì¬ëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ ì—†ìŒ)
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] âŒ WebSocket ì—°ê²° ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"[{session_id}] WebSocket ì—ëŸ¬: {e}")
    finally:
        # ì—°ê²° ì¢…ë£Œ ì‹œ ì„¸ì…˜ ì œê±°
        if session_id in active_websockets:
            del active_websockets[session_id]
            logger.info(f"[{session_id}] WebSocket ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")

# ============== WebRTC ì—”ë“œí¬ì¸íŠ¸ ==============
@app.post("/offer")
async def offer(request: Request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
    session_id = params.get("session_id", "unknown")

    # STUN ì„œë²„ ì„¤ì •ì€ ìœ ì§€ (ë¹„ë””ì˜¤ ì—°ê²° ì•ˆì •ì„±ì„ ìœ„í•´)
    pc = RTCPeerConnection(
        configuration=RTCConfiguration(
            iceServers=[RTCIceServer(urls="stun:stun.l.google.com:19302")]
        )
    )
    logger.info(f"[{session_id}] WebRTC ì—°ê²° ì‹œë„")

    @pc.on("track")
    def on_track(track):
        logger.info(f"[{session_id}] Received track: {track.kind}")

        if track.kind == "audio":
            # [ë³€ê²½] Deepgram ëŒ€ì‹  Local Whisper ì‚¬ìš©
            # asyncio.ensure_future(start_stt_with_deepgram(track, session_id))
            asyncio.ensure_future(start_stt_with_local_whisper(track, session_id))
            logger.info(f"[{session_id}] Audio track processing started (Local Whisper enabled)")
        elif track.kind == "video":
            pc.addTrack(VideoAnalysisTrack(relay.subscribe(track), session_id))
            logger.info(f"[{session_id}] Video analysis track added")
        elif track.kind == "audio":
            # ì˜¤ë””ì˜¤ íŠ¸ë™: ì„œë²„ì—ì„œëŠ” ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ (STTëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ìˆ˜í–‰)
            # ë‹¤ë§Œ WebRTC ì—°ê²° ìœ ì§€ë¥¼ ìœ„í•´ íŠ¸ë™ì„ ì†Œë¹„í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŒ (Blackhole)
            @track.on("ended")
            async def on_ended():
                logger.info(f"[{session_id}] Audio track ended")
            
            asyncio.ensure_future(consume_audio(track))
            logger.info(f"[{session_id}] Audio track ignored (Client-side STT used)")
        else:
            logger.warning(f"[{session_id}] Unknown track type: {track.kind}")

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

async def consume_audio(track):
    """ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì†Œë¹„í•˜ì—¬ ë²„í¼ê°€ ì°¨ì§€ ì•Šë„ë¡ í•¨"""
    try:
        while True:
            await track.recv()
    except Exception:
        # íŠ¸ë™ì´ ì¢…ë£Œë˜ë©´ ì˜ˆì™¸ ë°œìƒ (ì •ìƒì ì¸ ì¢…ë£Œ)
        pass

@app.get("/")
async def root():
    return {
        "service": "AI Interview Media Server",
        "status": "running",
        "mode": "Video Analysis Only (STT migrated to frontend)"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")
