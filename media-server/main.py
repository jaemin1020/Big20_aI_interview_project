import asyncio
import json
import logging
import os
import base64
import time
import cv2
from typing import Dict, Set
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack
from aiortc.contrib.media import MediaRelay
from celery import Celery
import av

# 1. ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger("Media-Server")

app = FastAPI()

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì—°ê²° í—ˆìš©)
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

relay = MediaRelay()

# 2. Celery ì„¤ì • (ai-workerë¡œ ê°ì • ë¶„ì„ ìš”ì²­ ì „ë‹¬ìš©)
celery_app = Celery("ai_worker", broker="redis://redis:6379/0", backend="redis://redis:6379/0")

# 3. WebSocket ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„ WebSocket ì €ì¥)
active_websockets: Dict[str, WebSocket] = {}

# 4. Deepgram ì„¤ì • (STTê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ)
DEEPGRAM_API_KEY = os.getenv("DEEPGRAM_API_KEY")
USE_DEEPGRAM = bool(DEEPGRAM_API_KEY)

if USE_DEEPGRAM:
    try:
        from deepgram import DeepgramClient
        from deepgram.core.events import EventType
        logger.info("âœ… Deepgram SDK v5+ loaded successfully")
    except ImportError as e:
        logger.error(f"âŒ deepgram-sdk import failed: {e}")
        USE_DEEPGRAM = False
    except Exception as e:
        logger.warning(f"âš ï¸ Error loading Deepgram SDK: {e}. STT will be disabled.")
        USE_DEEPGRAM = False
else:
    logger.warning("âš ï¸ DEEPGRAM_API_KEY not set. STT will be disabled.")

import threading

class VideoAnalysisTrack(MediaStreamTrack):
    """ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ai-workerì— ê°ì • ë¶„ì„ì„ ìš”ì²­í•˜ëŠ” íŠ¸ë™"""
    kind = "video"

    def __init__(self, track, session_id):
        super().__init__()
        self.track = track
        self.session_id = session_id
        self.last_frame_time = 0

    async def recv(self):
        frame = await self.track.recv()
        current_time = time.time()

        # 2ì´ˆë§ˆë‹¤ í•œ ë²ˆì”© í”„ë ˆì„ ì¶”ì¶œ (CPU ë¶€í•˜ ë°©ì§€ ë° 4650G ìµœì í™”)
        if current_time - self.last_frame_time > 2.0:
            self.last_frame_time = current_time
            
            # í”„ë ˆì„ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            img = frame.to_ndarray(format="bgr24")
            _, buffer = cv2.imencode('.jpg', img)
            base64_img = base64.b64encode(buffer).decode('utf-8')

            # ai-workerì— ë¹„ë™ê¸° ê°ì • ë¶„ì„ íƒœìŠ¤í¬ ì „ë‹¬ (JSON í¬ë§· ë°ì´í„°)
            celery_app.send_task(
                "tasks.vision.analyze_emotion",
                args=[self.session_id, base64_img]
            )
            logger.info(f"[{self.session_id}] ê°ì • ë¶„ì„ í”„ë ˆì„ ì „ì†¡ ì™„ë£Œ")

        return frame

async def start_stt_with_deepgram(audio_track: MediaStreamTrack, session_id: str):
    """Deepgram ì‹¤ì‹œê°„ STT ì‹¤í–‰ (SDK v5 Sync Pattern with Threading)"""
    if not USE_DEEPGRAM:
        logger.warning(f"[{session_id}] Deepgram ë¹„í™œì„±í™” ìƒíƒœ. STT ê±´ë„ˆëœ€.")
        return
    
    try:
        # Deepgram í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (v5 ë°©ì‹)
        deepgram = DeepgramClient()
        
        # ì—°ê²° ì˜µì…˜
        options = {
            "model": "nova-2",
            "language": "ko",
            "smart_format": True,
            "encoding": "linear16",
            "channels": 1,
            "sample_rate": 16000,
            # VAD ë° ë°œí™” ê°ì§€ ì˜µì…˜ ì¶”ê°€
            "interim_results": True,      # ì¤‘ê°„ ê²°ê³¼ ìˆ˜ì‹  (ë¹ ë¥¸ í”¼ë“œë°±)
            "vad_events": True,           # ë°œí™” ì‹œì‘(SpeechStarted) ê°ì§€ í™œì„±í™”
            "utterance_end_ms": "3000",   # 1ì´ˆ ì¹¨ë¬µ ì‹œ ë°œí™” ì¢…ë£Œë¡œ ê°„ì£¼
            # "endpointing": 300            # (ì„ íƒ) ë” ë¹ ë¥¸ ë¬¸ì¥ ì¢…ê²° ì²˜ë¦¬
        }

        # Deepgram ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì˜¤ë””ì˜¤ ë³€í™˜ (16kHz, Mono, s16le)
        resampler = av.AudioResampler(format='s16', layout='mono', rate=16000)

        # Thread-safe WebSocket sending helper
        loop = asyncio.get_running_loop()

        # [ì¤‘ìš”] Deepgram ì—°ê²° íƒ€ì„ì•„ì›ƒ ë°©ì§€: ì²« ì˜¤ë””ì˜¤ í”„ë ˆì„ì´ ë„ì°©í•  ë•Œê¹Œì§€ ëŒ€ê¸°
        try:
            logger.info(f"[{session_id}] Waiting for first audio frame...")
            first_frame = await audio_track.recv()
            logger.info(f"[{session_id}] First audio frame received. Connecting to Deepgram...")
        except Exception as e:
            logger.warning(f"[{session_id}] Failed to receive first frame: {e}")
            return
        
        # Deepgram v5 Sync Connect Pattern
        with deepgram.listen.v1.connect(**options) as connection:
            logger.info(f"[{session_id}] Deepgram V5 Connection Established")

            def on_message(message, **kwargs):
                """Callback for receiving transcripts & events"""
                try:
                    # 1. ë©”ì‹œì§€ íƒ€ì… í™•ì¸ (SpeechStarted ë“±)
                    msg_type = getattr(message, "type", "Result")
                    
                    if msg_type == "SpeechStarted":
                        logger.info(f"[{session_id}] ğŸ—£ï¸ Speech Started detected")
                        # í”„ë¡ íŠ¸ì—”ë“œì— ë°œí™” ì‹œì‘ ì•Œë¦¼ (ë§í•˜ê¸° ì‹œì‘í–ˆìŒì„ UIì— í‘œì‹œ ê°€ëŠ¥)
                        event_data = {
                            "session_id": session_id,
                            "type": "speech_started",
                            "timestamp": time.time()
                        }
                        if session_id in active_websockets:
                            ws = active_websockets[session_id]
                            asyncio.run_coroutine_threadsafe(send_to_websocket(ws, event_data), loop)
                        return

                    # 2. ì¼ë°˜ Transcript ì²˜ë¦¬
                    if hasattr(message, 'channel') and hasattr(message.channel, 'alternatives'):
                        alt = message.channel.alternatives[0]
                        sentence = alt.transcript
                        
                        if len(sentence) == 0:
                            return
                        
                        # ìµœì¢… ê²°ê³¼(final)ë§Œ ë¡œê·¸ ë˜ëŠ” ì²˜ë¦¬í•  ìˆ˜ë„ ìˆê³ , interimë„ ë³´ë‚¼ ìˆ˜ ìˆìŒ
                        is_final = message.is_final if hasattr(message, 'is_final') else False
                        
                        # ë¡œê·¸ì—ëŠ” Finalë§Œ, í”„ë¡ íŠ¸ì—”ë“œì—ëŠ” ë‘˜ ë‹¤ ì „ì†¡í•˜ì—¬ ì‹¤ì‹œê°„ì„±ì„ ë†’ì„
                        if is_final:
                            logger.info(f"[{session_id}] STT (Final): {sentence}")
                        
                        stt_data = {
                            "session_id": session_id,
                            "text": sentence,
                            "type": "stt_result",
                            "is_final": is_final,
                            "timestamp": time.time()
                        }
                        
                        if session_id in active_websockets:
                            ws = active_websockets[session_id]
                            asyncio.run_coroutine_threadsafe(send_to_websocket(ws, stt_data), loop)

                except Exception as e:
                    logger.error(f"[{session_id}] on_message Error: {e}")

            def on_error(error, **kwargs):
                logger.error(f"[{session_id}] Deepgram Error: {error}")

            # Register Events
            connection.on(EventType.MESSAGE, on_message)
            connection.on(EventType.ERROR, on_error)
            
            # Start listening in a separate thread (Blocking call)
            def listening_thread_func():
                try:
                    connection.start_listening()
                except Exception as e:
                    logger.error(f"[{session_id}] Listening Thread Error: {e}")

            listen_thread = threading.Thread(target=listening_thread_func, daemon=True)
            listen_thread.start()

            
            try:
                # Main Audio Send Loop (Async)
                # 1. ì²« ë²ˆì§¸ í”„ë ˆì„ ì²˜ë¦¬ (ì´ë¯¸ ë°›ì•˜ìœ¼ë¯€ë¡œ)
                try:
                    transformed = resampler.resample(first_frame)
                    for tf in transformed:
                        connection.send_media(tf.to_ndarray().tobytes())
                except Exception as e:
                    logger.error(f"[{session_id}] Error sending first frame: {e}")

                # 2. ì´í›„ í”„ë ˆì„ ë£¨í”„
                logger.info(f"[{session_id}] Streaming audio to Deepgram...")
                frame_count = 1
                while True:
                    try:
                        frame = await audio_track.recv()
                        frame_count += 1
                        
                        # WebRTC AudioFrame(ë³´í†µ 48kHz, Stereo) -> Deepgram(16kHz, Mono) ë³€í™˜
                        # ë³€í™˜í•˜ì§€ ì•Šìœ¼ë©´ Deepgramì´ ë°ì´í„°ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•´ Timeout(1011) ë°œìƒ ê°€ëŠ¥
                        transformed_frames = resampler.resample(frame)
                        
                        for tf in transformed_frames:
                            audio_data = tf.to_ndarray().tobytes()
                            connection.send_media(audio_data)
                        
                        if frame_count % 100 == 0:
                            logger.debug(f"[{session_id}] Sent {frame_count} frames")
                            
                    except Exception as e:
                        logger.warning(f"[{session_id}] Audio Stream Ended/Error: {e}")
                        break
            finally:
                # Loop ends when track closes
                logger.info(f"[{session_id}] Audio track closed. Finishing Deepgram session...")
                # Context manager exit will automatically call finish(), but explicit call ensures thread unblocks
                connection.finish()
            
            # Wait for listening thread to exit
            listen_thread.join(timeout=2.0)
            if listen_thread.is_alive():
                logger.warning(f"[{session_id}] Deepgram listening thread did not exit cleanly")
            else:
                logger.info(f"[{session_id}] Deepgram listening thread finished")

    except Exception as e:
        logger.error(f"[{session_id}] Deepgram Init Failed: {e}")


async def send_to_websocket(ws: WebSocket, data: dict):
    """WebSocketìœ¼ë¡œ ë°ì´í„° ì „ì†¡ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
    try:
        await ws.send_json(data)
    except Exception as e:
        logger.error(f"WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")

# ============== WebSocket ì—”ë“œí¬ì¸íŠ¸ ==============
@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """í”„ë¡ íŠ¸ì—”ë“œì™€ ì‹¤ì‹œê°„ STT ê²°ê³¼ ê³µìœ ë¥¼ ìœ„í•œ WebSocket ì—°ê²°"""
    await websocket.accept()
    active_websockets[session_id] = websocket
    logger.info(f"[{session_id}] âœ… WebSocket ì—°ê²° ì„±ê³µ")
    
    try:
        # ì—°ê²° ìœ ì§€ ë° í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸°
        while True:
            data = await websocket.receive_text()
            # í•„ìš” ì‹œ í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë°›ì€ ë©”ì‹œì§€ ì²˜ë¦¬
            logger.debug(f"[{session_id}] Received from client: {data}")
            
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] âŒ WebSocket ì—°ê²° ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"[{session_id}] WebSocket ì—ëŸ¬: {e}")
    finally:
        # ì—°ê²° ì¢…ë£Œ ì‹œ ì„¸ì…˜ ì œê±°
        if session_id in active_websockets:
            del active_websockets[session_id]
            logger.info(f"[{session_id}] WebSocket ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")

# ============== WebRTC ì—”ë“œí¬ì¸íŠ¸ ==============
@app.post("/offer")
async def offer(request: Request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
    session_id = params.get("session_id", "unknown")

    pc = RTCPeerConnection()
    logger.info(f"[{session_id}] WebRTC ì—°ê²° ì‹œë„")

    @pc.on("track")
    def on_track(track):
        logger.info(f"[{session_id}] Received track: {track.kind}")
        if track.kind == "audio":
            asyncio.ensure_future(start_stt_with_deepgram(track, session_id))
            logger.info(f"[{session_id}] Audio track processing started (STT enabled)")
        elif track.kind == "video":
            pc.addTrack(VideoAnalysisTrack(relay.subscribe(track), session_id))
            logger.info(f"[{session_id}] Video track processing started (Emotion analysis enabled)")
        else:
            logger.warning(f"[{session_id}] Unknown track type: {track.kind}")

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

@app.get("/")
async def root():
    return {
        "service": "AI Interview Media Server",
        "status": "running",
        "websocket_endpoint": "/ws/{session_id}",
        "webrtc_endpoint": "/offer",
        "deepgram_enabled": USE_DEEPGRAM
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")