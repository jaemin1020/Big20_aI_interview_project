import os
import sys

# [DEBUG] ì„œë²„ ì‹œì‘ ì¦‰ì‹œ ì¶œë ¥ (ë²„í¼ë§ ë°©ì§€ìš© flush=True)
print("ğŸš€ [Media-Server] Starting module initialization...", flush=True)

import asyncio
import json
import logging
import os
import base64
import time
import cv2
from typing import Dict, Set
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack, RTCConfiguration, RTCIceServer
from aiortc.contrib.media import MediaRelay
from celery import Celery
import av
from vision_analyzer import VisionAnalyzer  # [NEW] MediaPipe Vision Analyzer
import io  # [NEW] ì˜¤ë””ì˜¤ ë²„í¼ë§ìš©

# 1. ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger("Media-Server")

app = FastAPI()

# CORS ì„¤ì •
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://127.0.0.1:3000",
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "http://127.0.0.1:5173"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

relay = MediaRelay()

# ë¹„ì „ ë¶„ì„ê¸° ì „ì—­ ë³€ìˆ˜
analyzer_instance = None

def get_analyzer():
    global analyzer_instance
    if analyzer_instance is None:
        print("ğŸš€ [Media-Server] VisionAnalyzer first access - initializing (Lazy)...", flush=True)
        analyzer_instance = VisionAnalyzer()
    return analyzer_instance

async def background_init_analyzer():
    """ì„œë²„ ì‹œì‘ ì‹œ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”© (Non-blocking)"""
    global analyzer_instance
    try:
        print("ğŸš€ [Media-Server] Background VisionAnalyzer initialization started...", flush=True)
        # ë¸”ë¡œí‚¹ ì˜¤í¼ë ˆì´ì…˜ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        analyzer_instance = await loop.run_in_executor(None, VisionAnalyzer)
        print("âœ… [Media-Server] Background VisionAnalyzer initialization complete!", flush=True)
    except Exception as e:
        print(f"âŒ [Media-Server] Background initialization failed: {e}", flush=True)

# 2. Celery ì„¤ì •
redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
celery_app = Celery("ai_worker", broker=redis_url, backend=redis_url)

# 3. ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„ WebSocket ë° PeerConnection ì €ì¥)
active_websockets: Dict[str, WebSocket] = {}
active_pcs: Dict[str, RTCPeerConnection] = {}
active_video_tracks: Dict[str, 'VideoAnalysisTrack'] = {}

class VideoAnalysisTrack(MediaStreamTrack):
    """ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ai-workerì— ê°ì • ë¶„ì„ì„ ìš”ì²­í•˜ëŠ” íŠ¸ë™"""
    kind = "video"

    def __init__(self, track, session_id):
        super().__init__()
        self.track = track
        self.session_id = session_id
        
        # [ë°ì´í„° ëˆ„ì ìš©] ì§€ì—° ë¡œë”© í˜¸ì¶œ
        self.analyzer = get_analyzer()
        self.session_started_at = time.time()
        self.total_frames = 0
        
        # ì§ˆë¬¸ë³„ ë°ì´í„° (ì „ì²´ í•©ì‚°ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬)
        self.questions_history = [] 
        self.current_q_index = 0
        self.current_q_data = self._get_empty_q_data()
        
        # [ì‹ ê·œ] ì „ì²´ ë©´ì ‘ í†µí•© ë°ì´í„° ë²„ì¼“ (ëª¨ë“  í”„ë ˆì„ ëˆ„ì )
        self.session_all_data = self._get_empty_q_data()
        
        # ì‹¤ì‹œê°„ ë¡œê·¸ ì¿¨íƒ€ì„
        self.last_log_time = 0
        self.last_tracking_time = 0
        
        print(f"âœ… [{session_id}] VideoAnalysisTrack Created (Continuous Analysis Mode)")

    def _get_empty_q_data(self):
        """ìƒˆ ì§ˆë¬¸ì„ ìœ„í•œ ë¹ˆ ë°ì´í„° êµ¬ì¡° ìƒì„±"""
        return {
            "smile_scores": [],
            "anxiety_scores": [],
            "gaze_center_frames": 0,
            "posture_stable_frames": 0,
            "total_frames": 0,
            "start_time": time.time()
        }

    def switch_question(self, new_index):
        """ì§ˆë¬¸ì´ ë°”ë€” ë•Œ í˜¸ì¶œ (from WebSocket)"""
        # [ë³€ê²½] ì¤‘ê°„ ë¦¬í¬íŠ¸ ì¶œë ¥ì€ ìƒëµí•˜ê³  ë°ì´í„°ë§Œ ë°±ì—…
        if self.current_q_data["total_frames"] > 0:
            self.questions_history.append(self.current_q_data)
        
        self.current_q_index = new_index
        self.current_q_data = self._get_empty_q_data()
        print(f"â¡ï¸ [{self.session_id}] Moved to Question {new_index} (Continuous tracking...)", flush=True)

    def _calculate_scores(self, q_list):
        """ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸(ë˜ëŠ” ë‹¨ì¼ ì§ˆë¬¸)ë¡œë¶€í„° POC ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
        if not q_list: return None
        if isinstance(q_list, dict): q_list = [q_list]
        
        total_frames = sum(q["total_frames"] for q in q_list)
        if total_frames == 0: return None

        all_smiles = []
        all_anxiety = []
        total_gaze_center = 0
        total_posture_stable = 0
        
        for q in q_list:
            all_smiles.extend(q["smile_scores"])
            all_anxiety.extend(q["anxiety_scores"])
            total_gaze_center += q["gaze_center_frames"]
            total_posture_stable += q["posture_stable_frames"]

        # [ë³´ì •] POC ìˆ˜ì‹ì€ ë„ˆë¬´ ì—„ê²©í•¨ (ë¯¸ì†Œê°€ 0ì´ë©´ ìì‹ ê° 0ì  ì²˜ë¦¬ë¨)
        # ë©´ì ‘ ë¬¸ë§¥ì— ë§ê²Œ ë³´ì •: (í‰ê·  ì ìˆ˜ * 0.6) + 40 (ê¸°ë³¸ 40ì  ë² ì´ìŠ¤)
        
        # 1. ìì‹ ê° (ë¯¸ì†Œ): ë¬´í‘œì •(0%)ì¼ ë•Œ 40ì , í™œì§(100%)ì¼ ë•Œ 100ì 
        adj_smile = (avg_smile * 0.6) + 40
        score_conf = adj_smile * 0.3
        
        # 2. ì‹œì„ ì§‘ì¤‘: ì •ë©´ ì‘ì‹œ ë¹„ìœ¨ì— ë”°ë¼ 40~100ì 
        adj_focus = (gaze_ratio * 0.6) + 40
        score_focus = adj_focus * 0.3
        
        # 3. ìì„¸ì•ˆì •: 40~100ì 
        adj_posture = (posture_ratio * 0.6) + 40
        score_posture = adj_posture * 0.2
        
        # 4. ì •ì„œì•ˆì •: ê¸´ì¥ë„(anxiety)ê°€ 0ì¼ ë•Œ 100ì , 100ì¼ ë•Œ 40ì 
        adj_emotion = ((100 - avg_anxiety) * 0.6) + 40
        score_emotion = adj_emotion * 0.2
        
        overall_score = score_conf + score_focus + score_posture + score_emotion
        
        return {
            "avg_smile": adj_smile, "avg_anxiety": avg_anxiety,
            "gaze_ratio": adj_focus, "posture_ratio": adj_posture,
            "raw_smile": avg_smile, "raw_focus": gaze_ratio, # ë””ë²„ê¹…ìš© ì›ë³¸ê°’
            "score_conf": score_conf, "score_focus": score_focus,
            "score_posture": score_posture, "score_emotion": score_emotion,
            "overall_score": overall_score, "total_frames": total_frames
        }

    def _log_question_summary(self):
        """ì§ˆë¬¸ë³„ ìƒì„¸ ì±„ì  ë¦¬í¬íŠ¸ ë¡œê·¸ ì¶œë ¥ (POC ë””ìì¸)"""
        s = self._calculate_scores(self.current_q_data)
        if not s: return
        
        print("\n" + "-"*50)
        print(f"ğŸ“ AI ë©´ì ‘ [{self.current_q_index}ë²ˆ] ì§ˆë¬¸ ë¶„ì„ ë¦¬í¬íŠ¸")
        print("-" * 50)
        print(f"   1. ìì‹ ê°(ë¯¸ì†Œ) : {s['avg_smile']:5.1f}ì  x 0.3 = {s['score_conf']:4.1f}ì ")
        print(f"   2. ì‹œì„ ì§‘ì¤‘     : {s['gaze_ratio']:5.1f}ì  x 0.3 = {s['score_focus']:4.1f}ì ")
        print(f"   3. ìì„¸ì•ˆì •     : {s['posture_ratio']:5.1f}ì  x 0.2 = {s['score_posture']:4.1f}ì ")
        print(f"   4. ì •ì„œì•ˆì •     : {100-s['avg_anxiety']:5.1f}ì  x 0.2 = {s['score_emotion']:4.1f}ì ")
        print(f"   -------------------------------------------")
        print(f"   âˆ‘ í•´ë‹¹ ì§ˆë¬¸ í•©ê³„: {s['overall_score']:.1f}ì ")
        print("-" * 50 + "\n")

    def generate_final_report(self):
        """ë©´ì ‘ ì¢…ë£Œ ì‹œ ì „ì²´ í•©ì‚° ë¦¬í¬íŠ¸ ë¡œê·¸ ì¶œë ¥ (POC ë””ìì¸)"""
        # [ë³€ê²½] ëª¨ë“  í”„ë ˆì„ì´ ì´ë¯¸ session_all_dataì— ëª¨ì—¬ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°
        s = self._calculate_scores(self.session_all_data)
        if not s: 
            print(f"âš ï¸ [{self.session_id}] No analysis data captured during session.")
            return

        print("\n" + "="*50)
        print(f"ğŸ† AI ë©´ì ‘ [ìµœì¢… ì¢…í•©] ë¶„ì„ ë¦¬í¬íŠ¸ [{self.session_id}]")
        print("="*50)
        print(f"â±ï¸ ì´ ì§ˆë¬¸ ìˆ˜: {len(self.questions_history) + 1}ê°œ")
        print(f"â±ï¸ ë¶„ì„ ê¸°ê°„: {int(time.time() - self.session_started_at)}ì´ˆ / {s['total_frames']} frames")
        print("-" * 50)
        print("ğŸ§® [Holistic Capture] ì „ì²´ í‰ê·  ì±„ì  ë‚´ì—­:")
        print(f"   1. ìì‹ ê°(ë¯¸ì†Œ) : {s['avg_smile']:5.1f}ì  x 0.3 = {s['score_conf']:4.1f}ì ")
        print(f"   2. ì‹œì„ ì§‘ì¤‘     : {s['gaze_ratio']:5.1f}ì  x 0.3 = {s['score_focus']:4.1f}ì ")
        print(f"   3. ìì„¸ì•ˆì •     : {s['posture_ratio']:5.1f}ì  x 0.2 = {s['score_posture']:4.1f}ì ")
        print(f"   4. ì •ì„œì•ˆì •     : {100-s['avg_anxiety']:5.1f}ì  x 0.2 = {s['score_emotion']:4.1f}ì ")
        print(f"   -------------------------------------------")
        print(f"   âˆ‘ ìµœì¢… ì¢…í•© í•©ê³„: {s['overall_score']:.1f}ì ")
        print("="*50 + "\n")

    async def process_vision(self, frame, timestamp_ms):
        if not self.analyzer.is_ready:
            print(f"âš ï¸ [{self.session_id}] Vision Analyzer NOT READY", flush=True)
            return

        try:
            # print(f"[{self.session_id}] Processing frame at {timestamp_ms}", flush=True)
            img = frame.to_ndarray(format="bgr24")
            result = self.analyzer.process_frame(img, timestamp_ms)
            
            if result and result.get("status") == "detected":
                self.total_frames += 1
                
                # 1. í˜„ì¬ ì§ˆë¬¸ ë°ì´í„° ëˆ„ì 
                q = self.current_q_data
                q["total_frames"] += 1
                q["smile_scores"].append(result["scores"]["smile"])
                q["anxiety_scores"].append(result["scores"]["anxiety"])
                if result["flags"]["is_center"]: q["gaze_center_frames"] += 1
                if result["flags"]["is_stable"]: q["posture_stable_frames"] += 1

                # 2. [ë³€ê²½] ì „ì²´ ì„¸ì…˜ ë°ì´í„°ì—ë„ í†µí•© ëˆ„ì 
                a = self.session_all_data
                a["total_frames"] += 1
                a["smile_scores"].append(result["scores"]["smile"])
                a["anxiety_scores"].append(result["scores"]["anxiety"])
                if result["flags"]["is_center"]: a["gaze_center_frames"] += 1
                if result["flags"]["is_stable"]: a["posture_stable_frames"] += 1

                # [DEBUG] ì²« í”„ë ˆì„ ìˆ˜ì‹  ì‹œ ë¡œê·¸
                if self.total_frames == 1:
                    print(f"ğŸ“Š [{self.session_id}] Video capture started (Analyzing whole session...)", flush=True)

                current_time = time.time()
                if current_time - self.last_log_time > 1.5:
                    self.last_log_time = current_time
                    labels = result["labels"]
                    print(f"[{self.session_id}] Q{self.current_q_index} | ğŸ‘€ ì‹œì„ : {labels['gaze']} | ğŸ‘¤ ìì„¸: {labels['posture']} | ğŸ˜Š ë¯¸ì†Œ: {int(result['scores']['smile']*100)}%")

                ws = active_websockets.get(self.session_id)
                if ws:
                    await send_to_websocket(ws, {
                        "type": "vision_analysis",
                        "data": result,
                        "timestamp": current_time
                    })
        except Exception as e:
            logger.error(f"Vision analysis failed: {e}")

    async def recv(self):
        # MediaStreamTrack ì„œë¸Œí´ë˜ì‹± ìœ ì§€ (ì´í›„ í•„ìš” ì‹œ í™•ì¥ì„ ìœ„í•´)
        return await self.track.recv()

async def start_video_analysis(track, session_id):
    """ë¹„ë””ì˜¤ íŠ¸ë™ì„ ì§ì ‘ ì†Œë¹„í•˜ë©° ë¶„ì„í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ë£¨í”„ (ê°•ì œ í”„ë ˆì„ ìˆ˜ì‹ )"""
    print(f"ğŸ¬ [{session_id}] Video analysis background loop STARTED", flush=True)
    analysis_track = VideoAnalysisTrack(track, session_id)
    active_video_tracks[session_id] = analysis_track
    
    try:
        while True:
            frame = await track.recv()
            curr = time.time()
            # 10FPS (0.1s ê°„ê²©) ë¶„ì„
            if curr - analysis_track.last_tracking_time > 0.1:
                analysis_track.last_tracking_time = curr
                asyncio.create_task(analysis_track.process_vision(frame, int(curr * 1000)))
    except asyncio.CancelledError:
        pass
    except Exception as e:
        print(f"âš ï¸ [{session_id}] Video analysis loop error: {e}", flush=True)
    finally:
        print(f"ğŸ [{session_id}] Video analysis loop FINISHED", flush=True)
        if analysis_track.current_q_data["total_frames"] > 0:
            analysis_track._log_question_summary()
        analysis_track.generate_final_report()
        active_video_tracks.pop(session_id, None)

async def start_remote_stt(track, session_id):
    """
    AI-Workerì—ê²Œ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì „ì†¡í•˜ì—¬ STT ì²˜ë¦¬ (Remote STT)
    """
    logger.info(f"[{session_id}] Remote STT Task Loop Started")
    
    audio_buffer = []
    # 2ì´ˆ ë¶„ëŸ‰ ëª¨ì•„ì„œ ì „ì†¡ (ë¹ˆë²ˆí•œ Task ìƒì„± ë°©ì§€)
    # 16kHz, 16bit(2bytes), Mono -> 2ì´ˆ = 16000 * 2 * 2 = 64000 bytes
    BUFFER_SIZE = 64000 
    
    try:
        while True:
            frame = await track.recv()
            
            # 1. ë¦¬ìƒ˜í”Œë§ (WebRTC 48k -> Whisper 16k)
            resampler = av.AudioResampler(format='s16', layout='mono', rate=16000)
            resampled_frames = resampler.resample(frame)
            
            for f in resampled_frames:
                # av.AudioFrame.to_ndarray() -> numpy array
                # tobytes()ë¡œ raw bytes ì¶”ì¶œ
                data = f.to_ndarray().tobytes()
                audio_buffer.append(data)
                
            # 2. ë²„í¼ í¬ê¸° í™•ì¸
            current_size = sum(len(b) for b in audio_buffer)
            
            if current_size >= BUFFER_SIZE:
                # ì²­í¬ ë³‘í•©
                full_audio = b"".join(audio_buffer)
                audio_buffer = [] # ì´ˆê¸°í™”
                
                # Base64 ì¸ì½”ë”©
                b64_audio = base64.b64encode(full_audio).decode('utf-8')
                
                # 3. AI-Workerë¡œ Task ì „ì†¡
                # CeleryëŠ” ë¹„ë™ê¸°ì´ë¯€ë¡œ ì—¬ê¸°ì„œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  Taskë§Œ íì— ë„£ìŒ
                # í•„ìš” ì‹œ ê²°ê³¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³„ë„ ë©”ì»¤ë‹ˆì¦˜ í•„ìš” (ì˜ˆ: Taskê°€ ê²°ê³¼ DBì— ì“°ê³  Polling ë“±)
                task = celery_app.send_task(
                    "tasks.stt.recognize",
                    args=[b64_audio]
                )
                logger.debug(f"[{session_id}] Sent STT chunk to AI-Worker. Task ID: {task.id}")
                
                # (Optional) ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ë¡œ ê¸°ë‹¤ë¦¬ëŠ” ë¡œì§ì„ ì¶”ê°€í•˜ë ¤ë©´ asyncio.to_thread ë“± ì‚¬ìš©
                # í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ Celery RTTëŠ” ì§€ì—°ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ.
                
    except Exception as e:
        logger.error(f"[{session_id}] Remote STT Fail: {e}")
    finally:
        logger.info(f"[{session_id}] Remote STT Stopped")

async def send_to_websocket(ws: WebSocket, data: dict):
    """WebSocketìœ¼ë¡œ ë°ì´í„° ì „ì†¡"""
    try:
        await ws.send_json(data)
    except Exception as e:
        logger.error(f"WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")

# ============== WebSocket ì—”ë“œí¬ì¸íŠ¸ ==============
# [ì¶”ê°€ ë‚´ì—­: 2026-02-11]
# STT ì¤‘ê³„ í•¨ìˆ˜ (Remote STT)
# WebRTC ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ -> WAV íŒŒì¼ ë³€í™˜ -> AI Workerë¡œ ì „ì†¡
async def start_remote_stt(track, session_id):
    logger.info(f"[{session_id}] ğŸ™ï¸ ì›ê²© STT ì‹œì‘ (Remote STT Started)")
    
    # 3ì´ˆ ë‹¨ìœ„ë¡œ ì˜¤ë””ì˜¤ë¥¼ ëª¨ì•„ì„œ ì „ì†¡ (VAD ì—†ì´ ì‹œê°„ ê¸°ë°˜ ë¶„í• )
    CHUNK_DURATION_MS = 3000 
    accumulated_frames = []
    accumulated_time = 0
    
    try:
        while True:
            # 1. ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹ 
            frame = await track.recv()
            accumulated_frames.append(frame)
            
            # í”„ë ˆì„ ì‹œê°„ ëˆ„ì  (packet.duration ì‚¬ìš©í•˜ê±°ë‚˜ ê°œìˆ˜ë¡œ ì¶”ì •)
            # ë³´í†µ Opus í”„ë ˆì„ì€ 20ms or 60ms
            # ì—¬ê¸°ì„œëŠ” í”„ë ˆì„ ê°œìˆ˜ë¡œ ëŒ€ëµì ì¸ ì‹œê°„ ê³„ì‚° (50ê°œ = ì•½ 1ì´ˆ ê°€ì •)
            # ì •í™•ì„±ì„ ìœ„í•´ av.AudioFrame.time ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¨ìˆœí™”
            if len(accumulated_frames) >= 150: # ì•½ 3ì´ˆ (20ms * 150 = 3000ms)
                
                # 2. WAV ë³€í™˜ (In-Memory)
                # av ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Output Container ì‚¬ìš©
                output_buffer = io.BytesIO()
                output_container = av.open(output_buffer, mode='w', format='wav')
                output_stream = output_container.add_stream('pcm_s16le', rate=16000, layout='mono')
                
                for f in accumulated_frames:
                    # ë¦¬ìƒ˜í”Œë§ ë° íŒ¨í‚· ì‘ì„±
                    for packet in output_stream.encode(f):
                        output_container.mux(packet)
                        
                # 3. ë§ˆë¬´ë¦¬ (Flush)
                for packet in output_stream.encode(None):
                    output_container.mux(packet)
                output_container.close()
                
                # 4. Base64 ì¸ì½”ë”©
                wav_bytes = output_buffer.getvalue()
                audio_b64 = base64.b64encode(wav_bytes).decode('utf-8')
                
                # 5. Celery Task ë°°ë‹¬ (AI Workerì—ê²Œ)
                # ê²°ê³¼ê°’ì€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 'ë³´ëƒˆë‹¤'ëŠ” ì‚¬ì‹¤ë§Œ ì¤‘ìš”
                celery_app.send_task(
                    "tasks.stt.recognize",
                    args=[audio_b64],
                    queue="gpu_queue" # GPU ì›Œì»¤ ì „ìš© í ì‚¬ìš©
                )
                
                logger.info(f"[{session_id}] ğŸ“¤ ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡ ì™„ë£Œ ({len(wav_bytes)} bytes)")
                
                # ë²„í¼ ì´ˆê¸°í™”
                accumulated_frames = []

    except Exception as e:
        logger.info(f"[{session_id}] STT ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ: {e}")
    finally:
        logger.info(f"[{session_id}] STT ë¦¬ì†ŒìŠ¤ ì •ë¦¬")


@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    active_websockets[session_id] = websocket
    logger.info(f"[{session_id}] âœ… WebSocket ì—°ê²° ì„±ê³µ")
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸°
            data = await websocket.receive_text()
            try:
                msg = json.loads(data)
                # [ì¶”ê°€] ì§ˆë¬¸ ì „í™˜ ì‹ í˜¸ ì²˜ë¦¬
                if msg.get("type") == "next_question":
                    new_idx = msg.get("index", 0)
                    # [ë³€ê²½] active_video_tracksì—ì„œ ì§ì ‘ íŠ¸ë™ ì°¾ê¸°
                    video_track = active_video_tracks.get(session_id)
                    if video_track:
                        video_track.switch_question(new_idx)
            except json.JSONDecodeError:
                pass
            
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] âŒ WebSocket ì—°ê²° ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"[{session_id}] WebSocket ì—ëŸ¬: {e}")
    finally:
        if session_id in active_websockets:
            del active_websockets[session_id]
        if session_id in active_pcs:
            # PCëŠ” ë³„ë„ë¡œ ë‹«íˆì§€ ì•Šì•˜ì„ ê²½ìš°ë¥¼ ìœ„í•´ ìœ ì§€í•˜ê±°ë‚˜ ì¢…ë£Œ ì²˜ë¦¬ ê³ ë¯¼
            # ì—¬ê¸°ì„œëŠ” WebSocket ì¢…ë£Œ ì‹œ PCë„ ì •ë¦¬í•˜ë„ë¡ êµ¬í˜„
            pc = active_pcs.pop(session_id, None)
            if pc:
                await pc.close()
            logger.info(f"[{session_id}] ì„¸ì…˜ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ============== WebRTC ì—”ë“œí¬ì¸íŠ¸ ==============
@app.post("/offer")
async def offer(request: Request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
    session_id = params.get("session_id", "unknown")

    # STUN ì„œë²„ ì„¤ì •ì€ ìœ ì§€ (ë¹„ë””ì˜¤ ì—°ê²° ì•ˆì •ì„±ì„ ìœ„í•´)
    pc = RTCPeerConnection(
        configuration=RTCConfiguration(
            iceServers=[RTCIceServer(urls="stun:stun.l.google.com:19302")]
        )
    )
    active_pcs[session_id] = pc # [ì¶”ê°€] ì„¸ì…˜ë³„ PC ì €ì¥
    @pc.on("track")
    def on_track(track):
        logger.info(f"[{session_id}] Received track: {track.kind}")

        if track.kind == "audio":
            # [ë³€ê²½ ë‚´ì—­: 2026-02-11]
            # 1. ì´ì „ ì½”ë“œì˜ `start_stt_with_local_whisper` í•¨ìˆ˜ëŠ” ì •ì˜ë˜ì§€ ì•Šì•„ ì„œë²„ í¬ë˜ì‹œë¥¼ ìœ ë°œí–ˆìŠµë‹ˆë‹¤.
            # 2. ë¯¸ë””ì–´ ì„œë²„ì—ì„œ ëª¨ë¸ì„ ì§ì ‘ ëŒë¦¬ë©´ ë¹„ë””ì˜¤ ì¤‘ê³„ê°€ ë ‰ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
            #    ë¬´ê±°ìš´ STT ì‘ì—…ì€ ì „ìš© GPU ì›Œì»¤(AI-Worker)ì—ê²Œ ìœ„ì„(Delegate)í•©ë‹ˆë‹¤.
            asyncio.ensure_future(start_remote_stt(track, session_id))
            logger.info(f"[{session_id}] Audio track processing started (Remote STT via AI-Worker)")
            
        elif track.kind == "video":
            # ë¹„ë””ì˜¤ íŠ¸ë™: ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ë£¨í”„ ì‹œì‘ (addTrack ëŒ€ì‹  ì§ì ‘ ì†Œë¹„)
            asyncio.ensure_future(start_video_analysis(relay.subscribe(track), session_id))
            logger.info(f"[{session_id}] Video analysis loop scheduled")

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

async def consume_audio(track):
    """ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì†Œë¹„í•˜ì—¬ ë²„í¼ê°€ ì°¨ì§€ ì•Šë„ë¡ í•¨"""
    try:
        while True:
            await track.recv()
    except Exception:
        # íŠ¸ë™ì´ ì¢…ë£Œë˜ë©´ ì˜ˆì™¸ ë°œìƒ (ì •ìƒì ì¸ ì¢…ë£Œ)
        pass

@app.get("/")
async def root():
    return {
        "service": "AI Interview Media Server",
        "status": "running",
        "mode": "Video Analysis + Remote STT (via AI-Worker)"
    }

# [ë³µêµ¬: 2026-02-12]
# EnvTestPage.jsx í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸
from fastapi import UploadFile, File, HTTPException

@app.post("/stt/recognize")
async def stt_recognize(file: UploadFile = File(...)):
    """
    STT í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸ (EnvTestPage.jsxì—ì„œ í˜¸ì¶œ)
    """
    try:
        audio_bytes = await file.read()
        audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')
        
        task = celery_app.send_task(
            "tasks.stt.recognize",
            args=[audio_b64],
            queue="gpu_queue"
        )
        # í…ŒìŠ¤íŠ¸ìš©ì´ë¯€ë¡œ ê²°ê³¼ ëŒ€ê¸°
        result = task.get(timeout=30)
        return result
    except Exception as e:
        logger.error(f"STT Test Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.on_event("startup")
async def on_startup():
    print("ğŸš€ [Media-Server] FastAPI startup complete. Port 8080 is now open.", flush=True)
    # ì„œë²„ ê¸°ë™ ì§í›„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ë¡œë”© ì‹œì‘ (ë¹„ë¸”ë¡œí‚¹)
    asyncio.create_task(background_init_analyzer())

@app.get("/status")
async def status():
    is_ready = analyzer_instance.is_ready if analyzer_instance else False
    return {
        "status": "running",
        "vision_analyzer_ready": is_ready,
        "session_count": len(active_pcs)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")