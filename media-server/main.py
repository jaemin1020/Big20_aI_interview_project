import asyncio
import json
import logging
import os
import base64
import time
import cv2
from typing import Dict, Set
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack, RTCConfiguration, RTCIceServer
from aiortc.contrib.media import MediaRelay
from celery import Celery
import av
from vision_analyzer import VisionAnalyzer  # [NEW] MediaPipe Vision Analyzer
import io  # [NEW] ì˜¤ë””ì˜¤ ë²„í¼ë§ìš©

# 1. ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger("Media-Server")

app = FastAPI()

# CORS ì„¤ì •
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://127.0.0.1:3000",
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "http://127.0.0.1:5173"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

relay = MediaRelay()

# 2. Celery ì„¤ì •
redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
celery_app = Celery("ai_worker", broker=redis_url, backend=redis_url)

# 3. WebSocket ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„ WebSocket ì €ì¥)
active_websockets: Dict[str, WebSocket] = {}

# 4. Local Whisper ì„¤ì • (Removed: Delegated to AI-Worker)
# WHISPER_MODEL = None

class VideoAnalysisTrack(MediaStreamTrack):
    """ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ai-workerì— ê°ì • ë¶„ì„ì„ ìš”ì²­í•˜ëŠ” íŠ¸ë™"""
    kind = "video"

    def __init__(self, track, session_id):
        super().__init__()
        self.track = track
        self.session_id = session_id
        self.last_frame_time = 0

        self.last_frame_time = 0

        # [ë³€ê²½ ë‚´ì—­: 2026-02-11]
        # ì´ì „ ì½”ë“œ (Legacy):
        # self.face_cascade = cv2.CascadeClassifier(...) -> OpenCV Haar Cascade ì‚¬ìš© (êµ¬í˜•, CPU ë¶€í•˜ ë†’ìŒ)
        # self.eye_cascade = cv2.CascadeClassifier(...)
        #
        # ë³€ê²½ ì½”ë“œ (New):
        # self.analyzer = VisionAnalyzer() -> MediaPipe ê¸°ë°˜ ìµœì‹  ë¶„ì„ê¸° ì‚¬ìš©
        #
        # ë³€ê²½ ì´ìœ :
        # 1. 3D Face Landmark (478ê°œ ì ) ì¶”ì ìœ¼ë¡œ ì •ë°€ë„ í–¥ìƒ
        # 2. ê°ì •(Blendshapes), ì‹œì„ , ìì„¸ ë¶„ì„ì„ í•œ ë²ˆì˜ ì¶”ë¡ ìœ¼ë¡œ í†µí•© (íš¨ìœ¨ì„±)
        # 3. GPU/CPU ìµœì í™”ëœ MediaPipe ì‚¬ìš©ìœ¼ë¡œ ì‹¤ì‹œê°„ì„± í™•ë³´
        self.analyzer = VisionAnalyzer()
        logger.info(f"[{session_id}] VideoAnalysisTrack initialized with MediaPipe")


    async def process_vision(self, frame, timestamp_ms):
        """WebRTC í”„ë ˆì„ -> MediaPipe ë¶„ì„ -> WebSocket ì „ì†¡"""
        # [ë³€ê²½ ë‚´ì—­: 2026-02-11]
        # ì´ì „ í•¨ìˆ˜ëª…: process_eye_tracking
        # ì´ì „ ë¡œì§: OpenCVë¡œ ì–¼êµ´/ëˆˆ ì‚¬ê°í˜•ë§Œ ì°¾ì•„ì„œ ì¢Œí‘œ ë³´ëƒ„. ê°ì • ë¶„ì„ì€ ë³„ë„ë¡œ Celery íƒœìŠ¤í¬ë¡œ ë³´ëƒ„.
        #
        # ë³€ê²½ ë¡œì§:
        # 1. process_visionìœ¼ë¡œ í†µí•©.
        # 2. MediaPipeê°€ ì–¼êµ´+ëˆˆ+ê°ì •+ìì„¸ë¥¼ í•œ ë²ˆì— ë¶„ì„.
        # 3. WebSocketìœ¼ë¡œ 'vision_analysis'ë¼ëŠ” í†µí•©ëœ ë°ì´í„° ì „ì†¡.
        try:
            # OpenCV í¬ë§· ë³€í™˜
            img = frame.to_ndarray(format="bgr24")
            
            # [NEW] MediaPipe ë¶„ì„ ì‹¤í–‰
            result = self.analyzer.process_frame(img, timestamp_ms)
            
            if result:
                # 1. í„°ë¯¸ë„ ë¡œê·¸ (ë””ë²„ê¹…ìš©, 2ì´ˆë§ˆë‹¤)
                current_time = time.time()
                if current_time - getattr(self, 'last_log_time', 0) > 2.0:
                    self.last_log_time = current_time
                    logger.info(f"[{self.session_id}] Vision: {result['emotion']} / {result['gaze']} (Smile: {result['scores']['smile']})")

                # 2. WebSocket ì „ì†¡ (í”„ë¡ íŠ¸ì—”ë“œ ì‹œê°í™”ìš©)
                ws = active_websockets.get(self.session_id)
                if ws:
                    await send_to_websocket(ws, {
                        "type": "vision_analysis", # í†µí•©ëœ ë¹„ì „ ë°ì´í„° íƒ€ì…
                        "data": result,
                        "timestamp": current_time
                    })
        except Exception as e:
            logger.error(f"Vision analysis failed: {e}")

    async def recv(self):
        frame = await self.track.recv()
        current_time = time.time()

        # 1. ë¹„ì „ ë¶„ì„ (ì‹¤ì‹œê°„ì„± ì¤‘ìš” - 0.1ì´ˆë§ˆë‹¤ ìˆ˜í–‰)
        if current_time - getattr(self, 'last_tracking_time', 0) > 0.1:
            self.last_tracking_time = current_time
            # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ì—¬ ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ì§€ì—° ë°©ì§€
            # timestampìš©ìœ¼ë¡œ time.time() * 1000 ì‚¬ìš©
            asyncio.create_task(self.process_vision(frame, int(current_time * 1000)))

        # 2. (êµ¬ë²„ì „) ê°ì • ë¶„ì„ íƒœìŠ¤í¬ í˜¸ì¶œ ì œê±°
        # MediaPipeê°€ ê°ì •ê¹Œì§€ ë‹¤ í•˜ë¯€ë¡œ ë” ì´ìƒ í•„ìš” ì—†ìŒ.
        # if current_time - self.last_frame_time > 2.0: ...

        return frame

async def start_remote_stt(track, session_id):
    """
    AI-Workerì—ê²Œ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì „ì†¡í•˜ì—¬ STT ì²˜ë¦¬ (Remote STT)
    """
    logger.info(f"[{session_id}] Remote STT Task Loop Started")
    
    audio_buffer = []
    # 2ì´ˆ ë¶„ëŸ‰ ëª¨ì•„ì„œ ì „ì†¡ (ë¹ˆë²ˆí•œ Task ìƒì„± ë°©ì§€)
    # 16kHz, 16bit(2bytes), Mono -> 2ì´ˆ = 16000 * 2 * 2 = 64000 bytes
    BUFFER_SIZE = 64000 
    
    try:
        while True:
            frame = await track.recv()
            
            # 1. ë¦¬ìƒ˜í”Œë§ (WebRTC 48k -> Whisper 16k)
            resampler = av.AudioResampler(format='s16', layout='mono', rate=16000)
            resampled_frames = resampler.resample(frame)
            
            for f in resampled_frames:
                # av.AudioFrame.to_ndarray() -> numpy array
                # tobytes()ë¡œ raw bytes ì¶”ì¶œ
                data = f.to_ndarray().tobytes()
                audio_buffer.append(data)
                
            # 2. ë²„í¼ í¬ê¸° í™•ì¸
            current_size = sum(len(b) for b in audio_buffer)
            
            if current_size >= BUFFER_SIZE:
                # ì²­í¬ ë³‘í•©
                full_audio = b"".join(audio_buffer)
                audio_buffer = [] # ì´ˆê¸°í™”
                
                # Base64 ì¸ì½”ë”©
                b64_audio = base64.b64encode(full_audio).decode('utf-8')
                
                # 3. AI-Workerë¡œ Task ì „ì†¡
                # CeleryëŠ” ë¹„ë™ê¸°ì´ë¯€ë¡œ ì—¬ê¸°ì„œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  Taskë§Œ íì— ë„£ìŒ
                # í•„ìš” ì‹œ ê²°ê³¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³„ë„ ë©”ì»¤ë‹ˆì¦˜ í•„ìš” (ì˜ˆ: Taskê°€ ê²°ê³¼ DBì— ì“°ê³  Polling ë“±)
                task = celery_app.send_task(
                    "tasks.stt.recognize",
                    args=[b64_audio]
                )
                logger.debug(f"[{session_id}] Sent STT chunk to AI-Worker. Task ID: {task.id}")
                
                # (Optional) ê²°ê³¼ë¥¼ ë¹„ë™ê¸°ë¡œ ê¸°ë‹¤ë¦¬ëŠ” ë¡œì§ì„ ì¶”ê°€í•˜ë ¤ë©´ asyncio.to_thread ë“± ì‚¬ìš©
                # í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œ Celery RTTëŠ” ì§€ì—°ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ.
                
    except Exception as e:
        logger.error(f"[{session_id}] Remote STT Fail: {e}")
    finally:
        logger.info(f"[{session_id}] Remote STT Stopped")

async def send_to_websocket(ws: WebSocket, data: dict):
    """WebSocketìœ¼ë¡œ ë°ì´í„° ì „ì†¡"""
    try:
        await ws.send_json(data)
    except Exception as e:
        logger.error(f"WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")

# ============== WebSocket ì—”ë“œí¬ì¸íŠ¸ ==============
# [ì¶”ê°€ ë‚´ì—­: 2026-02-11]
# STT ì¤‘ê³„ í•¨ìˆ˜ (Remote STT)
# WebRTC ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ -> WAV íŒŒì¼ ë³€í™˜ -> AI Workerë¡œ ì „ì†¡
async def start_remote_stt(track, session_id):
    logger.info(f"[{session_id}] ğŸ™ï¸ ì›ê²© STT ì‹œì‘ (Remote STT Started)")
    
    # 3ì´ˆ ë‹¨ìœ„ë¡œ ì˜¤ë””ì˜¤ë¥¼ ëª¨ì•„ì„œ ì „ì†¡ (VAD ì—†ì´ ì‹œê°„ ê¸°ë°˜ ë¶„í• )
    CHUNK_DURATION_MS = 3000 
    accumulated_frames = []
    accumulated_time = 0
    
    try:
        while True:
            # 1. ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹ 
            frame = await track.recv()
            accumulated_frames.append(frame)
            
            # í”„ë ˆì„ ì‹œê°„ ëˆ„ì  (packet.duration ì‚¬ìš©í•˜ê±°ë‚˜ ê°œìˆ˜ë¡œ ì¶”ì •)
            # ë³´í†µ Opus í”„ë ˆì„ì€ 20ms or 60ms
            # ì—¬ê¸°ì„œëŠ” í”„ë ˆì„ ê°œìˆ˜ë¡œ ëŒ€ëµì ì¸ ì‹œê°„ ê³„ì‚° (50ê°œ = ì•½ 1ì´ˆ ê°€ì •)
            # ì •í™•ì„±ì„ ìœ„í•´ av.AudioFrame.time ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ ë‹¨ìˆœí™”
            if len(accumulated_frames) >= 150: # ì•½ 3ì´ˆ (20ms * 150 = 3000ms)
                
                # 2. WAV ë³€í™˜ (In-Memory)
                # av ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Output Container ì‚¬ìš©
                output_buffer = io.BytesIO()
                output_container = av.open(output_buffer, mode='w', format='wav')
                output_stream = output_container.add_stream('pcm_s16le', rate=16000, layout='mono')
                
                for f in accumulated_frames:
                    # ë¦¬ìƒ˜í”Œë§ ë° íŒ¨í‚· ì‘ì„±
                    for packet in output_stream.encode(f):
                        output_container.mux(packet)
                        
                # 3. ë§ˆë¬´ë¦¬ (Flush)
                for packet in output_stream.encode(None):
                    output_container.mux(packet)
                output_container.close()
                
                # 4. Base64 ì¸ì½”ë”©
                wav_bytes = output_buffer.getvalue()
                audio_b64 = base64.b64encode(wav_bytes).decode('utf-8')
                
                # 5. Celery Task ë°°ë‹¬ (AI Workerì—ê²Œ)
                # ê²°ê³¼ê°’ì€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” 'ë³´ëƒˆë‹¤'ëŠ” ì‚¬ì‹¤ë§Œ ì¤‘ìš”
                celery_app.send_task(
                    "tasks.stt.recognize",
                    args=[audio_b64],
                    queue="gpu_queue" # GPU ì›Œì»¤ ì „ìš© í ì‚¬ìš©
                )
                
                logger.info(f"[{session_id}] ğŸ“¤ ì˜¤ë””ì˜¤ ì²­í¬ ì „ì†¡ ì™„ë£Œ ({len(wav_bytes)} bytes)")
                
                # ë²„í¼ ì´ˆê¸°í™”
                accumulated_frames = []

    except Exception as e:
        logger.info(f"[{session_id}] STT ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ: {e}")
    finally:
        logger.info(f"[{session_id}] STT ë¦¬ì†ŒìŠ¤ ì •ë¦¬")


@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    active_websockets[session_id] = websocket
    logger.info(f"[{session_id}] âœ… WebSocket ì—°ê²° ì„±ê³µ")
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸° (í˜„ì¬ëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ ì—†ìŒ)
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] âŒ WebSocket ì—°ê²° ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"[{session_id}] WebSocket ì—ëŸ¬: {e}")
    finally:
        # ì—°ê²° ì¢…ë£Œ ì‹œ ì„¸ì…˜ ì œê±°
        if session_id in active_websockets:
            del active_websockets[session_id]
            logger.info(f"[{session_id}] WebSocket ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")

# ============== WebRTC ì—”ë“œí¬ì¸íŠ¸ ==============
@app.post("/offer")
async def offer(request: Request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
    session_id = params.get("session_id", "unknown")

    # STUN ì„œë²„ ì„¤ì •ì€ ìœ ì§€ (ë¹„ë””ì˜¤ ì—°ê²° ì•ˆì •ì„±ì„ ìœ„í•´)
    pc = RTCPeerConnection(
        configuration=RTCConfiguration(
            iceServers=[RTCIceServer(urls="stun:stun.l.google.com:19302")]
        )
    )
    @pc.on("track")
    def on_track(track):
        logger.info(f"[{session_id}] Received track: {track.kind}")

        if track.kind == "audio":
            # [ë³€ê²½ ë‚´ì—­: 2026-02-11]
            # 1. ì´ì „ ì½”ë“œì˜ `start_stt_with_local_whisper` í•¨ìˆ˜ëŠ” ì •ì˜ë˜ì§€ ì•Šì•„ ì„œë²„ í¬ë˜ì‹œë¥¼ ìœ ë°œí–ˆìŠµë‹ˆë‹¤.
            # 2. ë¯¸ë””ì–´ ì„œë²„ì—ì„œ ëª¨ë¸ì„ ì§ì ‘ ëŒë¦¬ë©´ ë¹„ë””ì˜¤ ì¤‘ê³„ê°€ ë ‰ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
            #    ë¬´ê±°ìš´ STT ì‘ì—…ì€ ì „ìš© GPU ì›Œì»¤(AI-Worker)ì—ê²Œ ìœ„ì„(Delegate)í•©ë‹ˆë‹¤.
            asyncio.ensure_future(start_remote_stt(track, session_id))
            logger.info(f"[{session_id}] Audio track processing started (Remote STT via AI-Worker)")
            
        elif track.kind == "video":
            # ë¹„ë””ì˜¤ íŠ¸ë™: ê°ì • ë¶„ì„ ì²˜ë¦¬
            pc.addTrack(VideoAnalysisTrack(relay.subscribe(track), session_id))
            logger.info(f"[{session_id}] Video analysis track added")

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

async def consume_audio(track):
    """ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì†Œë¹„í•˜ì—¬ ë²„í¼ê°€ ì°¨ì§€ ì•Šë„ë¡ í•¨"""
    try:
        while True:
            await track.recv()
    except Exception:
        # íŠ¸ë™ì´ ì¢…ë£Œë˜ë©´ ì˜ˆì™¸ ë°œìƒ (ì •ìƒì ì¸ ì¢…ë£Œ)
        pass

@app.get("/")
async def root():
    return {
        "service": "AI Interview Media Server",
        "status": "running",
        "mode": "Video Analysis + Remote STT (via AI-Worker)"
    }

# [ë³µêµ¬: 2026-02-12]
# EnvTestPage.jsx í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸
from fastapi import UploadFile, File, HTTPException

@app.post("/stt/recognize")
async def stt_recognize(file: UploadFile = File(...)):
    """
    STT í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸ (EnvTestPage.jsxì—ì„œ í˜¸ì¶œ)
    """
    try:
        audio_bytes = await file.read()
        audio_b64 = base64.b64encode(audio_bytes).decode('utf-8')
        
        task = celery_app.send_task(
            "tasks.stt.recognize",
            args=[audio_b64],
            queue="gpu_queue"
        )
        # í…ŒìŠ¤íŠ¸ìš©ì´ë¯€ë¡œ ê²°ê³¼ ëŒ€ê¸°
        result = task.get(timeout=30)
        return result
    except Exception as e:
        logger.error(f"STT Test Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")
