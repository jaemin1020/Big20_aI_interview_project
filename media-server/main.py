import asyncio
import json
import logging
import os
import base64
import time
import cv2
from typing import Dict, Set
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack, RTCConfiguration, RTCIceServer
from aiortc.contrib.media import MediaRelay
from celery import Celery
import av

# 1. ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger("Media-Server")

app = FastAPI()

# CORS ì„¤ì •
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://127.0.0.1:3000",
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "http://127.0.0.1:5173"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

relay = MediaRelay()

# 2. Celery ì„¤ì •
redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
celery_app = Celery("ai_worker", broker=redis_url, backend=redis_url)

# 3. WebSocket ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„ WebSocket ì €ì¥)
active_websockets: Dict[str, WebSocket] = {}

class VideoAnalysisTrack(MediaStreamTrack):
    """ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ai-workerì— ê°ì • ë¶„ì„ì„ ìš”ì²­í•˜ëŠ” íŠ¸ë™"""
    kind = "video"

    def __init__(self, track, session_id):
        super().__init__()
        self.track = track
        self.session_id = session_id
        self.last_frame_time = 0

        # Haar Cascade Load
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')


    async def process_eye_tracking(self, frame):
        """WebRTC í”„ë ˆì„ì—ì„œ ëˆˆ/ì–¼êµ´ ì¶”ì  í›„ WebSocket ì „ì†¡"""
        try:
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            img = frame.to_ndarray(format="bgr24")
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)
            
            tracking_data = []
            
            for (x, y, w, h) in faces:
                roi_gray = gray[y:y+h, x:x+w]
                eyes = self.eye_cascade.detectMultiScale(roi_gray)
                
                eyes_coords = []
                for (ex, ey, ew, eh) in eyes:
                    eyes_coords.append({
                        "x": int(x + ex),
                        "y": int(y + ey),
                        "w": int(ew),
                        "h": int(eh)
                    })
                
                tracking_data.append({
                    "face": {"x": int(x), "y": int(y), "w": int(w), "h": int(h)},
                    "eyes": eyes_coords
                })

            # Status determination
            status = "not_detected"
            if len(tracking_data) > 0:
                face = tracking_data[0] # Assuming first face
                num_eyes = len(face["eyes"])
                
                if num_eyes >= 2:
                    status = "focused"
                elif num_eyes == 1:
                    status = "partially_detected"
                else:
                    status = "eyes_not_visible"
            
            # Log status (throttled)
            current_time = time.time()
            if current_time - getattr(self, 'last_log_time', 0) > 2.0: # Log every 2 seconds
                self.last_log_time = current_time
                logger.info(f"[{self.session_id}] Eye Tracking Status: {status} (Faces: {len(faces)})")

            # WebSocketìœ¼ë¡œ ì „ì†¡
            ws = active_websockets.get(self.session_id)
            if ws:
                await send_to_websocket(ws, {
                    "type": "eye_tracking",
                    "data": tracking_data,
                    "status": status  # Send status to frontend as well
                })

        except Exception as e:
            logger.error(f"Eye tracking frame failed: {e}")

    async def recv(self):
        frame = await self.track.recv()
        current_time = time.time()

        # 1. ëˆˆ ì¶”ì  (ì‹¤ì‹œê°„ì„± ì¤‘ìš” - 0.1ì´ˆë§ˆë‹¤ ìˆ˜í–‰)
        # ëª¨ë“  í”„ë ˆì„ì„ í•˜ë©´ ë¶€í•˜ê°€ í´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°„ê²© ì¡°ì ˆ
        if current_time - getattr(self, 'last_tracking_time', 0) > 0.1:
            self.last_tracking_time = current_time
            # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ì—¬ ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ì§€ì—° ë°©ì§€
            asyncio.create_task(self.process_eye_tracking(frame))

        # 2. ê°ì • ë¶„ì„ (ë¬´ê±°ìš´ ì‘ì—… - 2ì´ˆë§ˆë‹¤ ìˆ˜í–‰)
        if current_time - self.last_frame_time > 2.0:
            self.last_frame_time = current_time
            
            # í”„ë ˆì„ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            img = frame.to_ndarray(format="bgr24")
            _, buffer = cv2.imencode('.jpg', img)
            base64_img = base64.b64encode(buffer).decode('utf-8')

            # ai-workerì— ë¹„ë™ê¸° ê°ì • ë¶„ì„ íƒœìŠ¤í¬ ì „ë‹¬
            celery_app.send_task(
                "tasks.vision.analyze_emotion",
                args=[self.session_id, base64_img]
            )
            # ëˆˆ ì¶”ì  Taskë„ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ì €ì¥ (ì„ íƒì )
            # celery_app.send_task("tasks.vision.track_eyes", args=[self.session_id, base64_img])

        return frame

async def start_remote_stt(track, session_id):
    """
    AI-Workerì—ê²Œ ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ ì „ì†¡í•˜ì—¬ STT ì²˜ë¦¬ (Remote STT)
    """
    logger.info(f"[{session_id}] Remote STT Task Loop Started")
    
    audio_buffer = []
    # 2ì´ˆ ë¶„ëŸ‰ ëª¨ì•„ì„œ ì „ì†¡ (ë¹ˆë²ˆí•œ Task ìƒì„± ë°©ì§€)
    # 16kHz, 16bit(2bytes), Mono -> 2ì´ˆ = 16000 * 2 * 2 = 64000 bytes
    BUFFER_SIZE = 64000 
    
    try:
        while True:
            frame = await track.recv()
            
            # 1. ë¦¬ìƒ˜í”Œë§ (WebRTC 48k -> Whisper 16k)
            resampler = av.AudioResampler(format='s16', layout='mono', rate=16000)
            resampled_frames = resampler.resample(frame)
            
            for f in resampled_frames:
                # av.AudioFrame.to_ndarray() -> numpy array
                # tobytes()ë¡œ raw bytes ì¶”ì¶œ
                data = f.to_ndarray().tobytes()
                audio_buffer.append(data)
                
            # 2. ë²„í¼ í¬ê¸° í™•ì¸
            current_size = sum(len(b) for b in audio_buffer)
            
            if current_size >= BUFFER_SIZE:
                # ì²­í¬ ë³‘í•©
                full_audio = b"".join(audio_buffer)
                audio_buffer = [] # ì´ˆê¸°í™”
                
                # Base64 ì¸ì½”ë”©
                b64_audio = base64.b64encode(full_audio).decode('utf-8')
                
                # 3. AI-Workerë¡œ Task ì „ì†¡
                # CeleryëŠ” ë¹„ë™ê¸°ì´ë¯€ë¡œ ì—¬ê¸°ì„œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  Taskë§Œ íì— ë„£ìŒ
                # í•„ìš” ì‹œ ê²°ê³¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë³„ë„ ë©”ì»¤ë‹ˆì¦˜ í•„ìš” (ì˜ˆ: Taskê°€ ê²°ê³¼ DBì— ì“°ê³  Polling ë“±)
                task = celery_app.send_task(
                    "tasks.stt.recognize",
                    args=[b64_audio]
                )
                logger.debug(f"[{session_id}] Sent STT chunk to AI-Worker. Task ID: {task.id}")
                
                # [ì¶”ê°€] STT ê²°ê³¼ ëŒ€ê¸° ë° WebSocket ì „ì†¡
                async def wait_for_stt_result(task_id, sid):
                    try:
                        # Celery AsyncResultë¡œ ê²°ê³¼ ì¶”ì 
                        from celery.result import AsyncResult
                        res = AsyncResult(task_id, app=celery_app)
                        
                        # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
                        start_wait = time.time()
                        while not res.ready():
                            await asyncio.sleep(0.1)
                            if time.time() - start_wait > 5.0:
                                return
                        
                        text_result = res.result
                        if text_result and text_result.strip():
                            ws = active_websockets.get(sid)
                            if ws:
                                await send_to_websocket(ws, {
                                    "type": "stt_result",
                                    "text": text_result
                                })
                                logger.info(f"[{sid}] ğŸ¤ STT Result Sent: {text_result[:30]}...")
                    except Exception as ex:
                        logger.error(f"[{sid}] Error waiting for STT result: {ex}")

                # ë¹„ë™ê¸°ì ìœ¼ë¡œ ê²°ê³¼ ëŒ€ê¸° ë£¨í”„ ì‹¤í–‰ (ë©”ì¸ ë£¨í”„ ì°¨ë‹¨ ë°©ì§€)
                asyncio.create_task(wait_for_stt_result(task.id, session_id))
                
    except Exception as e:
        logger.error(f"[{session_id}] Remote STT Fail: {e}")
    finally:
        logger.info(f"[{session_id}] Remote STT Stopped")

async def send_to_websocket(ws: WebSocket, data: dict):
    """WebSocketìœ¼ë¡œ ë°ì´í„° ì „ì†¡"""
    try:
        await ws.send_json(data)
    except Exception as e:
        logger.error(f"WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")

# ============== WebSocket ì—”ë“œí¬ì¸íŠ¸ ==============
@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    active_websockets[session_id] = websocket
    logger.info(f"[{session_id}] âœ… WebSocket ì—°ê²° ì„±ê³µ")
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸° (í˜„ì¬ëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ ì—†ìŒ)
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] âŒ WebSocket ì—°ê²° ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"[{session_id}] WebSocket ì—ëŸ¬: {e}")
    finally:
        # ì—°ê²° ì¢…ë£Œ ì‹œ ì„¸ì…˜ ì œê±°
        if session_id in active_websockets:
            del active_websockets[session_id]
            logger.info(f"[{session_id}] WebSocket ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")

# ============== WebRTC ì—”ë“œí¬ì¸íŠ¸ ==============
@app.post("/offer")
async def offer(request: Request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
    session_id = params.get("session_id", "unknown")

    # STUN ì„œë²„ ì„¤ì •ì€ ìœ ì§€ (ë¹„ë””ì˜¤ ì—°ê²° ì•ˆì •ì„±ì„ ìœ„í•´)
    pc = RTCPeerConnection(
        configuration=RTCConfiguration(
            iceServers=[RTCIceServer(urls="stun:stun.l.google.com:19302")]
        )
    )
    logger.info(f"[{session_id}] WebRTC ì—°ê²° ì‹œë„")

    @pc.on("track")
    def on_track(track):
        logger.info(f"[{session_id}] Received track: {track.kind}")
        if track.kind == "video":
            # ë¹„ë””ì˜¤ íŠ¸ë™: ê°ì • ë¶„ì„ ì²˜ë¦¬
            pc.addTrack(VideoAnalysisTrack(relay.subscribe(track), session_id))
            logger.info(f"[{session_id}] Video analysis track added")
        elif track.kind == "audio":
            # [ë³€ê²½] AI Workerë¡œ ìœ„ì„ (Remote STT)
            asyncio.ensure_future(start_remote_stt(track, session_id))
            logger.info(f"[{session_id}] Audio track processing started (Remote STT)")
        else:
            logger.warning(f"[{session_id}] Unknown track type: {track.kind}")

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

async def consume_audio(track):
    """ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì†Œë¹„í•˜ì—¬ ë²„í¼ê°€ ì°¨ì§€ ì•Šë„ë¡ í•¨"""
    try:
        while True:
            await track.recv()
    except Exception:
        # íŠ¸ë™ì´ ì¢…ë£Œë˜ë©´ ì˜ˆì™¸ ë°œìƒ (ì •ìƒì ì¸ ì¢…ë£Œ)
        pass

@app.get("/")
async def root():
    return {
        "service": "AI Interview Media Server",
        "status": "running",
        "mode": "Video Analysis + Remote STT (via AI-Worker)"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")