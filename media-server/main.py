import asyncio
import json
import logging
import os
import base64
import time
import cv2
from typing import Dict, Set
from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack, RTCConfiguration, RTCIceServer
from aiortc.contrib.media import MediaRelay
from celery import Celery
import av
import numpy as np
from faster_whisper import WhisperModel

# 1. ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger("Media-Server")

app = FastAPI()

# CORS ì„¤ì •
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://127.0.0.1:3000",
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "http://127.0.0.1:5173"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

relay = MediaRelay()

# 2. Celery ì„¤ì •
redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
celery_app = Celery("ai_worker", broker=redis_url, backend=redis_url)

# 3. WebSocket ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„ WebSocket ì €ì¥)
active_websockets: Dict[str, WebSocket] = {}

# 4. Local Whisper ì„¤ì •
WHISPER_MODEL = None
LOCAL_MODEL_SIZE = "large-v3-turbo" # or small, medium, etc.

def load_local_whisper():
    global WHISPER_MODEL
    try:
        if WHISPER_MODEL is None:
            logger.info(f"â³ Loading Local Whisper Model ({LOCAL_MODEL_SIZE})...")
            # Run on GPU with FP16
            WHISPER_MODEL = WhisperModel(LOCAL_MODEL_SIZE, device="cuda", compute_type="float16")
            logger.info("âœ… Local Whisper Model Loaded")
    except Exception as e:
        logger.error(f"âŒ Failed to load Local Whisper: {e}")

import threading
class VideoAnalysisTrack(MediaStreamTrack):
    """ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ai-workerì— ê°ì • ë¶„ì„ì„ ìš”ì²­í•˜ëŠ” íŠ¸ë™"""
    kind = "video"

    def __init__(self, track, session_id):
        super().__init__()
        self.track = track
        self.session_id = session_id
        self.last_frame_time = 0

        # Haar Cascade Load
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')



    async def process_eye_tracking(self, frame):
        """WebRTC í”„ë ˆì„ì—ì„œ ëˆˆ/ì–¼êµ´ ì¶”ì  í›„ WebSocket ì „ì†¡"""
        try:
            # OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            img = frame.to_ndarray(format="bgr24")
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)
            
            tracking_data = []
            
            for (x, y, w, h) in faces:
                roi_gray = gray[y:y+h, x:x+w]
                eyes = self.eye_cascade.detectMultiScale(roi_gray)
                
                eyes_coords = []
                for (ex, ey, ew, eh) in eyes:
                    eyes_coords.append({
                        "x": int(x + ex),
                        "y": int(y + ey),
                        "w": int(ew),
                        "h": int(eh)
                    })
                
                tracking_data.append({
                    "face": {"x": int(x), "y": int(y), "w": int(w), "h": int(h)},
                    "eyes": eyes_coords
                })

            # Status determination
            status = "not_detected"
            if len(tracking_data) > 0:
                face = tracking_data[0] # Assuming first face
                num_eyes = len(face["eyes"])
                
                if num_eyes >= 2:
                    status = "focused"
                elif num_eyes == 1:
                    status = "partially_detected"
                else:
                    status = "eyes_not_visible"
            
            # Log status (throttled)
            current_time = time.time()
            if current_time - getattr(self, 'last_log_time', 0) > 2.0: # Log every 2 seconds
                self.last_log_time = current_time
                logger.info(f"[{self.session_id}] Eye Tracking Status: {status} (Faces: {len(faces)})")

            # WebSocketìœ¼ë¡œ ì „ì†¡
            ws = active_websockets.get(self.session_id)
            if ws:
                await send_to_websocket(ws, {
                    "type": "eye_tracking",
                    "data": tracking_data,
                    "status": status  # Send status to frontend as well
                })

        except Exception as e:
            logger.error(f"Eye tracking frame failed: {e}")

    async def recv(self):
        frame = await self.track.recv()
        current_time = time.time()

        # 1. ëˆˆ ì¶”ì  (ì‹¤ì‹œê°„ì„± ì¤‘ìš” - 0.1ì´ˆë§ˆë‹¤ ìˆ˜í–‰)
        # ëª¨ë“  í”„ë ˆì„ì„ í•˜ë©´ ë¶€í•˜ê°€ í´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°„ê²© ì¡°ì ˆ
        if current_time - getattr(self, 'last_tracking_time', 0) > 0.1:
            self.last_tracking_time = current_time
            # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ì—¬ ë©”ì¸ ìŠ¤íŠ¸ë¦¼ ì§€ì—° ë°©ì§€
            asyncio.create_task(self.process_eye_tracking(frame))

        # 2. ê°ì • ë¶„ì„ (ë¬´ê±°ìš´ ì‘ì—… - 2ì´ˆë§ˆë‹¤ ìˆ˜í–‰)
        if current_time - self.last_frame_time > 2.0:
            self.last_frame_time = current_time
            
            # í”„ë ˆì„ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            img = frame.to_ndarray(format="bgr24")
            _, buffer = cv2.imencode('.jpg', img)
            base64_img = base64.b64encode(buffer).decode('utf-8')

            # ai-workerì— ë¹„ë™ê¸° ê°ì • ë¶„ì„ íƒœìŠ¤í¬ ì „ë‹¬
            celery_app.send_task(
                "tasks.vision.analyze_emotion",
                args=[self.session_id, base64_img]
            )
            # ëˆˆ ì¶”ì  Taskë„ í˜¸ì¶œí•˜ì—¬ ë°ì´í„° ì €ì¥ (ì„ íƒì )
            # celery_app.send_task("tasks.vision.track_eyes", args=[self.session_id, base64_img])

        return frame

async def start_stt_with_local_whisper(audio_track: MediaStreamTrack, session_id: str):
    """Local Faster-Whisper ì‹¤ì‹œê°„ STT ì‹¤í–‰ (Buffering approach)"""
    
    logger.info(f"[{session_id}] â­ start_stt_with_local_whisper CALLED - Function entered successfully")
    
    # ëª¨ë¸ ë¡œë”© (ìµœì´ˆ 1íšŒ)
    if WHISPER_MODEL is None:
        load_local_whisper()
    
    if WHISPER_MODEL is None:
        logger.error(f"[{session_id}] Local Whisper Model not available.")
        return

    try:
        logger.info(f"[{session_id}] Starting Local Whisper STT Stream...")
        loop = asyncio.get_running_loop()
        
        # WhisperëŠ” 16kHz, Mono, Float32 ì…ë ¥ì„ ê¸°ëŒ€í•¨
        resampler = av.AudioResampler(format='flt', layout='mono', rate=16000)
        
        buffer = [] # Float32 samples accumulation
        BUFFER_DURATION_SEC = 1.0 # [ìˆ˜ì •] 2ì´ˆ â†’ 1ì´ˆë¡œ ë‹¨ì¶• (ì§§ì€ ë‹µë³€ ì¸ì‹ ê°œì„ )
        SAMPLE_RATE = 16000
        CHUNK_SIZE = int(SAMPLE_RATE * BUFFER_DURATION_SEC)
        
        # ì´ì „ í…ìŠ¤íŠ¸ ì¤‘ë³µ ë°©ì§€ìš©
        last_text = ""

        frame_count = 0
        while True:
            try:
                frame = await audio_track.recv()
                frame_count += 1
                
                # Resample & Convert to Numpy
                frame_resampled = resampler.resample(frame)
                for f in frame_resampled:
                    # to_ndarray returns (1, samples) for stereo or mono depending on layout
                    # format='flt' -> float32
                    chunk = f.to_ndarray()[0] 
                    buffer.extend(chunk)
                
                # í”„ë ˆì„ ìˆ˜ì‹  ë¡œê·¸ (10í”„ë ˆì„ë§ˆë‹¤)
                if frame_count % 10 == 0:
                    logger.info(f"[{session_id}] ğŸ“Š Received {frame_count} frames, buffer size: {len(buffer)}")
                
                # ë²„í¼ê°€ ì¼ì • í¬ê¸° ì´ìƒ ìŒ“ì´ë©´ ì¶”ë¡  ì‹¤í–‰
                if len(buffer) >= CHUNK_SIZE:
                    logger.info(f"[{session_id}] ğŸ¤ Buffer full ({len(buffer)} samples), starting transcription...")
                    audio_data = np.array(buffer, dtype=np.float32)
                    buffer = [] # ë²„í¼ ì´ˆê¸°í™” (ë˜ëŠ” ì˜¤ë²„ë© êµ¬í˜„ ê°€ëŠ¥)
 
                    # VAD Filterë¥¼ ì¼œì„œ ë¬´ìŒ êµ¬ê°„ ì œì™¸í•˜ê³  ì¸ì‹
                    segments, info = WHISPER_MODEL.transcribe(audio_data, language="ko", vad_filter=True)
                    
                    text_segments = [s.text for s in segments]
                    current_text = " ".join(text_segments).strip()
                    
                    if not current_text:
                        logger.info(f"[{session_id}] ğŸ”‡ No active speech detected (Silence/VAD)")
                    elif current_text == last_text:
                        logger.info(f"[{session_id}] ğŸ” Duplicate text (ignored): {current_text}")
                    else:
                        logger.info(f"[{session_id}] Local STT: {current_text}")
                        last_text = current_text
                        
                        stt_data = {
                            "session_id": session_id,
                            "text": current_text,
                            "type": "stt_result",
                            "is_final": True, # ë¡œì»¬ ë°°ì¹˜ëŠ” í•­ìƒ Finalë¡œ ì·¨ê¸‰
                            "timestamp": time.time()
                        }
                        
                        if session_id in active_websockets:
                            ws = active_websockets[session_id]
                            asyncio.run_coroutine_threadsafe(send_to_websocket(ws, stt_data), loop)

            except Exception as e:
                logger.error(f"[{session_id}] Local Whisper Stream Error/End: {e}", exc_info=True)
                break
        
        # [ì¤‘ìš”] ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œ ë‚¨ì€ ë²„í¼ ì²˜ë¦¬ (ì§§ì€ ë‹µë³€ ë³´ì¡´)
        if len(buffer) > 0:
            logger.info(f"[{session_id}] ğŸ”š Processing remaining {len(buffer)} samples before stream end...")
            try:
                audio_data = np.array(buffer, dtype=np.float32)
                segments, info = WHISPER_MODEL.transcribe(audio_data, language="ko", vad_filter=True)
                text_segments = [s.text for s in segments]
                final_text = " ".join(text_segments).strip()
                
                if final_text and final_text != last_text:
                    logger.info(f"[{session_id}] Local STT (final): {final_text}")
                    stt_data = {
                        "session_id": session_id,
                        "text": final_text,
                        "type": "stt_result",
                        "is_final": True,
                        "timestamp": time.time()
                    }
                    if session_id in active_websockets:
                        ws = active_websockets[session_id]
                        asyncio.run_coroutine_threadsafe(send_to_websocket(ws, stt_data), loop)
            except Exception as e:
                logger.error(f"[{session_id}] Error processing final buffer: {e}")
                
        logger.info(f"[{session_id}] Local Whisper Stream Finished")

    except Exception as e:
        logger.error(f"[{session_id}] Local STT Init Error: {e}")
async def send_to_websocket(ws: WebSocket, data: dict):
    """WebSocketìœ¼ë¡œ ë°ì´í„° ì „ì†¡"""
    try:
        await ws.send_json(data)
    except Exception as e:
        logger.error(f"WebSocket ì „ì†¡ ì‹¤íŒ¨: {e}")

# ============== WebSocket ì—”ë“œí¬ì¸íŠ¸ ==============
@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    await websocket.accept()
    active_websockets[session_id] = websocket
    logger.info(f"[{session_id}] âœ… WebSocket ì—°ê²° ì„±ê³µ")
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸° (í˜„ì¬ëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ ì—†ìŒ)
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        logger.info(f"[{session_id}] âŒ WebSocket ì—°ê²° ì¢…ë£Œ")
    except Exception as e:
        logger.error(f"[{session_id}] WebSocket ì—ëŸ¬: {e}")
    finally:
        # ì—°ê²° ì¢…ë£Œ ì‹œ ì„¸ì…˜ ì œê±°
        if session_id in active_websockets:
            del active_websockets[session_id]
            logger.info(f"[{session_id}] WebSocket ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")

# ============== WebRTC ì—”ë“œí¬ì¸íŠ¸ ==============
@app.post("/offer")
async def offer(request: Request):
    params = await request.json()
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])
    session_id = params.get("session_id", "unknown")

    # STUN ì„œë²„ ì„¤ì •ì€ ìœ ì§€ (ë¹„ë””ì˜¤ ì—°ê²° ì•ˆì •ì„±ì„ ìœ„í•´)
    pc = RTCPeerConnection(
        configuration=RTCConfiguration(
            iceServers=[RTCIceServer(urls="stun:stun.l.google.com:19302")]
        )
    )
    @pc.on("track")
    def on_track(track):
        logger.info(f"[{session_id}] Received track: {track.kind}")

        if track.kind == "audio":
            # [ë³€ê²½] Deepgram ëŒ€ì‹  Local Whisper ì‚¬ìš©
            asyncio.ensure_future(start_stt_with_local_whisper(track, session_id))
            logger.info(f"[{session_id}] Audio track processing started (Local Whisper enabled)")
        elif track.kind == "video":
            # ë¹„ë””ì˜¤ íŠ¸ë™: ê°ì • ë¶„ì„ ì²˜ë¦¬
            pc.addTrack(VideoAnalysisTrack(relay.subscribe(track), session_id))
            logger.info(f"[{session_id}] Video analysis track added")
        else:
            logger.warning(f"[{session_id}] Unknown track type: {track.kind}")

    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type
    }

async def consume_audio(track):
    """ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì†Œë¹„í•˜ì—¬ ë²„í¼ê°€ ì°¨ì§€ ì•Šë„ë¡ í•¨"""
    try:
        while True:
            await track.recv()
    except Exception:
        # íŠ¸ë™ì´ ì¢…ë£Œë˜ë©´ ì˜ˆì™¸ ë°œìƒ (ì •ìƒì ì¸ ì¢…ë£Œ)
        pass

@app.get("/")
async def root():
    return {
        "service": "AI Interview Media Server",
        "status": "running",
        "mode": "Video Analysis + Server-side Whisper STT"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")
