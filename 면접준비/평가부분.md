# 답변 평가 시스템 분석 및 루브릭 개선 계획

---

## 1. 현재 평가 코드 위치 및 구조

### 트리거 지점: `backend-core/routes/transcripts.py` (line 51~72)

사용자가 답변을 제출할 때마다 `POST /transcripts`가 호출되고, 여기서 두 개의 Celery 태스크가 동시에 발사된다.

```python
# 1. 다음 질문 생성 → gpu_queue
celery_app.send_task("tasks.question_generation.generate_next_question", ...)

# 2. 현재 답변 평가 → gpu_queue
celery_app.send_task("tasks.evaluator.analyze_answer", ...)
```

---

## 2. 평가 코드: `ai-worker/tasks/evaluator.py`

평가 시스템은 두 단계로 구성된다.

### 1단계: 답변마다 실시간 평가 (`analyze_answer`, line 70)

| 항목 | 내용 |
|---|---|
| 입력 | question_text, answer_text, rubric_json |
| 평가 항목 | `technical_score` (0-5), `communication_score` (0-5), `feedback` |
| 방법 | EXAONE LLM에 루브릭 기준과 함께 질문/답변 쌍을 전달 |
| 저장 위치 | `transcripts.sentiment_score` (감성 점수로 변환해서 저장) |
| 루브릭 | `question.rubric_json` 필드 사용 |

현재 루브릭은 다음과 같이 생성된다.

```python
# interviews.py에서 템플릿 질문 생성 시
rubric_json={"criteria": ["명확성"]}  # ← 모든 질문에 동일한 단순 루브릭
```

즉 루브릭이 사실상 껍데기다. `{"criteria": ["명확성"]}` 하나만 있어서 LLM이 "표준 면접 평가 기준"에 의존한다.

### 2단계: 면접 종료 후 최종 리포트 (`generate_final_report`, line 153)

| 항목 | 내용 |
|---|---|
| 입력 | 면접 전체 대화 기록 (AI + User 발화 합쳐서) |
| 평가 항목 | 6개 지표(기술/경험/문제해결/소통/책임감/성장) 각 0-100점 |
| 방법 | 전체 대화를 EXAONE에 한 번에 넣고 JSON으로 출력 |
| 저장 위치 | `evaluation_reports` 테이블 |
| 페르소나 | "수천 명을 검증한 시니어 면접관 위원장" |

---

## 3. 현재 구조의 문제점

### 문제 1: 루브릭이 비어있음
모든 질문의 `rubric_json = {"criteria": ["명확성"]}` 으로 동일하다. stage별로 다른 기준이 없어서 LLM이 매번 자체 판단한다.

### 문제 2: analyze_answer 결과가 최종 리포트에 반영 안됨
`analyze_answer`가 `sentiment_score`에 저장해두는데, `generate_final_report`는 이 값을 폴백으로만 사용하고 실제로는 대화 전체를 다시 LLM에 넣어 새로 평가한다. 중간 평가가 최종 평가에 직접 연동되지 않는다.

### 문제 3: 두 LLM 태스크가 GPU 큐에서 경쟁
다음 질문 생성(`generate_next_question`)과 답변 평가(`analyze_answer`)가 동시에 `gpu_queue`에 들어가서 서로 GPU 자원을 경쟁한다. 이게 면접 진행이 느려지는 또 다른 원인이다.

---

## 4. 루브릭 개선 계획

### 수정할 파일 4개

```
1. interview_scenario_transition.py  ← 각 stage에 rubric 필드 추가
2. ai-worker/db.py                   ← save_generated_question에 rubric_json 파라미터 추가
3. ai-worker/tasks/question_generator.py  ← stage rubric을 save_generated_question에 전달
4. backend-core/routes/interviews.py      ← 템플릿 질문 생성 시 stage rubric 사용
```

---

### 수정 1단계: 각 stage에 rubric 필드 추가

`interview_scenario_transition.py` 각 stage에 단계별 루브릭을 정의한다.

| stage | 루브릭 핵심 기준 |
|---|---|
| intro | 자기표현 명확성, 핵심 경험 요약, 1분 내 구조적 전달 |
| motivation | 직무 이해도, 지원 동기 진정성, 회사 연구 수준 |
| skill | 기술적 정확성, 개념-실무 연계, 용어 사용 적절성 |
| skill_followup | 원리 설명 능력, 선택 근거, 기술 심층 이해 |
| experience | STAR 구조, 역할 명확성, 성과 구체성 |
| experience_followup | 기술 선택 근거, 문제 인식, 비판적 사고 |
| problem_solving | 문제 분석력, 해결 과정 논리성, 결과 측정 |
| problem_solving_followup | 대안 검토, 최선 판단 근거, 기술적 사고 |
| communication | 협업 기여도, 갈등 해결 방식, 팀 내 역할 |
| communication_followup | 갈등 해결 구체성, 설득 방식, 의견 조율 |
| responsibility | 가치관 일관성, 직업윤리 인식, 직무 연계성 |
| responsibility_followup | 딜레마 판단력, 윤리적 사고, 원칙 견고성 |
| growth | 학습 계획 구체성, 기술 트렌드 인식, 성장 방향성 |
| growth_followup | 실제 학습 사례, 자기계발 실천력 |
| final_statement | 역질문 적절성, 마무리 인상, 기업 이해도 |

각 stage에 추가될 rubric 필드 구조 예시 (11번 가치관 질문):

```python
"rubric": {
    "criteria": ["가치관 일관성", "직업윤리 인식 수준", "직무 연계성"],
    "focus": "지원자가 밝힌 가치관이 지원 직무에서 어떻게 발현될 수 있는지 구체적 근거를 제시했는지 집중 평가",
    "scoring": {
        "technical_score": "기술적 맥락에서 가치관을 설명한 수준 (0-5)",
        "communication_score": "가치관을 설득력 있게 전달한 능력 (0-5)"
    }
}
```

---

### 수정 2단계: db.py 수정

`save_generated_question`과 `_save_generated_question_logic`에 `rubric_json` 파라미터 추가.

```python
# 변경 전 (db.py line 353)
def save_generated_question(interview_id, content, category, stage, guide=None, session=None):
    ...
    rubric_json={"guide": guide}   # ← 고정값

# 변경 후
def save_generated_question(interview_id, content, category, stage, guide=None, rubric_json=None, session=None):
    ...
    rubric_json=rubric_json if rubric_json else {"guide": guide}   # ← stage별 루브릭 사용
```

---

### 수정 3단계: question_generator.py 수정

`save_generated_question` 호출 시 stage rubric을 함께 전달한다.

```python
# 변경 전 (question_generator.py line 359~366)
q_id = save_generated_question(
    interview_id=interview_id,
    content=final_content,
    category=db_category,
    stage=next_stage['stage'],
    guide=next_stage.get('guide', ''),
    session=session
)

# 변경 후
q_id = save_generated_question(
    interview_id=interview_id,
    content=final_content,
    category=db_category,
    stage=next_stage['stage'],
    guide=next_stage.get('guide', ''),
    rubric_json=next_stage.get('rubric'),   # ← 추가
    session=session
)
```

---

### 수정 4단계: interviews.py 수정

초기 템플릿 질문 생성 시 `stage_config`에서 rubric을 가져온다.

```python
# 변경 전 (interviews.py line 202, line 719)
rubric_json={"criteria": ["명확성"]}

# 변경 후
rubric_json=stage_config.get("rubric", {"criteria": ["명확성"]})
```

---

## 5. 개선 후 결과 비교

### 현재 analyze_answer가 LLM에 전달하는 루브릭

```
[평가 루브릭]
{"criteria": ["명확성"]}
```

→ LLM이 "표준 면접 평가 기준"으로 알아서 판단. 질문 성격 무시.

### 개선 후 (11번 가치관 질문) analyze_answer가 LLM에 전달하는 루브릭

```
[평가 루브릭]
{
  "criteria": ["가치관 일관성", "직업윤리 인식 수준", "직무 연계성"],
  "focus": "지원자가 밝힌 가치관이 지원 직무에서 어떻게 발현될 수 있는지
            구체적 근거를 제시했는지 집중 평가",
  "scoring": {
    "technical_score": "기술적 맥락에서 가치관을 설명한 수준 (0-5)",
    "communication_score": "가치관을 설득력 있게 전달한 능력 (0-5)"
  }
}
```

→ LLM이 질문의 성격(가치관 평가)에 맞게 정밀 채점.

### 개선 후 (5번 실무경험 질문) analyze_answer가 LLM에 전달하는 루브릭

```
[평가 루브릭]
{
  "criteria": ["STAR 구조", "역할 명확성", "성과 구체성"],
  "focus": "Situation-Task-Action-Result 구조로 답변이 전개되었는지,
            본인의 역할이 팀 기여와 어떻게 연결되는지 집중 평가",
  "scoring": {
    "technical_score": "프로젝트/경험의 기술적 구체성 (0-5)",
    "communication_score": "경험을 논리적으로 전달한 능력 (0-5)"
  }
}
```

### 최종 리포트에 미치는 영향

`details_json` 안의 단계별 피드백(`responsibility_feedback`, `experience_feedback` 등)의 품질이 실질적으로 올라간다.

`analyze_answer`가 stage에 맞는 기준으로 채점 → `sentiment_score`의 신뢰도 상승 → `generate_final_report` 폴백 시 더 정확한 평균 점수 계산 가능.

---

## 6. 15번 질문 이후 면접 종료 지연 문제

### 원인 분석

15번(final_statement) 답변 제출 후 면접 종료까지 오래 걸리는 이유는 다음과 같다.

#### 전체 흐름 시퀀스

```
사용자가 15번(final_statement)에 답변 제출
         ↓
nextQuestion() 호출 (App.jsx line 729)
         ↓
createTranscript() 저장 → 백엔드에서 generate_next_question_task 트리거
         ↓
[지연 구간 시작]
currentIdx === questions.length - 1 → 폴링 루프 진입 (line 763)
         ↓
2초 간격으로 최대 60번(2분) 서버에 질문 목록 요청
         ↓
Celery GPU 큐에서 generate_next_question_task 실행
         ↓
final_statement → get_next_stage() → None → interview.status = COMPLETED
         ↓
프론트엔드가 COMPLETED 감지 → finishInterview() 호출
```

#### 구체적인 지연 원인 3가지

1. Celery GPU 큐 대기 시간
   - `generate_next_question_task`는 무조건 `gpu_queue`로 전달된다
   - 15번 직전에 14번 꼬리질문이 LLM으로 생성 중이라면 GPU 큐에 작업이 밀린다
   - final_statement 처리는 LLM/RAG가 전혀 필요 없고 DB 상태만 COMPLETED로 바꾸면 되는데, 무거운 GPU 큐를 타고 있다

2. 폴링 루프 설계
   ```javascript
   // App.jsx line 763
   for (let i = 0; i < 60; i++) {          // 최대 120초(2분) 대기
       await new Promise(r => setTimeout(r, 2000)); // 2초 sleep 먼저
       const data = await getInterviewQuestions(interview.id); // 그 다음 확인
   }
   ```
   루프 첫 번째 반복부터 2초를 먼저 재운 뒤 확인한다. 서버가 즉시 COMPLETED로 바꿔도 최소 2초는 기다린다.

3. COMPLETED 감지 경로가 indirect
   - 프론트엔드는 `interview.status`를 직접 폴링하는 게 아니라 `getInterviewQuestions()`의 응답 안에 있는 `data.status`를 확인한다
   - Celery 태스크 완료 → DB 커밋 → 프론트 폴링이 맞아야 해서 타이밍이 맞지 않으면 한 사이클(2초)을 더 기다린다

### 해결 방향

현재 질문이 `final_statement`임을 프론트엔드가 이미 알고 있으므로, 폴링 루프를 건너뛰고 바로 `finishInterview()`를 호출한다.

```javascript
// nextQuestion() 함수 내부 수정 (App.jsx)
const currentQuestion = questions[currentIdx];
const isFinalStage = currentQuestion?.content?.includes('[최종 발언]') ||
                     questions.length - 1 === currentIdx && currentIdx >= 14;

if (isFinalStage) {
    // GPU 큐 대기, 폴링 루프 완전 제거 → 즉시 종료
    await finishInterview();
    return;
}
```

이렇게 하면 GPU 큐 대기(수십 초) + 폴링 루프(최소 2초)가 모두 제거되어 즉시 면접 종료 화면으로 전환된다.
