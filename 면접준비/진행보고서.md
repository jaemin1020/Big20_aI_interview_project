# AI 면접 프로젝트 진행보고서

> 작성일: 2026-02-23
> 작성 목적: 프로젝트 개발 과정에서 발견된 주요 성능 이슈 및 해결 경위 기록 (면접 기술 설명용)

---

## 1. 이슈 개요

### 현상

AI 면접 시스템에서 지원자가 템플릿 기반 첫 두 질문에 답변한 후, **세 번째 이후의 AI 생성 질문을 기다리는 시간이 비정상적으로 길었음** (60초 이상 소요되는 경우 발생).

---

## 2. 원인 분석

### 원인 1: LLM 모델의 지연 로딩(Lazy Loading) 구조

**코드 위치**: `ai-worker/utils/exaone_llm.py`, `ai-worker/tasks/question_generator.py`

기존 시스템은 **EXAONE-3.5-7.8B** 모델(약 78억 파라미터, GGUF 양자화)을 아래와 같이 운용하고 있었다.

```python
# question_generator.py - 태스크 실행 시점에 비로소 임포트
def generate_next_question_task(interview_id: int):
    from utils.exaone_llm import get_exaone_llm  # ← 이 시점에 최초 로딩 발생
    llm = get_exaone_llm()
```

- Celery 워커 컨테이너가 시작될 때 모델을 로드하지 않음.
- 첫 번째 AI 질문 생성 태스크가 큐에 들어와 실행이 시작되는 그 순간에야 비로소 모델 파일을 디스크에서 읽어 GPU 메모리에 올리는 작업이 시작됨.
- **EXAONE 7.8B 모델의 로딩 시간: 약 30초 ~ 120초** (GPU 환경, Q4_K_M 양자화 기준).

### 원인 2: 싱글톤이지만 워밍업(Warm-up)이 없었음

```python
class ExaoneLLM(LLM):
    _instance: ClassVar[Optional["ExaoneLLM"]] = None
    _initialized: ClassVar[bool] = False

    def __new__(cls, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance  # 한 번 만들면 재사용 (싱글톤)
```

- 싱글톤 패턴으로 설계하여 두 번째 요청부터는 즉시 사용 가능하지만,
- **첫 번째 호출 시에 모델 파일 전체를 메모리에 올리는 초기화 비용이 반드시 발생**.
- 이 비용을 사용자가 직접 대기하는 구조였음.

### 원인 3: 초기 Template 질문과 AI 질문의 처리 방식 차이

| 질문 유형             | 처리 방식                              | LLM 필요 여부 | 생성 시간                       |
| --------------------- | -------------------------------------- | ------------- | ------------------------------- |
| Template 질문 (1~2번) | 이력서 구조화 데이터에서 문자열 포맷팅 | ❌ 불필요     | < 1초                           |
| AI 생성 질문 (3번~)   | EXAONE LLM 추론 + RAG 검색             | ✅ 필수       | 10~20초 (모델 이미 로드된 경우) |

- 초기 Template 질문들은 LLM 없이 생성 가능하지만, 그 태스크들이 실행되는 동안 LLM 로딩이 **병렬로 이루어지지 않았음**.
- 결과적으로 화면에 두 번째 질문이 나타나고, 사용자가 답변을 제출했을 때 세 번째 질문 생성 요청이 들어오면 그때서야 모델 로딩을 시작 → **모델 로딩 시간 + 질문 생성 시간**을 사용자가 고스란히 대기.

---

## 3. 발견 경위

```
[시나리오]
사용자가 2번 질문(경력 소개, Template)에 답변 제출
→ 3번 질문(AI 생성, 꼬리질문) 폴링 시작
→ 프론트엔드가 2초 간격으로 최대 60번 (2분) 폴링
→ 실제로는 모델 로딩만 30~120초 → 그 이후 질문 생성 10~20초
→ 최악의 경우 3분 이상 화면이 멈춘 것처럼 보임
```

로그 분석을 통해 `🚀 Starting streaming generation for Interview {id}` 로그가 찍히기 전까지 수십 초의 공백이 있었음을 확인.

---

## 4. 해결 방안: 면접 세션 생성 시 비동기 LLM 사전 로딩

### 핵심 아이디어

> "모델 로딩 비용을 사용자가 부담하는 시간에서, 서버가 초기화되는 시간으로 옮긴다."

구체적으로는, 면접 세션이 생성되어 Template 질문들이 DB에 저장되는 그 순간, **LLM을 메모리에 올리는 별도의 Celery 태스크를 즉시 GPU 큐에 발사**한다. 이 태스크는 완전히 비동기로 실행되므로 면접 세션 생성 API의 응답 속도에 영향을 주지 않는다.

### 수정 파일 1: `ai-worker/tasks/question_generator.py`

**Warmup 전용 태스크 추가**

```python
@shared_task(name="tasks.question_generation.preload_model")
def preload_model_task():
    """
    EXAONE 모델을 메모리에 미리 로드해두는 웜업(Warmup) 태스크.
    면접 세션 생성 시 즉시 실행되어, AI 질문이 필요한 시점에
    모델이 이미 준비된 상태가 되도록 합니다.
    """
    try:
        from utils.exaone_llm import get_exaone_llm
        logger.info("🔥 [Preload] EXAONE 모델 사전 로딩 시작...")
        get_exaone_llm()  # 싱글톤 - 한 번 로딩되면 이후 모든 태스크에서 재사용
        logger.info("✅ [Preload] EXAONE 모델 사전 로딩 완료.")
    except Exception as e:
        logger.warning(f"⚠️ [Preload] 실패 (AI 질문 생성 시 자동 재시도): {e}")
```

### 수정 파일 2: `backend-core/routes/interviews.py`

**면접 세션 생성 직후 Preload 태스크 즉시 발사**

```python
# 면접 세션 및 Template 질문 저장 완료 후
logger.info(f"✅ Interview setup SUCCESS for ID={interview_id}")

# [최적화] 사용자가 1~2번 질문에 답변하는 동안 LLM 모델 메모리 적재 (비동기)
celery_app.send_task(
    "tasks.question_generation.preload_model",
    queue="gpu_queue"
)
logger.info("🔥 [Preload] EXAONE 모델 사전 로딩 태스크 발사 완료 (비동기)")
```

---

## 5. 개선 전후 비교

```
[개선 전 흐름]
면접 시작 (Template Q1, Q2 생성 및 저장)
    ↓
사용자 Q1 답변 → 대기
사용자 Q2 답변 → 대기
Q3 AI 질문 요청 → 모델 로딩 시작 (30~120초) → 질문 생성 (10~20초)
→ 총 대기 시간: 40초 ~ 140초 (사용자 부담)

[개선 후 흐름]
면접 시작 (Template Q1, Q2 생성 및 저장)
    ↓                          ↓ (동시 실행)
사용자 Q1 답변 중          모델 로딩 시작
사용자 Q2 답변 중          모델 로딩 완료 ✅
Q3 AI 질문 요청 → 질문 생성만 (10~20초)
→ 총 대기 시간: 10~20초 (모델 로딩 비용 제거)
```

---

## 6. 면접용 핵심 설명 요약

### Q: 왜 AI 질문 생성이 처음에 느렸나요?

> "78억 파라미터 규모의 EXAONE LLM 모델이 컨테이너 시작 시점이 아닌, 첫 번째 AI 질문 요청 시점에 비로소 GPU 메모리에 적재되는 **지연 로딩(Lazy Loading)** 구조였기 때문입니다. 모델 로딩에만 최대 2분이 소요될 수 있었고, 사용자가 이 시간을 대기 화면에서 전부 부담하는 UX 문제가 있었습니다."

### Q: 어떻게 해결했나요?

> "면접 세션이 생성되는 시점에 LLM 모델을 GPU 메모리에 올리는 **경량 Warmup 태스크를 별도로 Celery GPU 큐에 즉시 발사**했습니다. 이 태스크는 완전히 비동기로 실행되므로 세션 생성 속도에 영향을 주지 않으면서, 사용자가 초반 2~3개의 템플릿 질문에 답변하는 시간 동안 모델 로딩이 병렬로 완료됩니다. 덕분에 AI 생성 질문이 처음 필요해지는 3번 문항부터는 모델 로딩 지연 없이 즉시 질문을 생성할 수 있게 되었습니다."

### Q: Celery를 사용한 이유는?

> "AI 워커와 백엔드 서버가 분리된 마이크로서비스 구조이기 때문입니다. GPU 연산(모델 로딩)은 반드시 GPU 워커 컨테이너에서 실행되어야 하며, Celery의 **태스크 큐(Task Queue) 메커니즘**을 통해 백엔드에서 GPU 워커로 작업을 위임하는 것이 아키텍처상 가장 자연스러운 방법이었습니다."
