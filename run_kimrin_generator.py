import os
import sys
import logging
print("DEBUG: Core modules imported", flush=True)

# [1. ê²½ë¡œ ì„¤ì •] í˜„ì¬ ì‹¤í–‰ ìœ„ì¹˜(ë£¨íŠ¸)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì¶”ê°€
root_dir = os.path.dirname(os.path.abspath(__file__))
# ë£¨íŠ¸ í´ë”ë¥¼ pathì— ì¶”ê°€í•˜ì—¬ íŒ¨í‚¤ì§€ëª…(ai_worker, backend_core)ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•¨
if root_dir not in sys.path:
    sys.path.insert(0, root_dir)

# í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ DB ì—°ê²° ì„¤ì • (ë¡œì»¬ ì‹¤í–‰ìš©)
if not os.getenv("DATABASE_URL"):
    os.environ["DATABASE_URL"] = "postgresql+psycopg://postgres:1234@localhost:15432/interview_db"

print(f"DEBUG: sys.path updated with {root_dir}", flush=True)

try:
    print("DEBUG: Importing modules...", flush=True)
    # ai-worker.utils ì™€ backend-core.utils ëª…ì¹­ ì¶©ëŒì„ í”¼í•˜ê¸° ìœ„í•´ 
    # ì„œë¸Œë””ë ‰í† ë¦¬ë¥¼ ì§ì ‘ ì¶”ê°€í•˜ì—¬ ëª¨ë“ˆì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
    ai_worker_path = os.path.join(root_dir, "ai-worker")
    ai_worker_utils_path = os.path.join(ai_worker_path, "utils")
    backend_core_path = os.path.join(root_dir, "backend-core")
    
    # ìˆœì„œ: ai-worker/utilsë¥¼ ìµœìƒë‹¨ì— ì¶”ê°€
    sys.path.insert(0, backend_core_path)
    sys.path.insert(0, ai_worker_path)
    sys.path.insert(0, ai_worker_utils_path) 
    
    from db import engine, save_generated_question
    from db_models import Question, User, Resume, Interview, InterviewStatus
    from config.interview_scenario import INTERVIEW_STAGES
    from tasks.parse_resume import parse_resume_final
    from tasks.chunking import chunk_resume
    
    # utils.xxx ëŒ€ì‹  xxx ë¡œ ì§ì ‘ ì„í¬íŠ¸ (ai-worker/utilsê°€ pathì— ë¨¼ì € ìˆìœ¼ë¯€ë¡œ)
    import question_retriever
    import exaone_llm
    import vector_utils
    get_question_retriever = question_retriever.get_question_retriever
    get_exaone_llm = exaone_llm.get_exaone_llm
    get_embedding_generator = vector_utils.get_embedding_generator
except ImportError as e:
    print(f"âŒ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}", flush=True)
    import traceback
    traceback.print_exc()
    sys.exit(1)
except Exception as e:
    print(f"âŒ ê¸°íƒ€ ì˜¤ë¥˜: {e}", flush=True)
    import traceback
    traceback.print_exc()
    sys.exit(1)

from sqlmodel import Session, select
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger("InterviewPipeline")

# [2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿]
PROMPT_TEMPLATE = """[|system|]
ë„ˆëŠ” ì „ë¬¸ ë©´ì ‘ê´€ì´ë‹¤. ì§€ì›ìì˜ [ì´ë ¥ì„œ ë§¥ë½]ê³¼ DBì—ì„œ ê²€ìƒ‰ëœ [ì°¸ì¡° ì§ˆë¬¸ 5ê°œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ, 
í•´ë‹¹ ì§€ì›ìì—ê²Œë§Œ ë˜ì§ˆ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•˜ë¼.

[ê·œì¹™]
1. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ, ë‘ ë¬¸ì¥ ì´ë‚´(150ì)ë¡œ ì‘ì„±í•˜ë¼.
2. DB ì°¸ì¡° ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ ìœ ì§€í•˜ë˜, ì§€ì›ìì˜ êµ¬ì²´ì ì¸ í”„ë¡œì íŠ¸ë‚˜ ê¸°ìˆ  ìŠ¤íƒ ë‚´ìš©ì„ ë¬¸ì¥ì— ë…¹ì—¬ë¼.
3. í˜„ì¬ ë©´ì ‘ ë‹¨ê³„ì˜ [í‰ê°€ ê°€ì´ë“œ]ë¥¼ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ë¼.
[|endofturn|]
[|user|]
# ë©´ì ‘ ë‹¨ê³„: {stage_name}
# í‰ê°€ ê°€ì´ë“œ: {guide}
# ì§€ì›ì ì„±í•¨: {name}
# ì´ë ¥ì„œ ë§¥ë½: {context}

# DB ê²€ìƒ‰ëœ ì°¸ì¡° ì§ˆë¬¸ (ì´ ì¤‘ ê°€ì¥ ì ì ˆí•œ ì˜ë„ë¥¼ ì„ íƒí•´ì„œ ë³€í˜•):
{db_questions}

# ìš”ì²­: ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ {name} ì§€ì›ìë§Œì„ ìœ„í•œ ë‚ ì¹´ë¡œìš´ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•´ì¤˜.
[|endofturn|]
[|assistant|]
"""

import numpy as np

def main():
    target_pdf = "ê¹€ë¦°_ì‹ ì…_ì‚¼ì„±-aiê°œë°œìì´ë ¥ì„œ.pdf"
    if not os.path.exists(target_pdf):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_pdf}")
        return

    # 1. ì´ë ¥ì„œ ë¶„ì„ (Tasks í™œìš©)
    print("ğŸ“„ 1ë‹¨ê³„: ì´ë ¥ì„œ íŒŒì‹± ë° ì²­í‚¹ ì¤‘...")
    parsed_data = parse_resume_final(target_pdf)
    if not parsed_data:
        print("âŒ ì´ë ¥ì„œ íŒŒì‹± ì‹¤íŒ¨")
        return

    # ì´ë ¥ì„œ ë‚´ í—¤ë” ì •ë³´ì—ì„œ ì´ë¦„ê³¼ ì§ë¬´ ìë™ ì¶”ì¶œ
    header = parsed_data.get("header", {})
    name = header.get("name") or "ê¹€ë¦°"
    target_role = header.get("target_role") or "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì"

    print(f"ğŸš€ [{name}] ì§€ì›ì ({target_role}) ë§ì¶¤í˜• 15ê°œ ì§ˆë¬¸ ìƒì„± ë° DB ì €ì¥ ì‹œì‘\n")
    
    # [DB ì¤€ë¹„] ì‚¬ìš©ì, ì´ë ¥ì„œ, ë©´ì ‘ ì„¸ì…˜ ìƒì„±
    with Session(engine) as session:
        # 1. ì‚¬ìš©ì ì¡°íšŒ ë˜ëŠ” ìƒì„±
        user = session.exec(select(User).where(User.full_name == name)).first()
        if not user:
            user = User(
                username=f"user_{name}", 
                email=f"{name}@example.com", 
                full_name=name,
                password_hash="dummy" # ì‹¤ì œ ë¡œê·¸ì¸ ìš©ì´ ì•„ë‹˜
            )
            session.add(user)
            session.commit()
            session.refresh(user)
            print(f"ğŸ‘¤ ìƒˆ ì‚¬ìš©ì ìƒì„± ì™„ë£Œ: {name}")

        # 2. ì´ë ¥ì„œ ë ˆì½”ë“œ ìƒì„± (íŒŒì¼ì€ ì´ë¯¸ ì¡´ì¬í•¨)
        resume = Resume(
            candidate_id=user.id,
            file_name=os.path.basename(target_pdf),
            file_path=os.path.abspath(target_pdf),
            file_size=os.path.getsize(target_pdf),
            target_position=target_role,
            structured_data=parsed_data,
            processing_status="completed"
        )
        session.add(resume)
        session.commit()
        session.refresh(resume)
        print(f"ğŸ“„ ì´ë ¥ì„œ DB ë“±ë¡ ì™„ë£Œ (ID: {resume.id})")

        # 3. ë©´ì ‘ ì„¸ì…˜ ìƒì„±
        interview = Interview(
            candidate_id=user.id,
            resume_id=resume.id,
            position=target_role,
            status=InterviewStatus.LIVE
        )
        session.add(interview)
        session.commit()
        session.refresh(interview)
        interview_id = interview.id
        print(f"ğŸ¤ ë©´ì ‘ ì„¸ì…˜ ìƒì„± ì™„ë£Œ (ID: {interview_id})")

    chunks = chunk_resume(parsed_data)
    # 2. ìœ í‹¸ë¦¬í‹° ì¤€ë¹„
    retriever = get_question_retriever()
    llm = get_exaone_llm()
    embedder = get_embedding_generator()
    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
    chain = prompt | llm | StrOutputParser()

    print("ğŸ” ì´ë ¥ì„œ ì²­í¬ ì„ë² ë”© ìƒì„± ì¤‘ (ê²€ìƒ‰ ìµœì í™”)...", flush=True)
    chunk_texts = [c['text'] for c in chunks]
    chunk_embeddings = embedder.encode_batch(chunk_texts, is_query=False)

    generated_questions = []
    last_primary_question = "" 

    # 3. 15ê°œ ë‹¨ê³„ ìˆœíšŒ
    for stage in INTERVIEW_STAGES:
        order = stage['order']
        stage_name = stage['stage']
        stage_type = stage['type']
        guide = stage.get('guide', '')

        print(f"\n{'='*60}")
        print(f"ğŸ“Œ [ì§ˆë¬¸ {order:02d}] ë‹¨ê³„: {stage_name} ({stage_type})")
        
        final_content = ""
        category = "technical" # ê¸°ë³¸ ê°’

        if stage_type == "template":
            tmpl = stage['template']
            final_content = tmpl.format(candidate_name=name, target_role=target_role)
            print(f"ğŸ’¬ [Template] {final_content}")

        # 3ë²ˆë¶€í„° AI ìƒì„± (RAG ì ìš©)
        elif stage_type == "ai":
            # [Step 1: í˜„ì¬ ë‹¨ê³„ì— ì í•©í•œ ì´ë ¥ì„œ ë§¥ë½ ì°¾ê¸°]
            print(f"ğŸ“‹ '{stage_name}' ê´€ë ¨ ì´ë ¥ì„œ ë‚´ìš© ë§¤ì¹­ ì¤‘...", flush=True)
            stage_vec = embedder.encode_query(f"{stage_name} {guide}")
            
            scores = []
            for emb in chunk_embeddings:
                sim = np.dot(stage_vec, emb) / (np.linalg.norm(stage_vec) * np.linalg.norm(emb))
                scores.append(sim)
            
            top_indices = np.argsort(scores)[::-1][:3]
            stage_resume_context = "\n".join([chunk_texts[i] for i in top_indices])
            
            print(f"--- [Applied Resume Context] ---", flush=True)
            for i in top_indices:
                print(f"   - {chunk_texts[i]} (Score: {scores[i]:.4f})", flush=True)
            print(f"--------------------------------", flush=True)

            # [Step 2: ê²€ìƒ‰ëœ ì´ë ¥ì„œ ë§¥ë½ ê¸°ë°˜ DB ê²€ìƒ‰]
            print(f"ğŸ” DBì—ì„œ '{stage_name}' ê´€ë ¨ ìœ ì‚¬ ì§ˆë¬¸ 5ê°œ ê²€ìƒ‰ ì¤‘...", flush=True)
            db_results = retriever.find_relevant_questions(
                text_context=stage_resume_context,
                question_type=stage_name, 
                top_k=5
            )
            
            if len(db_results) < 5:
                 db_results = retriever.find_relevant_questions(
                    text_context=stage_resume_context,
                    top_k=10
                )[:5]

            db_questions_str = ""
            print(f"--- [DB Search Logs: Reference Questions (Cosine Similarity)] ---", flush=True)
            if not db_results:
                print("   (ìœ ì‚¬ ì§ˆë¬¸ ì—†ìŒ - ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ìƒì„±)", flush=True)
                db_questions_str = "ì°¸ì¡°í•  DB ì§ˆë¬¸ ì—†ìŒ."
            else:
                for i, q in enumerate(db_results):
                    print(f"   [{i+1}] {q.content} (ID: {q.id})", flush=True)
                    db_questions_str += f"{i+1}. {q.content}\n"
            print(f"-------------------------------------------------------------", flush=True)

            # LLM í˜¸ì¶œ
            final_content = chain.invoke({
                "stage_name": stage_name,
                "guide": guide,
                "name": name,
                "context": stage_resume_context,
                "db_questions": db_questions_str
            })
            
            print(f"ğŸ¤– [Personalized] {final_content}")
            last_primary_question = final_content

        # ê¼¬ë¦¬ì§ˆë¬¸ (Followup)
        elif stage_type == "followup":
            followup_prompt = f"ë°©ê¸ˆ {name}ë‹˜ì—ê²Œ '{last_primary_question}'ë¼ê³  ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ì´ ì§ˆë¬¸ì˜ ë‹µë³€ì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì˜ˆìƒ ê¼¬ë¦¬ì§ˆë¬¸ì„ '{guide}' ì˜ë„ì— ë§ê²Œ ìƒì„±í•˜ì„¸ìš”."
            
            final_content = chain.invoke({
                "stage_name": stage_name,
                "guide": guide,
                "name": name,
                "context": followup_prompt,
                "db_questions": "ì´ì „ ì§ˆë¬¸ ê¸°ë°˜ ê¼¬ë¦¬ì§ˆë¬¸ìƒì„±ì´ë¯€ë¡œ DB ê²€ìƒ‰ ìƒëµ"
            })
            print(f"â†ªï¸ [Follow-up] {final_content}")

        # [DB ì €ì¥] ìƒì„±ëœ ì§ˆë¬¸ ì €ì¥ (1, 2, 15ë²ˆ ì œì™¸)
        if final_content:
            if order not in [1, 2, 15]:
                save_generated_question(
                    interview_id=interview_id,
                    content=final_content,
                    category=category,
                    stage=stage_name,
                    guide=guide
                )
            generated_questions.append(final_content)

    # 4. ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\n\n{'*'*20} ìƒì„± ì™„ë£Œ {'*'*20}")
    print(f"ì´ {len(generated_questions)}ê°œì˜ ì§ˆë¬¸ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
